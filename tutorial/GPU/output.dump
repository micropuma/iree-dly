// -----// IR Dump After AutoInputConversionPipelinePass (iree-auto-input-conversion) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After IREEImportPublicPass (iree-import-public) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    util.return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After ImportMLProgramPass (iree-import-ml-program) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    util.return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After SanitizeModuleNamesPass (iree-sanitize-module-names) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    util.return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After ConvertMeshToFlowPass (iree-convert-mesh-to-flow) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    util.return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After DemoteF64ToF32Pass (iree-input-conversion-demote-f64-to-f32) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    util.return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::ConvertStreamableOpsPass (iree-abi-convert-streamable-ops) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    util.return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = util.call @_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
  util.func private @_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
    %0 = linalg.matmul {compilation_info = #compilation} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    util.return %0 : tensor<512x512xf16>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: tensor<512x128xf16>, %arg1: tensor<128x512xf16>, %arg2: tensor<512x512xf16>) -> tensor<512x512xf16> {
  %0 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%arg0, %arg1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%arg2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  util.return %0 : tensor<512x512xf16>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = util.call @_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
module {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AssignLegacyTargetDevicesPass (iree-hal-assign-legacy-target-devices) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTargetDevicesPass (iree-hal-materialize-target-devices) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDevicePromisesPass (iree-hal-resolve-device-promises) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDeviceAliasesPass (iree-hal-resolve-device-aliases) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After LinalgQuantizedConvToConvPass (iree-global-opt-quantized-conv-to-conv) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After LinalgQuantizedMatmulToMatmulPass (iree-global-opt-quantized-matmul-to-matmul) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After RemoveZeroExtentTensorsPass (iree-global-opt-remove-zero-extent-tensors) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After DetachElementwiseFromNamedOpsPass (iree-global-opt-detach-elementwise-from-named-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After LinalgNamedOpConversionPass (linalg-named-op-conversion) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After EraseUnusedLinalgOperandsPass (iree-global-opt-erase-unused-linalg-operands) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ExpandTensorShapesPass (iree-global-opt-expand-tensor-shapes) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertElementwiseToLinalgPass (convert-elementwise-to-linalg) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After RaiseSpecialOpsPass (iree-global-opt-raise-special-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After DecomposeConcatPass (iree-global-opt-decompose-concat) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After GeneralizeLinalgNamedOpsPass (iree-global-opt-generalize-linalg-named-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldUnitExtentDimsPass (iree-dispatch-creation-fold-unit-extent-dims) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After DemoteContractionInputsToBF16Pass (iree-global-opt-demote-contraction-inputs-to-bf16) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SetEncodingPass (iree-dispatch-creation-set-encoding) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After MaterializeEncodingIntoNopPass (iree-codegen-materialize-encoding-into-nop) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After MaterializeHomogeneousEncodingsPass (iree-global-opt-materialize-homogeneous-encodings) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyPackUnpackPass (iree-global-opt-simplify-pack-unpack) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After DataLayoutPropagationPass (iree-global-opt-data-layout-propagation) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After GeneralizeLinalgNamedOpsPass (iree-global-opt-generalize-linalg-named-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After GlobalLoopInvariantCodeMotionPass (iree-global-opt-loop-invariant-code-motion) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After HoistIntoGlobals (iree-util-hoist-into-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After JitGlobalsPass (iree-consteval-jit-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After RaiseSpecialOpsPass (iree-global-opt-raise-special-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After InjectTensorTracingPass (iree-flow-inject-tensor-tracing) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After TensorPadToTensorInsertSlicePass (iree-dispatch-creation-tensor-pad-to-tensor-insert-slice) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FusionPreprocessingPass (iree-dispatch-creation-fusion-preprocessing) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ElementwiseOpFusionPass (iree-dispatch-creation-elementwise-op-fusion) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After BubbleUpExpandShapesPass (iree-dispatch-creation-bubble-up-expand-shapes) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After BubbleUpExtractSlicesPass (iree-dispatch-creation-bubble-up-extract-slices) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ElementwiseOpFusionPass (iree-dispatch-creation-elementwise-op-fusion) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SinkReshapesPass (iree-dispatch-creation-sink-reshapes) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FuseMultiUseElementwiseProducerPass (iree-dispatch-creation-fuse-multi-use-elementwise-producer) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SplitReductionPass (iree-dispatch-creation-split-reduction-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After TransposeGenericOpsPass (iree-dispatch-creation-transpose-generic-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FormScalarDispatchesPass (iree-dispatch-creation-form-scalar-dispatches) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchRegionsPass (iree-dispatch-creation-form-dispatch-regions) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.region -> (tensor<512x512xf16>) {
    %5 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.return %5 : tensor<512x512xf16>
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CloneProducersIntoDispatchRegionsPass (iree-dispatch-creation-clone-producers-into-dispatch-regions) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.region -> (tensor<512x512xf16>) {
    %5 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.return %5 : tensor<512x512xf16>
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CollapseDimensionsPass (iree-dispatch-creation-collapse-dimensions) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.region -> (tensor<512x512xf16>) {
    %5 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.return %5 : tensor<512x512xf16>
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ConvertDispatchRegionsToWorkgroupsPass (iree-dispatch-creation-convert-dispatch-regions-to-workgroups) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ConvertTensorToFlowPass (iree-dispatch-creation-convert-tensor-to-flow) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After MaterializeDefaultWorkgroupCountRegionPass (iree-dispatch-creation-materialize-default-workgroup-count-region) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After VerifyInputLegalityPass (iree-verify-input-legality) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
        (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
      %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
      %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
      %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
      %8 = linalg.matmul {compilation_info = #compilation} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
      flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CaptureDynamicDimsPass (iree-flow-capture-dynamic-dims) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After InitializeEmptyTensorsPass (iree-flow-initialize-empty-tensors) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
    %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %8 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OutlineDispatchExternsPass (iree-flow-outline-dispatch-externs) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2 =
        (%arg3: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg4: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg5: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
      %5 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
      %6 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
      %7 = flow.dispatch.tensor.load %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
      %8 = linalg.matmul {compilation_info = #compilation} ins(%5, %6 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%7 : tensor<512x512xf16>) -> tensor<512x512xf16>
      flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineDispatchRegionsPass (iree-flow-outline-dispatch-regions) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchesPass (iree-flow-annotate-dispatches) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
  flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
      %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
      %3 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
      flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
      return
    }
  }
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After DeduplicateExecutablesPass (iree-flow-deduplicate-executables) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After InjectTensorTracingPass (iree-flow-inject-tensor-tracing) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CleanupTensorShapesPass (iree-flow-cleanup-tensor-shapes) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OutlineConstantsPass (iree-flow-outline-constants) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizerPass (iree-flow-canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInputPass (iree-stream-verify-input) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
  %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
  %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  flow.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    flow.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<512x128xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<128x512xf16>>, %arg2: !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %3 = linalg.matmul {compilation_info = #compilation} ins(%0, %1 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%2 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<512x128xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<128x512xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<512x512xf16>
    %3 = flow.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0, %1, %2) : (tensor<512x128xf16>, tensor<128x512xf16>, tensor<512x512xf16>) -> %2
    %4 = hal.tensor.export %3 "output0" : tensor<512x512xf16> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertToStreamPass (iree-stream-conversion) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    %element_type_f16_0 = hal.element_type<f16> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    %c128_2 = arith.constant 128 : index
    %c512_3 = arith.constant 512 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128_2, %c512_3]) type(%element_type_f16_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    %element_type_f16_4 = hal.element_type<f16> : i32
    %dense_row_major_5 = hal.encoding_type<dense_row_major> : i32
    %c512_6 = arith.constant 512 : index
    %c512_7 = arith.constant 512 : index
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512_6, %c512_7]) type(%element_type_f16_4) encoding(%dense_row_major_5)
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
    %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
    %c0 = arith.constant 0 : index
    %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %11 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensorsPass (iree-stream-verify-lowering-to-tensors) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    %element_type_f16_0 = hal.element_type<f16> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    %c128_2 = arith.constant 128 : index
    %c512_3 = arith.constant 512 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128_2, %c512_3]) type(%element_type_f16_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    %element_type_f16_4 = hal.element_type<f16> : i32
    %dense_row_major_5 = hal.encoding_type<dense_row_major> : i32
    %c512_6 = arith.constant 512 : index
    %c512_7 = arith.constant 512 : index
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512_6, %c512_7]) type(%element_type_f16_4) encoding(%dense_row_major_5)
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
    %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
    %c0 = arith.constant 0 : index
    %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %11 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
  %c0 = arith.constant 0 : index
  %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
  %element_type_f16_0 = hal.element_type<f16> : i32
  %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16_0) encoding(%dense_row_major_1)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
  %element_type_f16_2 = hal.element_type<f16> : i32
  %dense_row_major_3 = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16_2) encoding(%dense_row_major_3)
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
  %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
  %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
  %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %11 : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    %element_type_f16_0 = hal.element_type<f16> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    %element_type_f16_2 = hal.element_type<f16> : i32
    %dense_row_major_3 = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16_2) encoding(%dense_row_major_3)
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
    %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
    %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %11 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
  %element_type_f16_0 = hal.element_type<f16> : i32
  %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16_0) encoding(%dense_row_major_1)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
  %element_type_f16_2 = hal.element_type<f16> : i32
  %dense_row_major_3 = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16_2) encoding(%dense_row_major_3)
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
  %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
  %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
  %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %11 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
  %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
  %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
  %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %11 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
  %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
  %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
  %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %11 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
  %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
  %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
  %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %11 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
  %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
  %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
  %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %11 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
    %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
    %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %11 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
    %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
    %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %11 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
    %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
    %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %11 : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x128xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<128x512xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<512x512xf16> : index
    %7 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%6}
    %9 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%2[%c0 to %0 for %0], %5[%c0 to %3 for %3], %8[%c0 to %6 for %6]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}, !stream.resource<*>{%6}) -> %8{%6}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %10 : tensor<512x512xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %11 : !hal.buffer_view
  }
}


// -----// IR Dump After EncodeDeviceTensorsPass (iree-stream-encode-device-tensors) //----- //
stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
  stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
      %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
      %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
      %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
      flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
      return
    }
  }
}

// -----// IR Dump After EncodeHostTensorsPass (iree-stream-encode-host-tensors) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncResourcesPass (iree-stream-verify-lowering-to-async-resources) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWritePass (iree-stream-materialize-copy-on-write) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopiesPass (iree-stream-elide-async-copies) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocationsPass (iree-stream-emplace-allocations) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c131072} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%1[%c0 to %c131072 for %c131072], %3[%c0 to %c131072 for %c131072], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c131072}, !stream.resource<*>{%c131072}, !stream.resource<*>{%c524288}) -> %5{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%c524288}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After RefineUsagePass (iree-stream-refine-usage) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyAsyncAccessRangesPass (iree-stream-verify-async-access-ranges) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%0[%c0 to %c131072 for %c131072], %1[%c0 to %c131072 for %c131072], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %2{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleExecutionPass (iree-stream-schedule-execution) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
    %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ScheduleConcurrencyPass (iree-stream-schedule-concurrency) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
    %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After PropagateTimepointsPass (iree-stream-propagate-timepoints) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.immediate => !stream.timepoint
    %5 = stream.timepoint.immediate => !stream.timepoint
    %6 = stream.timepoint.join max(%3, %4, %5) => !stream.timepoint
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) await(%6) => with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
      %9 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
      stream.yield %9 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %7 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeBuiltinsPass (iree-stream-materialize-builtins) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.immediate => !stream.timepoint
    %5 = stream.timepoint.immediate => !stream.timepoint
    %6 = stream.timepoint.join max(%3, %4, %5) => !stream.timepoint
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) await(%6) => with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
      %9 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
      stream.yield %9 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %7 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
    %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
    %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
    %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
    %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
    %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
      %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
      %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
      %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncPass (iree-stream-verify-lowering-to-async) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) -> %2{%c524288} {
      %5 = stream.async.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg3[%c0 to %c131072 for %c131072], %arg4[%c0 to %c131072 for %c131072], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c131072}, !stream.resource<external>{%c131072}, !stream.resource<external>{%c524288}) -> %arg5{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleAllocationPass (iree-stream-schedule-allocation) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After PackConstantsPass (iree-stream-pack-constants) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %c0_0 = arith.constant 0 : index
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After LayoutSlicesPass (iree-stream-layout-slices) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %c0_0 = arith.constant 0 : index
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToCmdPass (iree-stream-verify-lowering-to-cmd) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepointsPass (iree-stream-elide-timepoints) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseDispatchBindingsPass (iree-stream-fuse-dispatch-bindings) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: index, %arg4: index, %arg5: index) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0, %c0, %c0 : index, index, index) {
        ro %arg3[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchArgumentsPass (iree-stream-annotate-dispatch-arguments) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.values = [0 : index]}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0, %c0, %c0 : index, index, index) {
        ro %arg3[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchAssumptionsPass (iree-stream-annotate-dispatch-assumptions) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.values = [0 : index]}) {
        %0:3 = util.assume.int 
            %arg3<umin = 0, umax = 0>, 
            %arg4<umin = 0, umax = 0>, 
            %arg5<umin = 0, umax = 0>
          : index, index, index
        %c0 = arith.constant 0 : index
        %1 = stream.binding.subspan %arg0[%0#0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %2 = stream.binding.subspan %arg1[%0#1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %3 = stream.binding.subspan %arg2[%0#2] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %6 = flow.dispatch.tensor.load %3, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %7 = linalg.matmul {compilation_info = #compilation} ins(%4, %5 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%6 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %7, %3, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0, %c0, %c0 : index, index, index) {
        ro %arg3[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After PackDispatchOperandsPass (iree-stream-pack-dispatch-operands) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg4 : i32 to i64
        %1 = arith.shli %0, %c32_i64 : i64
        %2 = arith.extui %arg3 : i32 to i64
        %3 = arith.ori %2, %1 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %c32_i64_0 = arith.constant 32 : i64
        %5 = arith.extui %arg6 : i32 to i64
        %6 = arith.shli %5, %c32_i64_0 : i64
        %7 = arith.extui %arg5 : i32 to i64
        %8 = arith.ori %7, %6 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %c32_i64_1 = arith.constant 32 : i64
        %10 = arith.extui %arg8 : i32 to i64
        %11 = arith.shli %10, %c32_i64_1 : i64
        %12 = arith.extui %arg7 : i32 to i64
        %13 = arith.ori %12, %11 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15:3 = util.assume.int 
            %4<umin = 0, umax = 0>, 
            %9<umin = 0, umax = 0>, 
            %14<umin = 0, umax = 0>
          : index, index, index
        %c0 = arith.constant 0 : index
        %16 = stream.binding.subspan %arg0[%15#0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %17 = stream.binding.subspan %arg1[%15#1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %18 = stream.binding.subspan %arg2[%15#2] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %20 = flow.dispatch.tensor.load %17, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %21 = flow.dispatch.tensor.load %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %22 = linalg.matmul {compilation_info = #compilation} ins(%19, %20 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%21 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %22, %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %c0_i64 = arith.constant 0 : i64
    %c0_i32 = arith.constant 0 : i32
    %c32_i64 = arith.constant 32 : i64
    %c0_i64_1 = arith.constant 0 : i64
    %c0_i32_2 = arith.constant 0 : i32
    %c0_i64_3 = arith.constant 0 : i64
    %c0_i32_4 = arith.constant 0 : i32
    %c32_i64_5 = arith.constant 32 : i64
    %c0_i64_6 = arith.constant 0 : i64
    %c0_i32_7 = arith.constant 0 : i32
    %c0_i64_8 = arith.constant 0 : i64
    %c0_i32_9 = arith.constant 0 : i32
    %c32_i64_10 = arith.constant 32 : i64
    %c0_i64_11 = arith.constant 0 : i64
    %c0_i32_12 = arith.constant 0 : i32
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32_2, %c0_i32_4, %c0_i32_7, %c0_i32_9, %c0_i32_12 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0_0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg4 : i32 to i64
        %1 = arith.shli %0, %c32_i64 : i64
        %2 = arith.extui %arg3 : i32 to i64
        %3 = arith.ori %2, %1 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg6 : i32 to i64
        %6 = arith.shli %5, %c32_i64 : i64
        %7 = arith.extui %arg5 : i32 to i64
        %8 = arith.ori %7, %6 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg8 : i32 to i64
        %11 = arith.shli %10, %c32_i64 : i64
        %12 = arith.extui %arg7 : i32 to i64
        %13 = arith.ori %12, %11 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15:3 = util.assume.int 
            %4<umin = 0, umax = 0>, 
            %9<umin = 0, umax = 0>, 
            %14<umin = 0, umax = 0>
          : index, index, index
        %16 = stream.binding.subspan %arg0[%15#0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %17 = stream.binding.subspan %arg1[%15#1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %18 = stream.binding.subspan %arg2[%15#2] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %20 = flow.dispatch.tensor.load %17, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %21 = flow.dispatch.tensor.load %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %22 = linalg.matmul {compilation_info = #compilation} ins(%19, %20 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%21 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %22, %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg4 : i32 to i64
        %1 = arith.shli %0, %c32_i64 : i64
        %2 = arith.extui %arg3 : i32 to i64
        %3 = arith.ori %2, %1 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg6 : i32 to i64
        %6 = arith.shli %5, %c32_i64 : i64
        %7 = arith.extui %arg5 : i32 to i64
        %8 = arith.ori %7, %6 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg8 : i32 to i64
        %11 = arith.shli %10, %c32_i64 : i64
        %12 = arith.extui %arg7 : i32 to i64
        %13 = arith.ori %12, %11 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15:3 = util.assume.int 
            %4<umin = 0, umax = 0>, 
            %9<umin = 0, umax = 0>, 
            %14<umin = 0, umax = 0>
          : index, index, index
        %16 = stream.binding.subspan %arg0[%15#0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %17 = stream.binding.subspan %arg1[%15#1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %18 = stream.binding.subspan %arg2[%15#2] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %20 = flow.dispatch.tensor.load %17, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %21 = flow.dispatch.tensor.load %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %22 = linalg.matmul {compilation_info = #compilation} ins(%19, %20 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%21 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %22, %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg4 : i32 to i64
        %1 = arith.shli %0, %c32_i64 : i64
        %2 = arith.extui %arg3 : i32 to i64
        %3 = arith.ori %2, %1 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg6 : i32 to i64
        %6 = arith.shli %5, %c32_i64 : i64
        %7 = arith.extui %arg5 : i32 to i64
        %8 = arith.ori %7, %6 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg8 : i32 to i64
        %11 = arith.shli %10, %c32_i64 : i64
        %12 = arith.extui %arg7 : i32 to i64
        %13 = arith.ori %12, %11 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15:3 = util.assume.int 
            %4<umin = 0, umax = 0>, 
            %9<umin = 0, umax = 0>, 
            %14<umin = 0, umax = 0>
          : index, index, index
        %16 = stream.binding.subspan %arg0[%15#0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %17 = stream.binding.subspan %arg1[%15#1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %18 = stream.binding.subspan %arg2[%15#2] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %20 = flow.dispatch.tensor.load %17, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %21 = flow.dispatch.tensor.load %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %22 = linalg.matmul {compilation_info = #compilation} ins(%19, %20 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%21 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %22, %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldUniformOperandsPass (iree-stream-fold-uniform-operands) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %c0_i32 : i32 to i64
        %1 = arith.shli %0, %c32_i64 : i64
        %2 = arith.extui %c0_i32 : i32 to i64
        %3 = arith.ori %2, %1 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %c0_i32 : i32 to i64
        %6 = arith.shli %5, %c32_i64 : i64
        %7 = arith.extui %c0_i32 : i32 to i64
        %8 = arith.ori %7, %6 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %c0_i32 : i32 to i64
        %11 = arith.shli %10, %c32_i64 : i64
        %12 = arith.extui %c0_i32 : i32 to i64
        %13 = arith.ori %12, %11 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15:3 = util.assume.int 
            %4<umin = 0, umax = 0>, 
            %9<umin = 0, umax = 0>, 
            %14<umin = 0, umax = 0>
          : index, index, index
        %16 = stream.binding.subspan %arg0[%15#0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %17 = stream.binding.subspan %arg1[%15#1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %18 = stream.binding.subspan %arg2[%15#2] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %20 = flow.dispatch.tensor.load %17, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %21 = flow.dispatch.tensor.load %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %22 = linalg.matmul {compilation_info = #compilation} ins(%19, %20 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%21 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %22, %18, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmetic (iree-util-optimize-int-arithmetic) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After AssignLegacyTargetDevicesPass (iree-hal-assign-legacy-target-devices) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTargetDevicesPass (iree-hal-materialize-target-devices) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDevicePromisesPass (iree-hal-resolve-device-promises) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDeviceAliasesPass (iree-hal-resolve-device-aliases) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
  %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
      ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
      ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
      rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  util.return %5 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  stream.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    stream.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
          %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
          %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
          %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
          flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
          return
        }
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#translation = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>
#compilation = #iree_codegen.compilation_info<lowering_config = #config, translation_info = #translation>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
          %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
          %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
          %6 = linalg.matmul {compilation_info = #compilation} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
          flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
          return
        }
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<512x128xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<128x512xf16> in !stream.resource<external>{%c131072}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<512x512xf16> in !stream.resource<external>{%c524288}
    %3 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c131072}, %1 as %arg4: !stream.resource<external>{%c131072}, %2 as %arg5: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 {
        ro %arg3[%c0 for %c131072] : !stream.resource<external>{%c131072},
        ro %arg4[%c0 for %c131072] : !stream.resource<external>{%c131072},
        rw %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c524288}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<512x512xf16> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    util.return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After GPUGeneralizeNamedOpsPass (iree-codegen-gpu-generalize-named-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After TypePropagationPass (iree-codegen-type-propagation) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOpsPass (iree-codegen-bubble-up-ordinal-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatchesPass (iree-codegen-bufferize-copy-only-dispatches) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After DecomposeSoftmaxPass (iree-codegen-decompose-softmax) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After MaterializeEncodingIntoNopPass (iree-codegen-materialize-encoding-into-nop) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatchesPass (iree-codegen-bufferize-copy-only-dispatches) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After BlockDynamicDimensionsPass (iree-codegen-block-dynamic-dimensions) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
  %6 = linalg.matmul {compilation_info = #iree_codegen.compilation_info<lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>, translation_info = <pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After MaterializeUserConfigsPass (iree-codegen-materialize-user-configs) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %6 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    return
  }
}

// -----// IR Dump After LLVMGPUSelectLoweringStrategyPass (iree-llvmgpu-select-lowering-strategy) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %6 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    return
  }
}

// -----// IR Dump After ConfigureTargetExecutableVariantsPass (iree-hal-configure-target-executable-variants) //----- //
hal.executable.variant public @cuda_nvptx_fb target(<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>) {
  hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) {
  ^bb0(%arg0: !hal.device):
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
      %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
      %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
      %6 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
      flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
      return
    }
  }
}

// -----// IR Dump After ConfigureExecutablesPass (iree-hal-configure-executables) //----- //
hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
  hal.executable.variant public @cuda_nvptx_fb target(<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>) {
    hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) {
    ^bb0(%arg0: !hal.device):
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      hal.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
        %c0 = arith.constant 0 : index
        %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
        %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
        %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
        %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
        %6 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
        flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
        return
      }
    }
  }
}

// -----// IR Dump After LowerExecutableUsingTransformDialectPass (iree-codegen-lower-executable-using-transform-dialect) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<512x128xf16>
    %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [128, 512], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x512xf16>
    %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<512x512xf16>
    %6 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%3, %4 : tensor<512x128xf16>, tensor<128x512xf16>) outs(%5 : tensor<512x512xf16>) -> tensor<512x512xf16>
    flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 512], strides = [1, 1] : tensor<512x512xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
    return
  }
}

// -----// IR Dump After TileAndDistributeToWorkgroupsPass (iree-codegen-tile-and-distribute-to-workgroups) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c32 = arith.constant 32 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%c32, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<?x128xf16>
  %5 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %6 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [128, %c32], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x?xf16>
  %7 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %9 = flow.dispatch.tensor.load %2, offsets = [%7, %8], sizes = [%c32, %c32], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<?x?xf16>
  %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%4, %6 : tensor<?x128xf16>, tensor<128x?xf16>) outs(%9 : tensor<?x?xf16>) -> tensor<?x?xf16>
  %11 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %12 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  flow.dispatch.tensor.store %10, %2, offsets = [%11, %12], sizes = [%c32, %c32], strides = [1, 1] : tensor<?x?xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c32 = arith.constant 32 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%c32, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<?x128xf16>
  %5 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %6 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [128, %c32], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x?xf16>
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%c32, %c32], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<?x?xf16>
  %8 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%4, %6 : tensor<?x128xf16>, tensor<128x?xf16>) outs(%7 : tensor<?x?xf16>) -> tensor<?x?xf16>
  flow.dispatch.tensor.store %8, %2, offsets = [%3, %5], sizes = [%c32, %c32], strides = [1, 1] : tensor<?x?xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After ConvertToDestinationPassingStylePass (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c32 = arith.constant 32 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%c32, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<?x128xf16>
  %5 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %6 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [128, %c32], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x?xf16>
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%c32, %c32], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<?x?xf16>
  %8 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%4, %6 : tensor<?x128xf16>, tensor<128x?xf16>) outs(%7 : tensor<?x?xf16>) -> tensor<?x?xf16>
  flow.dispatch.tensor.store %8, %2, offsets = [%3, %5], sizes = [%c32, %c32], strides = [1, 1] : tensor<?x?xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [32, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<32x128xf16>
  %5 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %6 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [128, 32], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x32xf16>
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<32x32xf16>
  %8 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%4, %6 : tensor<32x128xf16>, tensor<128x32xf16>) outs(%7 : tensor<32x32xf16>) -> tensor<32x32xf16>
  flow.dispatch.tensor.store %8, %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : tensor<32x32xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [32, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<32x128xf16>
  %5 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %6 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [128, 32], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x32xf16>
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<32x32xf16>
  %8 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%4, %6 : tensor<32x128xf16>, tensor<128x32xf16>) outs(%7 : tensor<32x32xf16>) -> tensor<32x32xf16>
  flow.dispatch.tensor.store %8, %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : tensor<32x32xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After EliminateEmptyTensorsPass (iree-eliminate-empty-tensors) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [32, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<32x128xf16>
  %5 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %6 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [128, 32], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x32xf16>
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<32x32xf16>
  %8 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%4, %6 : tensor<32x128xf16>, tensor<128x32xf16>) outs(%7 : tensor<32x32xf16>) -> tensor<32x32xf16>
  flow.dispatch.tensor.store %8, %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : tensor<32x32xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<512x128xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !flow.dispatch.tensor<readonly:tensor<128x512xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [32, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x128xf16>> -> tensor<32x128xf16>
  %5 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %6 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [128, 32], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<128x512xf16>> -> tensor<128x32xf16>
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x512xf16>> -> tensor<32x32xf16>
  %8 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%4, %6 : tensor<32x128xf16>, tensor<128x32xf16>) outs(%7 : tensor<32x32xf16>) -> tensor<32x32xf16>
  flow.dispatch.tensor.store %8, %2, offsets = [%3, %5], sizes = [32, 32], strides = [1, 1] : tensor<32x32xf16> -> !flow.dispatch.tensor<readwrite:tensor<512x512xf16>>
  return
}

// -----// IR Dump After IREEComprehensiveBufferizePass (iree-codegen-iree-comprehensive-bufferize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  %subview_2 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  memref.copy %subview_1, %subview_2 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  %subview_2 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  memref.copy %subview_1, %subview_2 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  %subview_2 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  memref.copy %subview_1, %subview_2 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  memref.copy %subview_1, %subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  return
}

// -----// IR Dump After CleanupBufferAllocViewPass (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_0 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview, %subview_0 : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  return
}

// -----// IR Dump After LLVMGPUTileAndDistributePass (iree-llvmgpu-tile-and-distribute) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c32 = arith.constant 32 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %subview_4 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_4, %alloc_0 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, #gpu.address_space<workgroup>>
    memref.copy %subview_5, %alloc {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    scf.for %arg1 = %5 to %c32 step %c32 {
      %6 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
      scf.for %arg2 = %6 to %c32 step %c32 {
        %subview_6 = memref.subview %alloc_0[%arg1, 0] [16, 16] [1, 1] : memref<32x16xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
        %subview_7 = memref.subview %alloc[0, %arg2] [16, 16] [1, 1] : memref<16x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
        %subview_8 = memref.subview %alloc_1[%arg1, %arg2] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_6, %subview_7 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_8 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
      }
    }
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After RemoveSingleIterationLoopPass (iree-codegen-remove-single-iteration-loop) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %subview_4 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_4, %alloc_0 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, #gpu.address_space<workgroup>>
    memref.copy %subview_5, %alloc {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %6 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_6 = memref.subview %alloc_0[%5, 0] [16, 16] [1, 1] : memref<32x16xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_7 = memref.subview %alloc[0, %6] [16, 16] [1, 1] : memref<16x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_8 = memref.subview %alloc_1[%5, %6] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_6, %subview_7 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_8 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After GPUMultiBufferingPass (iree-codegen-gpu-multi-buffering) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %6 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_5 = memref.subview %alloc[%6, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %7 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %8 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%7, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %8] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%7, %8] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_8, %subview_9 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_10 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %6 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_5 = memref.subview %alloc[%6, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %7 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %8 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%7, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %8] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%7, %8] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_8, %subview_9 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_10 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_5 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %6 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%6, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %7] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%6, %7] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_8, %subview_9 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_10 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After RemoveSingleIterationLoopPass (iree-codegen-remove-single-iteration-loop) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_5 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %6 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%6, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %7] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%6, %7] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_8, %subview_9 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_10 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After ReorderWorkgroupsPass (iree-codegen-reorder-workgroups) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_5 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %6 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%6, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %7] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%6, %7] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_8, %subview_9 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_10 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_5 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %6 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%6, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %7] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%6, %7] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_8, %subview_9 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_10 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_5 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %6 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%6, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %7] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%6, %7] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    linalg.matmul {__internal_linalg_transform__ = "vectorize", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 32, 16]]>} ins(%subview_8, %subview_9 : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_10 : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>)
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After LLVMGPUTensorCoreVectorizationPass (iree-llvmgpu-tensorcore-vectorization) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %cst = arith.constant 0.000000e+00 : f16
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %subview = memref.subview %0[%3, 0] [32, 128] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_2 = memref.subview %1[0, %4] [128, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview_3, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_4 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_5 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview[0, %arg0] [32, 16] [1, 1] : memref<32x128xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_7 = memref.subview %subview_2[%arg0, 0] [16, 32] [1, 1] : memref<128x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_6, %subview_4 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_7, %subview_5 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %6 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %subview_8 = memref.subview %subview_4[%6, 0] [16, 16] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_9 = memref.subview %subview_5[0, %7] [16, 16] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_10 = memref.subview %alloc_1[%6, %7] [16, 16] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %8 = vector.transfer_read %subview_8[%c0, %c0], %cst {in_bounds = [true, true]} : memref<16x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %9 = vector.transfer_read %subview_9[%c0, %c0], %cst {in_bounds = [true, true]} : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %10 = vector.transfer_read %subview_10[%c0, %c0], %cst {in_bounds = [true, true]} : memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %8, %9, %10 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    vector.transfer_write %11, %subview_10[%c0, %c0] {in_bounds = [true, true]} : vector<16x16xf16>, memref<16x16xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
  }
  gpu.barrier
  memref.copy %alloc_1, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %cst = arith.constant 0.000000e+00 : f16
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_2 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_3 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %6 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %subview_4 = memref.subview %0[%6, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
    %subview_5 = memref.subview %1[%arg0, %7] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_4, %subview_2 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_5, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %8 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %9 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %10 = vector.transfer_read %alloc_0[%8, %9, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %12 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %13 = vector.transfer_read %alloc[%11, %c0, %12], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %14 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %15 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %16 = vector.transfer_read %alloc_1[%14, %15], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %13, %16 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    %18 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %19 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    vector.transfer_write %17, %alloc_1[%18, %19] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  }
  gpu.barrier
  memref.copy %alloc_1, %subview {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %cst = arith.constant 0.000000e+00 : f16
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c128 step %c16 {
    %5 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_2 = memref.subview %alloc_0[%5, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_3 = memref.subview %alloc[%5, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_4 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_4, %subview_2 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_5, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %6 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %7 = vector.transfer_read %alloc_0[%5, %6, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %8 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
    %9 = vector.transfer_read %alloc[%5, %c0, %8], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %10 = vector.transfer_read %alloc_1[%6, %8], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %7, %9, %10 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    vector.transfer_write %11, %alloc_1[%6, %8] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  }
  gpu.barrier
  memref.copy %alloc_1, %subview {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After OptimizeVectorTransferPass (iree-codegen-optimize-vector-transfer) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %cst = arith.constant 0.000000e+00 : f16
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %6 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
  %7 = vector.transfer_read %alloc_1[%5, %6], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %8 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %7) -> (vector<16x16xf16>) {
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_2 = memref.subview %alloc_0[%9, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_3 = memref.subview %alloc[%9, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_4 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_4, %subview_2 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_5, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %10 = vector.transfer_read %alloc_0[%9, %5, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.transfer_read %alloc[%9, %c0, %6], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %11, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %12 : vector<16x16xf16>
  }
  vector.transfer_write %8, %alloc_1[%5, %6] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  memref.copy %alloc_1, %subview {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After OptimizeTensorInsertExtractSlicesPass (iree-codegen-optimize-tensor-insert-extract-slices) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %cst = arith.constant 0.000000e+00 : f16
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  memref.copy %subview, %alloc_1 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %6 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
  %7 = vector.transfer_read %alloc_1[%5, %6], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %8 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %7) -> (vector<16x16xf16>) {
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_2 = memref.subview %alloc_0[%9, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_3 = memref.subview %alloc[%9, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_4 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    memref.copy %subview_4, %subview_2 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    memref.copy %subview_5, %subview_3 {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    gpu.barrier
    %10 = vector.transfer_read %alloc_0[%9, %5, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.transfer_read %alloc[%9, %c0, %6], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %11, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %12 : vector<16x16xf16>
  }
  vector.transfer_write %8, %alloc_1[%5, %6] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  memref.copy %alloc_1, %subview {__internal_linalg_transform__ = "copy_to_workgroup_memory"} : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %cst = arith.constant 0.000000e+00 : f16
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %alloc = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%alloc_1 : memref<32x32xf16, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "copy_to_workgroup_memory"} {
  ^bb0(%in: f16, %out: f16):
    linalg.yield %in : f16
  }
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %6 = affine.apply affine_map<(d0) -> ((d0 floordiv 32) * 16)>(%thread_id_x)
  %7 = vector.transfer_read %alloc_1[%5, %6], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %8 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %7) -> (vector<16x16xf16>) {
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_2 = memref.subview %alloc_0[%9, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_3 = memref.subview %alloc[%9, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_4 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_4 : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "copy_to_workgroup_memory"} {
    ^bb0(%in: f16, %out: f16):
      linalg.yield %in : f16
    }
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_5 : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_3 : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "copy_to_workgroup_memory"} {
    ^bb0(%in: f16, %out: f16):
      linalg.yield %in : f16
    }
    gpu.barrier
    %10 = vector.transfer_read %alloc_0[%9, %5, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.transfer_read %alloc[%9, %c0, %6], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %11, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %12 : vector<16x16xf16>
  }
  vector.transfer_write %8, %alloc_1[%5, %6] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%alloc_1 : memref<32x32xf16, #gpu.address_space<workgroup>>) outs(%subview : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "copy_to_workgroup_memory"} {
  ^bb0(%in: f16, %out: f16):
    linalg.yield %in : f16
  }
  gpu.barrier
  return
}

// -----// IR Dump After GPUDistributeSharedMemoryCopyPass (iree-codegen-gpu-distribute-shared-memory-copy) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_6 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_7 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_6 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_7 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %thread_id_x_2 = gpu.thread_id  x
  %thread_id_y_3 = gpu.thread_id  y
  %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y_3]
  %6 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x_2]
  %7 = vector.transfer_read %alloc[%5, %6], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %8 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %7) -> (vector<16x16xf16>) {
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_6 = memref.subview %alloc_0[%9, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_7 = memref.subview %alloc_1[%9, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_8 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_9 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    %thread_id_x_10 = gpu.thread_id  x
    %thread_id_y_11 = gpu.thread_id  y
    scf.for %arg2 = %thread_id_y_11 to %c32 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x_10]
      scf.for %arg3 = %13 to %c16 step %c512 {
        %subview_14 = memref.subview %subview_8[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_15 = memref.subview %subview_6[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_14 : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_15 : memref<1x8xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    %thread_id_x_12 = gpu.thread_id  x
    %thread_id_y_13 = gpu.thread_id  y
    scf.for %arg2 = %thread_id_y_13 to %c16 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x_12]
      scf.for %arg3 = %13 to %c32 step %c512 {
        %subview_14 = memref.subview %subview_9[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_15 = memref.subview %subview_7[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_14 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_15 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %10 = vector.transfer_read %alloc_0[%9, %5, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.transfer_read %alloc_1[%9, %c0, %6], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %11, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %12 : vector<16x16xf16>
  }
  vector.transfer_write %8, %alloc[%5, %6] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  %thread_id_x_4 = gpu.thread_id  x
  %thread_id_y_5 = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y_5 to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x_4]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_6 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
      %subview_7 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_6 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_7 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_6 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_7 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_6 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_7 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %thread_id_x_2 = gpu.thread_id  x
  %thread_id_y_3 = gpu.thread_id  y
  %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y_3]
  %6 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x_2]
  %7 = vector.transfer_read %alloc[%5, %6], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %8 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %7) -> (vector<16x16xf16>) {
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_6 = memref.subview %alloc_0[%9, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_7 = memref.subview %alloc_1[%9, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_8 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_9 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    %thread_id_x_10 = gpu.thread_id  x
    %thread_id_y_11 = gpu.thread_id  y
    scf.for %arg2 = %thread_id_y_11 to %c32 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x_10]
      scf.for %arg3 = %13 to %c16 step %c512 {
        %subview_14 = memref.subview %subview_8[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_15 = memref.subview %subview_6[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_14 : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_15 : memref<1x8xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    %thread_id_x_12 = gpu.thread_id  x
    %thread_id_y_13 = gpu.thread_id  y
    scf.for %arg2 = %thread_id_y_13 to %c16 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x_12]
      scf.for %arg3 = %13 to %c32 step %c512 {
        %subview_14 = memref.subview %subview_9[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_15 = memref.subview %subview_7[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_14 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_15 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %10 = vector.transfer_read %alloc_0[%9, %5, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.transfer_read %alloc_1[%9, %c0, %6], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %11, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %12 : vector<16x16xf16>
  }
  vector.transfer_write %8, %alloc[%5, %6] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  %thread_id_x_4 = gpu.thread_id  x
  %thread_id_y_5 = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y_5 to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x_4]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_6 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
      %subview_7 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_6 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_7 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x32xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x16xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x32xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_2 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_3 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_3 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %6 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %7 = vector.transfer_read %alloc[%5, %6], %cst {in_bounds = [true, true]} : memref<32x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %8 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %7) -> (vector<16x16xf16>) {
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_2 = memref.subview %alloc_0[%9, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_3 = memref.subview %alloc_1[%9, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_4 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %13 to %c16 step %c512 {
        %subview_6 = memref.subview %subview_4[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_7 = memref.subview %subview_2[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_6 : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_7 : memref<1x8xf16, strided<[16, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %13 to %c32 step %c512 {
        %subview_6 = memref.subview %subview_5[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_7 = memref.subview %subview_3[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_6 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_7 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %10 = vector.transfer_read %alloc_0[%9, %5, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.transfer_read %alloc_1[%9, %c0, %6], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %11, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %12 : vector<16x16xf16>
  }
  vector.transfer_write %8, %alloc[%5, %6] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>
      %subview_3 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_2 : memref<1x8xf16, strided<[32, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_3 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After GPUReduceBankConflictsPass (iree-codegen-gpu-reduce-bank-conflicts) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %subview = memref.subview %alloc[0, 0] [32, 32] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<32x32xf16, strided<[40, 1]>, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %subview_1 = memref.subview %alloc_0[0, 0, 0] [3, 32, 16] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<3x32x16xf16, strided<[768, 24, 1]>, #gpu.address_space<workgroup>>
  %alloc_2 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %subview_3 = memref.subview %alloc_2[0, 0, 0] [3, 16, 32] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<3x16x32xf16, strided<[640, 40, 1]>, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_x]
  %subview_4 = memref.subview %2[%3, %4] [32, 32] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_5 = memref.subview %subview_4[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_6 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[40, 1]>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_5 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_6 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %5 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %6 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %7 = vector.transfer_read %subview[%5, %6], %cst {in_bounds = [true, true]} : memref<32x32xf16, strided<[40, 1]>, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %8 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %7) -> (vector<16x16xf16>) {
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %subview_5 = memref.subview %subview_1[%9, 0, 0] [1, 32, 16] [1, 1, 1] : memref<3x32x16xf16, strided<[768, 24, 1]>, #gpu.address_space<workgroup>> to memref<32x16xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_6 = memref.subview %subview_3[%9, 0, 0] [1, 16, 32] [1, 1, 1] : memref<3x16x32xf16, strided<[640, 40, 1]>, #gpu.address_space<workgroup>> to memref<16x32xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
    %subview_7 = memref.subview %0[%3, %arg0] [32, 16] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_8 = memref.subview %1[%arg0, %4] [16, 32] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %13 to %c16 step %c512 {
        %subview_9 = memref.subview %subview_7[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_10 = memref.subview %subview_5[%arg2, %arg3] [1, 8] [1, 1] : memref<32x16xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_9 : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_10 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %13 to %c32 step %c512 {
        %subview_9 = memref.subview %subview_8[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_10 = memref.subview %subview_6[%arg2, %arg3] [1, 8] [1, 1] : memref<16x32xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_9 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_10 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %10 = vector.transfer_read %subview_1[%9, %5, %c0], %cst {in_bounds = [true, true]} : memref<3x32x16xf16, strided<[768, 24, 1]>, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = vector.transfer_read %subview_3[%9, %c0, %6], %cst {in_bounds = [true, true]} : memref<3x16x32xf16, strided<[640, 40, 1]>, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %11, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %12 : vector<16x16xf16>
  }
  vector.transfer_write %8, %subview[%5, %6] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x32xf16, strided<[40, 1]>, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview_5 = memref.subview %subview[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[40, 1]>, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %subview_6 = memref.subview %subview_4[%arg0, %arg1] [1, 8] [1, 1] : memref<32x32xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_5 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_6 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %10 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %11 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%10, %11] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = vector.transfer_read %alloc[%3, %4], %cst {in_bounds = [true, true]} : memref<32x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (vector<16x16xf16>) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %16 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %16 to %c16 step %c512 {
        %17 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%17, %18] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %19 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%19, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %16 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %16 to %c32 step %c512 {
        %17 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%17, %18] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %19 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%19, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %10 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %11 = vector.transfer_read %alloc_0[%9, %10, %c0], %cst {in_bounds = [true, true]} : memref<3x32x24xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %13 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
    %14 = vector.transfer_read %alloc_1[%12, %c0, %13], %cst {in_bounds = [true, true]} : memref<3x16x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %15 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %11, %14, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %15 : vector<16x16xf16>
  }
  %7 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %8 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  vector.transfer_write %6, %alloc[%7, %8] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %10 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %11 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%10, %11] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %10 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %11 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%10, %11] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = vector.transfer_read %alloc[%3, %4], %cst {in_bounds = [true, true]} : memref<32x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (vector<16x16xf16>) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %16 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %16 to %c16 step %c512 {
        %17 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%17, %18] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %19 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%19, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %16 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %16 to %c32 step %c512 {
        %17 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%17, %18] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %19 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%19, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %9 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %10 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %11 = vector.transfer_read %alloc_0[%9, %10, %c0], %cst {in_bounds = [true, true]} : memref<3x32x24xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %12 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %13 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
    %14 = vector.transfer_read %alloc_1[%12, %c0, %13], %cst {in_bounds = [true, true]} : memref<3x16x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %15 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %11, %14, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %15 : vector<16x16xf16>
  }
  %7 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %8 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  vector.transfer_write %6, %alloc[%7, %8] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %9 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %9 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %10 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %11 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%10, %11] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = vector.transfer_read %alloc[%3, %4], %cst {in_bounds = [true, true]} : memref<32x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (vector<16x16xf16>) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = vector.transfer_read %alloc_0[%7, %3, %c0], %cst {in_bounds = [true, true]} : memref<3x32x24xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %9 = vector.transfer_read %alloc_1[%7, %c0, %4], %cst {in_bounds = [true, true]} : memref<3x16x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %10 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %8, %9, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %10 : vector<16x16xf16>
  }
  vector.transfer_write %6, %alloc[%3, %4] {in_bounds = [true, true]} : vector<16x16xf16>, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After LLVMGPUVectorToGPUPass (iree-llvmgpu-vector-to-gpu) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %8 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %8 to %c32 step %c512 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %10 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%9, %10] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %6 = vector.transfer_read %alloc[%3, %4], %cst {in_bounds = [true, true]} : memref<32x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
  %7:2 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %6, %arg2 = %5) -> (vector<16x16xf16>, !gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg3 = %thread_id_y to %c32 step %c2 {
      %15 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg4 = %15 to %c16 step %c512 {
        %16 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_y]
        %17 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg4]
        %subview = memref.subview %0[%16, %17] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %18 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%18, %arg3, %arg4] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg3 = %thread_id_y to %c16 step %c2 {
      %15 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg4 = %15 to %c32 step %c512 {
        %16 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %17 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg4, %workgroup_id_x]
        %subview = memref.subview %1[%16, %17] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %18 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%18, %arg3, %arg4] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %8 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %9 = gpu.subgroup_mma_load_matrix %alloc_0[%8, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %10 = vector.transfer_read %alloc_0[%8, %3, %c0], %cst {in_bounds = [true, true]} : memref<3x32x24xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %11 = gpu.subgroup_mma_load_matrix %alloc_1[%8, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %12 = vector.transfer_read %alloc_1[%8, %c0, %4], %cst {in_bounds = [true, true]} : memref<3x16x40xf16, #gpu.address_space<workgroup>>, vector<16x16xf16>
    %13 = gpu.subgroup_mma_compute %9, %11, %arg2 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %10, %12, %arg1 : vector<16x16xf16>, vector<16x16xf16> into vector<16x16xf16>
    scf.yield %6, %13 : vector<16x16xf16>, !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %7#1, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %8 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %8 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %10 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%9, %10] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After GPUPipeliningPass (iree-codegen-gpu-pipelining) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After LLVMGPUPackSharedMemoryAllocPass (iree-llvmgpu-pack-shared-memory-alloc) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After LLVMGPULowerExecutableTargetPass (iree-llvmgpu-lower-executable-target) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUMatmulTensorCore workgroup_size = [64, 2, 1], {pipeline_depth = 3 : i64, store_stage = 1 : i64}>} {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ReconcileTranslationInfoPass (iree-codegen-reconcile-translation-info) //----- //
hal.executable.variant public @cuda_nvptx_fb target(<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>) {
  hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) attributes {workgroup_size = [64 : index, 2 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %c16 = arith.constant 16 : index
    %c1 = arith.constant 1 : index
    hal.return %c16, %c16, %c1 : index, index, index
  }
  builtin.module {
    func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
      %c512 = arith.constant 512 : index
      %c2 = arith.constant 2 : index
      %c32 = arith.constant 32 : index
      %c16 = arith.constant 16 : index
      %c128 = arith.constant 128 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
      memref.assume_alignment %0, 64 : memref<512x128xf16, #hal.descriptor_type<storage_buffer>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
      memref.assume_alignment %1, 64 : memref<128x512xf16, #hal.descriptor_type<storage_buffer>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
      memref.assume_alignment %2, 64 : memref<512x512xf16, #hal.descriptor_type<storage_buffer>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      gpu.barrier
      %thread_id_x = gpu.thread_id  x
      %thread_id_y = gpu.thread_id  y
      %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
      scf.for %arg0 = %thread_id_y to %c32 step %c2 {
        %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg1 = %7 to %c32 step %c512 {
          %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
          %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
          %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      gpu.barrier
      %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
      %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
      %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
      %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
      %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
      %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
        gpu.barrier
        scf.for %arg2 = %thread_id_y to %c32 step %c2 {
          %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
          scf.for %arg3 = %11 to %c16 step %c512 {
            %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
            %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
            %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
            %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
            linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
            ^bb0(%in: f16, %out: f16):
              linalg.yield %in : f16
            }
          }
        }
        scf.for %arg2 = %thread_id_y to %c16 step %c2 {
          %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
          scf.for %arg3 = %11 to %c32 step %c512 {
            %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
            %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
            %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
            %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
            linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
            ^bb0(%in: f16, %out: f16):
              linalg.yield %in : f16
            }
          }
        }
        gpu.barrier
        %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
        %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
        %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
        scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
      }
      gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
      gpu.barrier
      scf.for %arg0 = %thread_id_y to %c32 step %c2 {
        %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg1 = %7 to %c32 step %c512 {
          %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
          %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
          %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #hal.descriptor_type<storage_buffer>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      gpu.barrier
      return
    }
  }
}

// -----// IR Dump After ConvertHALDescriptorTypeToGPUAddressSpacePass (iree-codegen-convert-hal-descriptor-type-to-gpu-address-space) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
    %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
      gpu.barrier
      scf.for %arg2 = %thread_id_y to %c32 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c16 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
          %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      scf.for %arg2 = %thread_id_y to %c16 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c32 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
          %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      gpu.barrier
      %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
      %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
      %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
      %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
      scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
    }
    gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
    %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
      gpu.barrier
      scf.for %arg2 = %thread_id_y to %c32 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c16 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
          %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      scf.for %arg2 = %thread_id_y to %c16 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c32 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
          %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      gpu.barrier
      %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
      %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
      %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
      %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
      scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
    }
    gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
    %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
      gpu.barrier
      scf.for %arg2 = %thread_id_y to %c32 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c16 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
          %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      scf.for %arg2 = %thread_id_y to %c16 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c32 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
          %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      gpu.barrier
      %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
      %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
      %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
      %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
      scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
    }
    gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    return
  }
}

// -----// IR Dump After LowerUKernelOpsToCallsPass (iree-codegen-lower-ukernel-ops-to-calls) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
    %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
    %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
      gpu.barrier
      scf.for %arg2 = %thread_id_y to %c32 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c16 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
          %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      scf.for %arg2 = %thread_id_y to %c16 step %c2 {
        %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
        scf.for %arg3 = %11 to %c32 step %c512 {
          %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
          %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
          %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
          %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
          ^bb0(%in: f16, %out: f16):
            linalg.yield %in : f16
          }
        }
      }
      gpu.barrier
      %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
      %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
      %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
      %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
      scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
    }
    gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg1 = %7 to %c32 step %c512 {
        %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
        %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
        %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    return
  }
}

// -----// IR Dump After LinalgExtToLoopsPass (iree-linalg-ext-to-loops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) outs(%subview_2 : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>) outs(%subview_2 : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>) attrs =  {__internal_linalg_transform__ = "vectorize"} {
      ^bb0(%in: f16, %out: f16):
        linalg.yield %in : f16
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ConvertLinalgToLoopsPass (convert-linalg-to-loops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c1 step %c1 {
        scf.for %arg3 = %c0 to %c8 step %c1 {
          %10 = memref.load %subview[%arg2, %arg3] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %10, %subview_2[%arg2, %arg3] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c1 step %c1 {
          scf.for %arg5 = %c0 to %c8 step %c1 {
            %15 = memref.load %subview[%arg4, %arg5] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
            memref.store %15, %subview_2[%arg4, %arg5] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
          }
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c1 step %c1 {
          scf.for %arg5 = %c0 to %c8 step %c1 {
            %15 = memref.load %subview[%arg4, %arg5] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
            memref.store %15, %subview_2[%arg4, %arg5] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          }
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c1 step %c1 {
        scf.for %arg3 = %c0 to %c8 step %c1 {
          %10 = memref.load %subview[%arg2, %arg3] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          memref.store %10, %subview_2[%arg2, %arg3] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        }
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %10 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %10, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %15 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %15, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %15 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %15, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %10 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %10, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %10 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %10, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %15 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %15, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %15 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %15, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %10 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %10, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After PadDynamicAllocPass (iree-codegen-pad-dynamic-alloc) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %10 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %10, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = affine.apply affine_map<()[s0] -> (s0 * 16)>()[%thread_id_y]
  %4 = affine.apply affine_map<()[s0] -> ((s0 floordiv 32) * 16)>()[%thread_id_x]
  %5 = gpu.subgroup_mma_load_matrix %alloc[%3, %4] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %6 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %5) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c16 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg2, %workgroup_id_y]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg3]
        %subview = memref.subview %0[%12, %13] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_0[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %15 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %15, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %11 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
      scf.for %arg3 = %11 to %c32 step %c512 {
        %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg2]
        %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg3, %workgroup_id_x]
        %subview = memref.subview %1[%12, %13] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %14 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
        %subview_2 = memref.subview %alloc_1[%14, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %15 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %15, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %7 = affine.apply affine_map<(d0) -> ((d0 floordiv 16) mod 3)>(%arg0)
    %8 = gpu.subgroup_mma_load_matrix %alloc_0[%7, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %9 = gpu.subgroup_mma_load_matrix %alloc_1[%7, %c0, %4] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %10 = gpu.subgroup_mma_compute %8, %9, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %10 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %6, %alloc[%3, %4] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%thread_id_x]
    scf.for %arg1 = %7 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg0, %workgroup_id_y]
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 32)>()[%arg1, %workgroup_id_x]
      %subview_2 = memref.subview %2[%8, %9] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %10 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %10, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %c8_6 = arith.constant 8 : index
    %13 = arith.muli %thread_id_x, %c8_6 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %c32_7 = arith.constant 32 : index
      %14 = arith.muli %workgroup_id_y, %c32_7 : index
      %15 = arith.addi %arg0, %14 : index
      %c32_8 = arith.constant 32 : index
      %16 = arith.muli %workgroup_id_x, %c32_8 : index
      %17 = arith.addi %arg1, %16 : index
      %subview = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_9 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %18, %subview_9[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %c16_0 = arith.constant 16 : index
  %3 = arith.muli %thread_id_y, %c16_0 : index
  %c32_1 = arith.constant 32 : index
  %c0_2 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0_2 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32_1 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %c16_3 = arith.constant 16 : index
  %10 = arith.muli %9, %c16_3 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_4 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_5 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %c8_10 = arith.constant 8 : index
      %26 = arith.muli %thread_id_x, %c8_10 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %c32_11 = arith.constant 32 : index
        %27 = arith.muli %workgroup_id_y, %c32_11 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %subview = memref.subview %0[%28, %29] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %c16_12 = arith.constant 16 : index
        %c0_13 = arith.constant 0 : index
        %c-1_14 = arith.constant -1 : index
        %30 = arith.cmpi slt, %arg0, %c0_13 : index
        %31 = arith.subi %c-1_14, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16_12 : index
        %34 = arith.subi %c-1_14, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %c3_15 = arith.constant 3 : index
        %36 = arith.remsi %35, %c3_15 : index
        %c0_16 = arith.constant 0 : index
        %37 = arith.cmpi slt, %36, %c0_16 : index
        %38 = arith.addi %36, %c3_15 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_17 = memref.subview %alloc_4[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_17[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %c8_10 = arith.constant 8 : index
      %26 = arith.muli %thread_id_x, %c8_10 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %c32_11 = arith.constant 32 : index
        %28 = arith.muli %workgroup_id_x, %c32_11 : index
        %29 = arith.addi %arg3, %28 : index
        %subview = memref.subview %1[%27, %29] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %c16_12 = arith.constant 16 : index
        %c0_13 = arith.constant 0 : index
        %c-1_14 = arith.constant -1 : index
        %30 = arith.cmpi slt, %arg0, %c0_13 : index
        %31 = arith.subi %c-1_14, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16_12 : index
        %34 = arith.subi %c-1_14, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %c3_15 = arith.constant 3 : index
        %36 = arith.remsi %35, %c3_15 : index
        %c0_16 = arith.constant 0 : index
        %37 = arith.cmpi slt, %36, %c0_16 : index
        %38 = arith.addi %36, %c3_15 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_17 = memref.subview %alloc_5[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_17[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %c16_6 = arith.constant 16 : index
    %c0_7 = arith.constant 0 : index
    %c-1_8 = arith.constant -1 : index
    %13 = arith.cmpi slt, %arg0, %c0_7 : index
    %14 = arith.subi %c-1_8, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16_6 : index
    %17 = arith.subi %c-1_8, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %c3 = arith.constant 3 : index
    %19 = arith.remsi %18, %c3 : index
    %c0_9 = arith.constant 0 : index
    %20 = arith.cmpi slt, %19, %c0_9 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_4[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_5[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %c8_6 = arith.constant 8 : index
    %13 = arith.muli %thread_id_x, %c8_6 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %c32_7 = arith.constant 32 : index
      %14 = arith.muli %workgroup_id_y, %c32_7 : index
      %15 = arith.addi %arg0, %14 : index
      %c32_8 = arith.constant 32 : index
      %16 = arith.muli %workgroup_id_x, %c32_8 : index
      %17 = arith.addi %arg1, %16 : index
      %subview_9 = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %18, %subview_9[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      %subview = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %subview = memref.subview %0[%28, %29] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_2 = memref.subview %alloc_0[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %subview = memref.subview %1[%27, %29] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_2 = memref.subview %alloc_1[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      %subview_2 = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      %subview = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %subview = memref.subview %0[%28, %29] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_2 = memref.subview %alloc_0[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %subview = memref.subview %1[%27, %29] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_2 = memref.subview %alloc_1[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      %subview_2 = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After OneShotBufferize (one-shot-bufferize) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c3 = arith.constant 3 : index
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1 = arith.constant 1 : index
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %13 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg1 = %13 to %c32 step %c512 {
        %14 = arith.muli %workgroup_id_y, %c32 : index
        %15 = arith.addi %arg0, %14 : index
        %16 = arith.muli %workgroup_id_x, %c32 : index
        %17 = arith.addi %arg1, %16 : index
        %subview = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg2 = %c0 to %c8 step %c1 {
          %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %3 = arith.muli %thread_id_y, %c16 : index
    %4 = arith.cmpi slt, %thread_id_x, %c0 : index
    %5 = arith.subi %c-1, %thread_id_x : index
    %6 = arith.select %4, %5, %thread_id_x : index
    %7 = arith.divsi %6, %c32 : index
    %8 = arith.subi %c-1, %7 : index
    %9 = arith.select %4, %8, %7 : index
    %10 = arith.muli %9, %c16 : index
    %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
      gpu.barrier
      scf.for %arg2 = %thread_id_y to %c32 step %c2 {
        %26 = arith.muli %thread_id_x, %c8 : index
        scf.for %arg3 = %26 to %c16 step %c512 {
          %27 = arith.muli %workgroup_id_y, %c32 : index
          %28 = arith.addi %arg2, %27 : index
          %29 = arith.addi %arg0, %arg3 : index
          %subview = memref.subview %0[%28, %29] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %30 = arith.cmpi slt, %arg0, %c0 : index
          %31 = arith.subi %c-1, %arg0 : index
          %32 = arith.select %30, %31, %arg0 : index
          %33 = arith.divsi %32, %c16 : index
          %34 = arith.subi %c-1, %33 : index
          %35 = arith.select %30, %34, %33 : index
          %36 = arith.remsi %35, %c3 : index
          %37 = arith.cmpi slt, %36, %c0 : index
          %38 = arith.addi %36, %c3 : index
          %39 = arith.select %37, %38, %36 : index
          %subview_2 = memref.subview %alloc_0[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
          scf.for %arg4 = %c0 to %c8 step %c1 {
            %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
            memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
          }
        }
      }
      scf.for %arg2 = %thread_id_y to %c16 step %c2 {
        %26 = arith.muli %thread_id_x, %c8 : index
        scf.for %arg3 = %26 to %c32 step %c512 {
          %27 = arith.addi %arg0, %arg2 : index
          %28 = arith.muli %workgroup_id_x, %c32 : index
          %29 = arith.addi %arg3, %28 : index
          %subview = memref.subview %1[%27, %29] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %30 = arith.cmpi slt, %arg0, %c0 : index
          %31 = arith.subi %c-1, %arg0 : index
          %32 = arith.select %30, %31, %arg0 : index
          %33 = arith.divsi %32, %c16 : index
          %34 = arith.subi %c-1, %33 : index
          %35 = arith.select %30, %34, %33 : index
          %36 = arith.remsi %35, %c3 : index
          %37 = arith.cmpi slt, %36, %c0 : index
          %38 = arith.addi %36, %c3 : index
          %39 = arith.select %37, %38, %36 : index
          %subview_2 = memref.subview %alloc_1[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          scf.for %arg4 = %c0 to %c8 step %c1 {
            %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
            memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          }
        }
      }
      gpu.barrier
      %13 = arith.cmpi slt, %arg0, %c0 : index
      %14 = arith.subi %c-1, %arg0 : index
      %15 = arith.select %13, %14, %arg0 : index
      %16 = arith.divsi %15, %c16 : index
      %17 = arith.subi %c-1, %16 : index
      %18 = arith.select %13, %17, %16 : index
      %19 = arith.remsi %18, %c3 : index
      %20 = arith.cmpi slt, %19, %c0 : index
      %21 = arith.addi %19, %c3 : index
      %22 = arith.select %20, %21, %19 : index
      %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
      %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
      %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
      scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
    }
    gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    scf.for %arg0 = %thread_id_y to %c32 step %c2 {
      %13 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg1 = %13 to %c32 step %c512 {
        %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %14 = arith.muli %workgroup_id_y, %c32 : index
        %15 = arith.addi %arg0, %14 : index
        %16 = arith.muli %workgroup_id_x, %c32 : index
        %17 = arith.addi %arg1, %16 : index
        %subview_2 = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        scf.for %arg2 = %c0 to %c8 step %c1 {
          %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
          memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        }
      }
    }
    gpu.barrier
    return
  }
}

// -----// IR Dump After FoldTensorExtractOpPass (iree-codegen-fold-tensor-extract-op) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      %subview = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      %subview_2 = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %subview = memref.subview %0[%28, %29] [1, 8] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_2 = memref.subview %alloc_0[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %subview = memref.subview %1[%27, %29] [1, 8] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        %subview_2 = memref.subview %alloc_1[%39, %arg2, %arg3] [1, 1, 8] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = memref.load %subview[%c0, %arg4] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          memref.store %40, %subview_2[%c0, %arg4] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %subview = memref.subview %alloc[%arg0, %arg1] [1, 8] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      %subview_2 = memref.subview %2[%15, %17] [1, 8] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = memref.load %subview[%c0, %arg2] : memref<1x8xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %18, %subview_2[%c0, %arg2] : memref<1x8xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After LLVMGPUVectorLoweringPass (iree-llvmgpu-vector-lowering) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ExpandGPUOpsPass (iree-codegen-expand-gpu-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ExtractAddressComputationGPUPass (extract-address-computation-gpu) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %subview = memref.subview %2[%15, %18] [1, 1] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %19 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %subview_2 = memref.subview %alloc[%arg0, %20] [1, 1] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %19, %subview_2[%c0, %c0] : memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %subview = memref.subview %0[%28, %40] [1, 1] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %41 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          %subview_2 = memref.subview %alloc_0[%39, %arg2, %42] [1, 1, 1] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x1x1xf16, strided<[768, 24, 1], offset: ?>, #gpu.address_space<workgroup>>
          memref.store %41, %subview_2[%c0, %c0, %c0] : memref<1x1x1xf16, strided<[768, 24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %subview = memref.subview %1[%27, %40] [1, 1] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %41 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          %subview_2 = memref.subview %alloc_1[%39, %arg2, %42] [1, 1, 1] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x1x1xf16, strided<[640, 40, 1], offset: ?>, #gpu.address_space<workgroup>>
          memref.store %41, %subview_2[%c0, %c0, %c0] : memref<1x1x1xf16, strided<[640, 40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %subview = memref.subview %alloc[%arg0, %18] [1, 1] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %19 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %subview_2 = memref.subview %2[%15, %20] [1, 1] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %19, %subview_2[%c0, %c0] : memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %subview = memref.subview %2[%15, %18] [1, 1] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %19 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %subview_2 = memref.subview %alloc[%arg0, %20] [1, 1] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        memref.store %19, %subview_2[%c0, %c0] : memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %subview = memref.subview %0[%28, %40] [1, 1] [1, 1] : memref<512x128xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %41 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[128, 1], offset: ?>, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          %subview_2 = memref.subview %alloc_0[%39, %arg2, %42] [1, 1, 1] [1, 1, 1] : memref<3x32x24xf16, #gpu.address_space<workgroup>> to memref<1x1x1xf16, strided<[768, 24, 1], offset: ?>, #gpu.address_space<workgroup>>
          memref.store %41, %subview_2[%c0, %c0, %c0] : memref<1x1x1xf16, strided<[768, 24, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %subview = memref.subview %1[%27, %40] [1, 1] [1, 1] : memref<128x512xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %41 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          %subview_2 = memref.subview %alloc_1[%39, %arg2, %42] [1, 1, 1] [1, 1, 1] : memref<3x16x40xf16, #gpu.address_space<workgroup>> to memref<1x1x1xf16, strided<[640, 40, 1], offset: ?>, #gpu.address_space<workgroup>>
          memref.store %41, %subview_2[%c0, %c0, %c0] : memref<1x1x1xf16, strided<[640, 40, 1], offset: ?>, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %subview = memref.subview %alloc[%arg0, %18] [1, 1] [1, 1] : memref<32x40xf16, #gpu.address_space<workgroup>> to memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %19 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[40, 1], offset: ?>, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %subview_2 = memref.subview %2[%15, %20] [1, 1] [1, 1] : memref<512x512xf16, #gpu.address_space<global>> to memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
        memref.store %19, %subview_2[%c0, %c0] : memref<1x1xf16, strided<[512, 1], offset: ?>, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After DecomposeAffineOpsPass (iree-codegen-decompose-affine-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%29, %arg4]
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg3, %arg4]
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg1, %arg2]
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%17, %arg2]
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = arith.addi %17, %arg2 : index
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = arith.addi %arg1, %arg2 : index
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = arith.addi %29, %arg4 : index
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = arith.addi %arg3, %arg4 : index
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = arith.addi %29, %arg4 : index
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = arith.addi %arg3, %arg4 : index
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = arith.addi %arg1, %arg2 : index
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = arith.addi %17, %arg2 : index
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After GPUCheckResourceUsagePass (iree-codegen-gpu-check-resource-usage) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = arith.addi %17, %arg2 : index
        %19 = memref.load %2[%15, %18] : memref<512x512xf16, #gpu.address_space<global>>
        %20 = arith.addi %arg1, %arg2 : index
        memref.store %19, %alloc[%arg0, %20] : memref<32x40xf16, #gpu.address_space<workgroup>>
      }
    }
  }
  gpu.barrier
  %3 = arith.muli %thread_id_y, %c16 : index
  %4 = arith.cmpi slt, %thread_id_x, %c0 : index
  %5 = arith.subi %c-1, %thread_id_x : index
  %6 = arith.select %4, %5, %thread_id_x : index
  %7 = arith.divsi %6, %c32 : index
  %8 = arith.subi %c-1, %7 : index
  %9 = arith.select %4, %8, %7 : index
  %10 = arith.muli %9, %c16 : index
  %11 = gpu.subgroup_mma_load_matrix %alloc[%3, %10] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %12 = scf.for %arg0 = %c0 to %c128 step %c16 iter_args(%arg1 = %11) -> (!gpu.mma_matrix<16x16xf16, "COp">) {
    gpu.barrier
    scf.for %arg2 = %thread_id_y to %c32 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c16 step %c512 {
        %27 = arith.muli %workgroup_id_y, %c32 : index
        %28 = arith.addi %arg2, %27 : index
        %29 = arith.addi %arg0, %arg3 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = arith.addi %29, %arg4 : index
          %41 = memref.load %0[%28, %40] : memref<512x128xf16, #gpu.address_space<global>>
          %42 = arith.addi %arg3, %arg4 : index
          memref.store %41, %alloc_0[%39, %arg2, %42] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    scf.for %arg2 = %thread_id_y to %c16 step %c2 {
      %26 = arith.muli %thread_id_x, %c8 : index
      scf.for %arg3 = %26 to %c32 step %c512 {
        %27 = arith.addi %arg0, %arg2 : index
        %28 = arith.muli %workgroup_id_x, %c32 : index
        %29 = arith.addi %arg3, %28 : index
        %30 = arith.cmpi slt, %arg0, %c0 : index
        %31 = arith.subi %c-1, %arg0 : index
        %32 = arith.select %30, %31, %arg0 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c-1, %33 : index
        %35 = arith.select %30, %34, %33 : index
        %36 = arith.remsi %35, %c3 : index
        %37 = arith.cmpi slt, %36, %c0 : index
        %38 = arith.addi %36, %c3 : index
        %39 = arith.select %37, %38, %36 : index
        scf.for %arg4 = %c0 to %c8 step %c1 {
          %40 = arith.addi %29, %arg4 : index
          %41 = memref.load %1[%27, %40] : memref<128x512xf16, #gpu.address_space<global>>
          %42 = arith.addi %arg3, %arg4 : index
          memref.store %41, %alloc_1[%39, %arg2, %42] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
        }
      }
    }
    gpu.barrier
    %13 = arith.cmpi slt, %arg0, %c0 : index
    %14 = arith.subi %c-1, %arg0 : index
    %15 = arith.select %13, %14, %arg0 : index
    %16 = arith.divsi %15, %c16 : index
    %17 = arith.subi %c-1, %16 : index
    %18 = arith.select %13, %17, %16 : index
    %19 = arith.remsi %18, %c3 : index
    %20 = arith.cmpi slt, %19, %c0 : index
    %21 = arith.addi %19, %c3 : index
    %22 = arith.select %20, %21, %19 : index
    %23 = gpu.subgroup_mma_load_matrix %alloc_0[%22, %3, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %24 = gpu.subgroup_mma_load_matrix %alloc_1[%22, %c0, %10] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %25 = gpu.subgroup_mma_compute %23, %24, %arg1 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    scf.yield %25 : !gpu.mma_matrix<16x16xf16, "COp">
  }
  gpu.subgroup_mma_store_matrix %12, %alloc[%3, %10] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  scf.for %arg0 = %thread_id_y to %c32 step %c2 {
    %13 = arith.muli %thread_id_x, %c8 : index
    scf.for %arg1 = %13 to %c32 step %c512 {
      %14 = arith.muli %workgroup_id_y, %c32 : index
      %15 = arith.addi %arg0, %14 : index
      %16 = arith.muli %workgroup_id_x, %c32 : index
      %17 = arith.addi %arg1, %16 : index
      scf.for %arg2 = %c0 to %c8 step %c1 {
        %18 = arith.addi %arg1, %arg2 : index
        %19 = memref.load %alloc[%arg0, %18] : memref<32x40xf16, #gpu.address_space<workgroup>>
        %20 = arith.addi %17, %arg2 : index
        memref.store %19, %2[%15, %20] : memref<512x512xf16, #gpu.address_space<global>>
      }
    }
  }
  gpu.barrier
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb29
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb30
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20:  // pred: ^bb12
  cf.br ^bb21(%thread_id_y : index)
^bb21(%58: index):  // 2 preds: ^bb20, ^bb28
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb22, ^bb29
^bb22:  // pred: ^bb21
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb23(%60 : index)
^bb23(%61: index):  // 2 preds: ^bb22, ^bb27
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb24, ^bb28
^bb24:  // pred: ^bb23
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb25(%c0 : index)
^bb25(%76: index):  // 2 preds: ^bb24, ^bb26
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb26, ^bb27
^bb26:  // pred: ^bb25
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb25(%81 : index)
^bb27:  // pred: ^bb25
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb23(%82 : index)
^bb28:  // pred: ^bb23
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb21(%83 : index)
^bb29:  // pred: ^bb21
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb30:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb31(%thread_id_y : index)
^bb31(%98: index):  // 2 preds: ^bb30, ^bb38
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb32, ^bb39
^bb32:  // pred: ^bb31
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb33(%100 : index)
^bb33(%101: index):  // 2 preds: ^bb32, ^bb37
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb34, ^bb38
^bb34:  // pred: ^bb33
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb35(%c0 : index)
^bb35(%107: index):  // 2 preds: ^bb34, ^bb36
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb36, ^bb37
^bb36:  // pred: ^bb35
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb35(%112 : index)
^bb37:  // pred: ^bb35
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb33(%113 : index)
^bb38:  // pred: ^bb33
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb31(%114 : index)
^bb39:  // pred: ^bb31
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After ConvertBf16ArithToF32Pass (iree-convert-bf16-arith-to-f32) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After ConvertBf16ToUInt16BuffersPass (iree-codegen-convert-bf16-to-uint16-buffers) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After EmulateNarrowTypePass (iree-codegen-emulate-narrow-type) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
  %c3 = arith.constant 3 : index
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1 = arith.constant 1 : index
  %c512 = arith.constant 512 : index
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c16 = arith.constant 16 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
  memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
  memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  gpu.barrier
  %thread_id_x = gpu.thread_id  x
  %thread_id_y = gpu.thread_id  y
  %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb1(%thread_id_y : index)
^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
  %4 = arith.cmpi slt, %3, %c32 : index
  cf.cond_br %4, ^bb2, ^bb9
^bb2:  // pred: ^bb1
  %5 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb3(%5 : index)
^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
  %7 = arith.cmpi slt, %6, %c32 : index
  cf.cond_br %7, ^bb4, ^bb8
^bb4:  // pred: ^bb3
  %8 = arith.muli %workgroup_id_y, %c32 : index
  %9 = arith.addi %3, %8 : index
  %10 = arith.muli %workgroup_id_x, %c32 : index
  %11 = arith.addi %6, %10 : index
  cf.br ^bb5(%c0 : index)
^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
  %13 = arith.cmpi slt, %12, %c8 : index
  cf.cond_br %13, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %14 = arith.addi %11, %12 : index
  %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
  %16 = arith.addi %6, %12 : index
  memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %17 = arith.addi %12, %c1 : index
  cf.br ^bb5(%17 : index)
^bb7:  // pred: ^bb5
  %18 = arith.addi %6, %c512 : index
  cf.br ^bb3(%18 : index)
^bb8:  // pred: ^bb3
  %19 = arith.addi %3, %c2 : index
  cf.br ^bb1(%19 : index)
^bb9:  // pred: ^bb1
  gpu.barrier
  %20 = arith.muli %thread_id_y, %c16 : index
  %21 = arith.cmpi slt, %thread_id_x, %c0 : index
  %22 = arith.subi %c-1, %thread_id_x : index
  %23 = arith.select %21, %22, %thread_id_x : index
  %24 = arith.divsi %23, %c32 : index
  %25 = arith.subi %c-1, %24 : index
  %26 = arith.select %21, %25, %24 : index
  %27 = arith.muli %26, %c16 : index
  %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
  %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
  %31 = arith.cmpi slt, %29, %c128 : index
  cf.cond_br %31, ^bb11, ^bb29
^bb11:  // pred: ^bb10
  gpu.barrier
  cf.br ^bb12(%thread_id_y : index)
^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
  %33 = arith.cmpi slt, %32, %c32 : index
  cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
^bb13:  // pred: ^bb12
  %34 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb14(%34 : index)
^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
  %36 = arith.cmpi slt, %35, %c16 : index
  cf.cond_br %36, ^bb15, ^bb19
^bb15:  // pred: ^bb14
  %37 = arith.muli %workgroup_id_y, %c32 : index
  %38 = arith.addi %32, %37 : index
  %39 = arith.addi %29, %35 : index
  %40 = arith.cmpi slt, %29, %c0 : index
  %41 = arith.subi %c-1, %29 : index
  %42 = arith.select %40, %41, %29 : index
  %43 = arith.divsi %42, %c16 : index
  %44 = arith.subi %c-1, %43 : index
  %45 = arith.select %40, %44, %43 : index
  %46 = arith.remsi %45, %c3 : index
  %47 = arith.cmpi slt, %46, %c0 : index
  %48 = arith.addi %46, %c3 : index
  %49 = arith.select %47, %48, %46 : index
  cf.br ^bb16(%c0 : index)
^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
  %51 = arith.cmpi slt, %50, %c8 : index
  cf.cond_br %51, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %52 = arith.addi %39, %50 : index
  %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
  %54 = arith.addi %35, %50 : index
  memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
  %55 = arith.addi %50, %c1 : index
  cf.br ^bb16(%55 : index)
^bb18:  // pred: ^bb16
  %56 = arith.addi %35, %c512 : index
  cf.br ^bb14(%56 : index)
^bb19:  // pred: ^bb14
  %57 = arith.addi %32, %c2 : index
  cf.br ^bb12(%57 : index)
^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
  %59 = arith.cmpi slt, %58, %c16 : index
  cf.cond_br %59, ^bb21, ^bb28
^bb21:  // pred: ^bb20
  %60 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb22(%60 : index)
^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb23, ^bb27
^bb23:  // pred: ^bb22
  %63 = arith.addi %29, %58 : index
  %64 = arith.muli %workgroup_id_x, %c32 : index
  %65 = arith.addi %61, %64 : index
  %66 = arith.cmpi slt, %29, %c0 : index
  %67 = arith.subi %c-1, %29 : index
  %68 = arith.select %66, %67, %29 : index
  %69 = arith.divsi %68, %c16 : index
  %70 = arith.subi %c-1, %69 : index
  %71 = arith.select %66, %70, %69 : index
  %72 = arith.remsi %71, %c3 : index
  %73 = arith.cmpi slt, %72, %c0 : index
  %74 = arith.addi %72, %c3 : index
  %75 = arith.select %73, %74, %72 : index
  cf.br ^bb24(%c0 : index)
^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
  %77 = arith.cmpi slt, %76, %c8 : index
  cf.cond_br %77, ^bb25, ^bb26
^bb25:  // pred: ^bb24
  %78 = arith.addi %65, %76 : index
  %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
  %80 = arith.addi %61, %76 : index
  memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
  %81 = arith.addi %76, %c1 : index
  cf.br ^bb24(%81 : index)
^bb26:  // pred: ^bb24
  %82 = arith.addi %61, %c512 : index
  cf.br ^bb22(%82 : index)
^bb27:  // pred: ^bb22
  %83 = arith.addi %58, %c2 : index
  cf.br ^bb20(%83 : index)
^bb28:  // pred: ^bb20
  gpu.barrier
  %84 = arith.cmpi slt, %29, %c0 : index
  %85 = arith.subi %c-1, %29 : index
  %86 = arith.select %84, %85, %29 : index
  %87 = arith.divsi %86, %c16 : index
  %88 = arith.subi %c-1, %87 : index
  %89 = arith.select %84, %88, %87 : index
  %90 = arith.remsi %89, %c3 : index
  %91 = arith.cmpi slt, %90, %c0 : index
  %92 = arith.addi %90, %c3 : index
  %93 = arith.select %91, %92, %90 : index
  %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
  %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
  %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
  %97 = arith.addi %29, %c16 : index
  cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
^bb29:  // pred: ^bb10
  gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
  gpu.barrier
  cf.br ^bb30(%thread_id_y : index)
^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
  %99 = arith.cmpi slt, %98, %c32 : index
  cf.cond_br %99, ^bb31, ^bb38
^bb31:  // pred: ^bb30
  %100 = arith.muli %thread_id_x, %c8 : index
  cf.br ^bb32(%100 : index)
^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
  %102 = arith.cmpi slt, %101, %c32 : index
  cf.cond_br %102, ^bb33, ^bb37
^bb33:  // pred: ^bb32
  %103 = arith.muli %workgroup_id_y, %c32 : index
  %104 = arith.addi %98, %103 : index
  %105 = arith.muli %workgroup_id_x, %c32 : index
  %106 = arith.addi %101, %105 : index
  cf.br ^bb34(%c0 : index)
^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
  %108 = arith.cmpi slt, %107, %c8 : index
  cf.cond_br %108, ^bb35, ^bb36
^bb35:  // pred: ^bb34
  %109 = arith.addi %101, %107 : index
  %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
  %111 = arith.addi %106, %107 : index
  memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
  %112 = arith.addi %107, %c1 : index
  cf.br ^bb34(%112 : index)
^bb36:  // pred: ^bb34
  %113 = arith.addi %101, %c512 : index
  cf.br ^bb32(%113 : index)
^bb37:  // pred: ^bb32
  %114 = arith.addi %98, %c2 : index
  cf.br ^bb30(%114 : index)
^bb38:  // pred: ^bb30
  gpu.barrier
  return
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c3 = arith.constant 3 : index
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1 = arith.constant 1 : index
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    cf.br ^bb1(%thread_id_y : index)
  ^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
    %4 = arith.cmpi slt, %3, %c32 : index
    cf.cond_br %4, ^bb2, ^bb9
  ^bb2:  // pred: ^bb1
    %5 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb3(%5 : index)
  ^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
    %7 = arith.cmpi slt, %6, %c32 : index
    cf.cond_br %7, ^bb4, ^bb8
  ^bb4:  // pred: ^bb3
    %8 = arith.muli %workgroup_id_y, %c32 : index
    %9 = arith.addi %3, %8 : index
    %10 = arith.muli %workgroup_id_x, %c32 : index
    %11 = arith.addi %6, %10 : index
    cf.br ^bb5(%c0 : index)
  ^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
    %13 = arith.cmpi slt, %12, %c8 : index
    cf.cond_br %13, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %14 = arith.addi %11, %12 : index
    %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
    %16 = arith.addi %6, %12 : index
    memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
    %17 = arith.addi %12, %c1 : index
    cf.br ^bb5(%17 : index)
  ^bb7:  // pred: ^bb5
    %18 = arith.addi %6, %c512 : index
    cf.br ^bb3(%18 : index)
  ^bb8:  // pred: ^bb3
    %19 = arith.addi %3, %c2 : index
    cf.br ^bb1(%19 : index)
  ^bb9:  // pred: ^bb1
    gpu.barrier
    %20 = arith.muli %thread_id_y, %c16 : index
    %21 = arith.cmpi slt, %thread_id_x, %c0 : index
    %22 = arith.subi %c-1, %thread_id_x : index
    %23 = arith.select %21, %22, %thread_id_x : index
    %24 = arith.divsi %23, %c32 : index
    %25 = arith.subi %c-1, %24 : index
    %26 = arith.select %21, %25, %24 : index
    %27 = arith.muli %26, %c16 : index
    %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
  ^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
    %31 = arith.cmpi slt, %29, %c128 : index
    cf.cond_br %31, ^bb11, ^bb29
  ^bb11:  // pred: ^bb10
    gpu.barrier
    cf.br ^bb12(%thread_id_y : index)
  ^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
    %33 = arith.cmpi slt, %32, %c32 : index
    cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
  ^bb13:  // pred: ^bb12
    %34 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb14(%34 : index)
  ^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
    %36 = arith.cmpi slt, %35, %c16 : index
    cf.cond_br %36, ^bb15, ^bb19
  ^bb15:  // pred: ^bb14
    %37 = arith.muli %workgroup_id_y, %c32 : index
    %38 = arith.addi %32, %37 : index
    %39 = arith.addi %29, %35 : index
    %40 = arith.cmpi slt, %29, %c0 : index
    %41 = arith.subi %c-1, %29 : index
    %42 = arith.select %40, %41, %29 : index
    %43 = arith.divsi %42, %c16 : index
    %44 = arith.subi %c-1, %43 : index
    %45 = arith.select %40, %44, %43 : index
    %46 = arith.remsi %45, %c3 : index
    %47 = arith.cmpi slt, %46, %c0 : index
    %48 = arith.addi %46, %c3 : index
    %49 = arith.select %47, %48, %46 : index
    cf.br ^bb16(%c0 : index)
  ^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
    %51 = arith.cmpi slt, %50, %c8 : index
    cf.cond_br %51, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %52 = arith.addi %39, %50 : index
    %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
    %54 = arith.addi %35, %50 : index
    memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %55 = arith.addi %50, %c1 : index
    cf.br ^bb16(%55 : index)
  ^bb18:  // pred: ^bb16
    %56 = arith.addi %35, %c512 : index
    cf.br ^bb14(%56 : index)
  ^bb19:  // pred: ^bb14
    %57 = arith.addi %32, %c2 : index
    cf.br ^bb12(%57 : index)
  ^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
    %59 = arith.cmpi slt, %58, %c16 : index
    cf.cond_br %59, ^bb21, ^bb28
  ^bb21:  // pred: ^bb20
    %60 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb22(%60 : index)
  ^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
    %62 = arith.cmpi slt, %61, %c32 : index
    cf.cond_br %62, ^bb23, ^bb27
  ^bb23:  // pred: ^bb22
    %63 = arith.addi %29, %58 : index
    %64 = arith.muli %workgroup_id_x, %c32 : index
    %65 = arith.addi %61, %64 : index
    %66 = arith.cmpi slt, %29, %c0 : index
    %67 = arith.subi %c-1, %29 : index
    %68 = arith.select %66, %67, %29 : index
    %69 = arith.divsi %68, %c16 : index
    %70 = arith.subi %c-1, %69 : index
    %71 = arith.select %66, %70, %69 : index
    %72 = arith.remsi %71, %c3 : index
    %73 = arith.cmpi slt, %72, %c0 : index
    %74 = arith.addi %72, %c3 : index
    %75 = arith.select %73, %74, %72 : index
    cf.br ^bb24(%c0 : index)
  ^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
    %77 = arith.cmpi slt, %76, %c8 : index
    cf.cond_br %77, ^bb25, ^bb26
  ^bb25:  // pred: ^bb24
    %78 = arith.addi %65, %76 : index
    %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
    %80 = arith.addi %61, %76 : index
    memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %81 = arith.addi %76, %c1 : index
    cf.br ^bb24(%81 : index)
  ^bb26:  // pred: ^bb24
    %82 = arith.addi %61, %c512 : index
    cf.br ^bb22(%82 : index)
  ^bb27:  // pred: ^bb22
    %83 = arith.addi %58, %c2 : index
    cf.br ^bb20(%83 : index)
  ^bb28:  // pred: ^bb20
    gpu.barrier
    %84 = arith.cmpi slt, %29, %c0 : index
    %85 = arith.subi %c-1, %29 : index
    %86 = arith.select %84, %85, %29 : index
    %87 = arith.divsi %86, %c16 : index
    %88 = arith.subi %c-1, %87 : index
    %89 = arith.select %84, %88, %87 : index
    %90 = arith.remsi %89, %c3 : index
    %91 = arith.cmpi slt, %90, %c0 : index
    %92 = arith.addi %90, %c3 : index
    %93 = arith.select %91, %92, %90 : index
    %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    %97 = arith.addi %29, %c16 : index
    cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
  ^bb29:  // pred: ^bb10
    gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    cf.br ^bb30(%thread_id_y : index)
  ^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
    %99 = arith.cmpi slt, %98, %c32 : index
    cf.cond_br %99, ^bb31, ^bb38
  ^bb31:  // pred: ^bb30
    %100 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb32(%100 : index)
  ^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
    %102 = arith.cmpi slt, %101, %c32 : index
    cf.cond_br %102, ^bb33, ^bb37
  ^bb33:  // pred: ^bb32
    %103 = arith.muli %workgroup_id_y, %c32 : index
    %104 = arith.addi %98, %103 : index
    %105 = arith.muli %workgroup_id_x, %c32 : index
    %106 = arith.addi %101, %105 : index
    cf.br ^bb34(%c0 : index)
  ^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
    %108 = arith.cmpi slt, %107, %c8 : index
    cf.cond_br %108, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %109 = arith.addi %101, %107 : index
    %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
    %111 = arith.addi %106, %107 : index
    memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
    %112 = arith.addi %107, %c1 : index
    cf.br ^bb34(%112 : index)
  ^bb36:  // pred: ^bb34
    %113 = arith.addi %101, %c512 : index
    cf.br ^bb32(%113 : index)
  ^bb37:  // pred: ^bb32
    %114 = arith.addi %98, %c2 : index
    cf.br ^bb30(%114 : index)
  ^bb38:  // pred: ^bb30
    gpu.barrier
    return
  }
}

// -----// IR Dump After LLVMGPUCastAddressSpaceFunctionPass (iree-llvmgpu-cast-address-space-function) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c3 = arith.constant 3 : index
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1 = arith.constant 1 : index
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    cf.br ^bb1(%thread_id_y : index)
  ^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
    %4 = arith.cmpi slt, %3, %c32 : index
    cf.cond_br %4, ^bb2, ^bb9
  ^bb2:  // pred: ^bb1
    %5 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb3(%5 : index)
  ^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
    %7 = arith.cmpi slt, %6, %c32 : index
    cf.cond_br %7, ^bb4, ^bb8
  ^bb4:  // pred: ^bb3
    %8 = arith.muli %workgroup_id_y, %c32 : index
    %9 = arith.addi %3, %8 : index
    %10 = arith.muli %workgroup_id_x, %c32 : index
    %11 = arith.addi %6, %10 : index
    cf.br ^bb5(%c0 : index)
  ^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
    %13 = arith.cmpi slt, %12, %c8 : index
    cf.cond_br %13, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %14 = arith.addi %11, %12 : index
    %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
    %16 = arith.addi %6, %12 : index
    memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
    %17 = arith.addi %12, %c1 : index
    cf.br ^bb5(%17 : index)
  ^bb7:  // pred: ^bb5
    %18 = arith.addi %6, %c512 : index
    cf.br ^bb3(%18 : index)
  ^bb8:  // pred: ^bb3
    %19 = arith.addi %3, %c2 : index
    cf.br ^bb1(%19 : index)
  ^bb9:  // pred: ^bb1
    gpu.barrier
    %20 = arith.muli %thread_id_y, %c16 : index
    %21 = arith.cmpi slt, %thread_id_x, %c0 : index
    %22 = arith.subi %c-1, %thread_id_x : index
    %23 = arith.select %21, %22, %thread_id_x : index
    %24 = arith.divsi %23, %c32 : index
    %25 = arith.subi %c-1, %24 : index
    %26 = arith.select %21, %25, %24 : index
    %27 = arith.muli %26, %c16 : index
    %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
  ^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
    %31 = arith.cmpi slt, %29, %c128 : index
    cf.cond_br %31, ^bb11, ^bb29
  ^bb11:  // pred: ^bb10
    gpu.barrier
    cf.br ^bb12(%thread_id_y : index)
  ^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
    %33 = arith.cmpi slt, %32, %c32 : index
    cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
  ^bb13:  // pred: ^bb12
    %34 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb14(%34 : index)
  ^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
    %36 = arith.cmpi slt, %35, %c16 : index
    cf.cond_br %36, ^bb15, ^bb19
  ^bb15:  // pred: ^bb14
    %37 = arith.muli %workgroup_id_y, %c32 : index
    %38 = arith.addi %32, %37 : index
    %39 = arith.addi %29, %35 : index
    %40 = arith.cmpi slt, %29, %c0 : index
    %41 = arith.subi %c-1, %29 : index
    %42 = arith.select %40, %41, %29 : index
    %43 = arith.divsi %42, %c16 : index
    %44 = arith.subi %c-1, %43 : index
    %45 = arith.select %40, %44, %43 : index
    %46 = arith.remsi %45, %c3 : index
    %47 = arith.cmpi slt, %46, %c0 : index
    %48 = arith.addi %46, %c3 : index
    %49 = arith.select %47, %48, %46 : index
    cf.br ^bb16(%c0 : index)
  ^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
    %51 = arith.cmpi slt, %50, %c8 : index
    cf.cond_br %51, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %52 = arith.addi %39, %50 : index
    %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
    %54 = arith.addi %35, %50 : index
    memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %55 = arith.addi %50, %c1 : index
    cf.br ^bb16(%55 : index)
  ^bb18:  // pred: ^bb16
    %56 = arith.addi %35, %c512 : index
    cf.br ^bb14(%56 : index)
  ^bb19:  // pred: ^bb14
    %57 = arith.addi %32, %c2 : index
    cf.br ^bb12(%57 : index)
  ^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
    %59 = arith.cmpi slt, %58, %c16 : index
    cf.cond_br %59, ^bb21, ^bb28
  ^bb21:  // pred: ^bb20
    %60 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb22(%60 : index)
  ^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
    %62 = arith.cmpi slt, %61, %c32 : index
    cf.cond_br %62, ^bb23, ^bb27
  ^bb23:  // pred: ^bb22
    %63 = arith.addi %29, %58 : index
    %64 = arith.muli %workgroup_id_x, %c32 : index
    %65 = arith.addi %61, %64 : index
    %66 = arith.cmpi slt, %29, %c0 : index
    %67 = arith.subi %c-1, %29 : index
    %68 = arith.select %66, %67, %29 : index
    %69 = arith.divsi %68, %c16 : index
    %70 = arith.subi %c-1, %69 : index
    %71 = arith.select %66, %70, %69 : index
    %72 = arith.remsi %71, %c3 : index
    %73 = arith.cmpi slt, %72, %c0 : index
    %74 = arith.addi %72, %c3 : index
    %75 = arith.select %73, %74, %72 : index
    cf.br ^bb24(%c0 : index)
  ^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
    %77 = arith.cmpi slt, %76, %c8 : index
    cf.cond_br %77, ^bb25, ^bb26
  ^bb25:  // pred: ^bb24
    %78 = arith.addi %65, %76 : index
    %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
    %80 = arith.addi %61, %76 : index
    memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %81 = arith.addi %76, %c1 : index
    cf.br ^bb24(%81 : index)
  ^bb26:  // pred: ^bb24
    %82 = arith.addi %61, %c512 : index
    cf.br ^bb22(%82 : index)
  ^bb27:  // pred: ^bb22
    %83 = arith.addi %58, %c2 : index
    cf.br ^bb20(%83 : index)
  ^bb28:  // pred: ^bb20
    gpu.barrier
    %84 = arith.cmpi slt, %29, %c0 : index
    %85 = arith.subi %c-1, %29 : index
    %86 = arith.select %84, %85, %29 : index
    %87 = arith.divsi %86, %c16 : index
    %88 = arith.subi %c-1, %87 : index
    %89 = arith.select %84, %88, %87 : index
    %90 = arith.remsi %89, %c3 : index
    %91 = arith.cmpi slt, %90, %c0 : index
    %92 = arith.addi %90, %c3 : index
    %93 = arith.select %91, %92, %90 : index
    %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    %97 = arith.addi %29, %c16 : index
    cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
  ^bb29:  // pred: ^bb10
    gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    cf.br ^bb30(%thread_id_y : index)
  ^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
    %99 = arith.cmpi slt, %98, %c32 : index
    cf.cond_br %99, ^bb31, ^bb38
  ^bb31:  // pred: ^bb30
    %100 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb32(%100 : index)
  ^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
    %102 = arith.cmpi slt, %101, %c32 : index
    cf.cond_br %102, ^bb33, ^bb37
  ^bb33:  // pred: ^bb32
    %103 = arith.muli %workgroup_id_y, %c32 : index
    %104 = arith.addi %98, %103 : index
    %105 = arith.muli %workgroup_id_x, %c32 : index
    %106 = arith.addi %101, %105 : index
    cf.br ^bb34(%c0 : index)
  ^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
    %108 = arith.cmpi slt, %107, %c8 : index
    cf.cond_br %108, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %109 = arith.addi %101, %107 : index
    %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
    %111 = arith.addi %106, %107 : index
    memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
    %112 = arith.addi %107, %c1 : index
    cf.br ^bb34(%112 : index)
  ^bb36:  // pred: ^bb34
    %113 = arith.addi %101, %c512 : index
    cf.br ^bb32(%113 : index)
  ^bb37:  // pred: ^bb32
    %114 = arith.addi %98, %c2 : index
    cf.br ^bb30(%114 : index)
  ^bb38:  // pred: ^bb30
    gpu.barrier
    return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
module {
  func.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16() {
    %c3 = arith.constant 3 : index
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1 = arith.constant 1 : index
    %c512 = arith.constant 512 : index
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c16 = arith.constant 16 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<512x128xf16, #gpu.address_space<global>>
    memref.assume_alignment %0, 64 : memref<512x128xf16, #gpu.address_space<global>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<128x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %1, 64 : memref<128x512xf16, #gpu.address_space<global>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<512x512xf16, #gpu.address_space<global>>
    memref.assume_alignment %2, 64 : memref<512x512xf16, #gpu.address_space<global>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    gpu.barrier
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %alloc = memref.alloc() : memref<32x40xf16, #gpu.address_space<workgroup>>
    cf.br ^bb1(%thread_id_y : index)
  ^bb1(%3: index):  // 2 preds: ^bb0, ^bb8
    %4 = arith.cmpi slt, %3, %c32 : index
    cf.cond_br %4, ^bb2, ^bb9
  ^bb2:  // pred: ^bb1
    %5 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb3(%5 : index)
  ^bb3(%6: index):  // 2 preds: ^bb2, ^bb7
    %7 = arith.cmpi slt, %6, %c32 : index
    cf.cond_br %7, ^bb4, ^bb8
  ^bb4:  // pred: ^bb3
    %8 = arith.muli %workgroup_id_y, %c32 : index
    %9 = arith.addi %3, %8 : index
    %10 = arith.muli %workgroup_id_x, %c32 : index
    %11 = arith.addi %6, %10 : index
    cf.br ^bb5(%c0 : index)
  ^bb5(%12: index):  // 2 preds: ^bb4, ^bb6
    %13 = arith.cmpi slt, %12, %c8 : index
    cf.cond_br %13, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %14 = arith.addi %11, %12 : index
    %15 = memref.load %2[%9, %14] : memref<512x512xf16, #gpu.address_space<global>>
    %16 = arith.addi %6, %12 : index
    memref.store %15, %alloc[%3, %16] : memref<32x40xf16, #gpu.address_space<workgroup>>
    %17 = arith.addi %12, %c1 : index
    cf.br ^bb5(%17 : index)
  ^bb7:  // pred: ^bb5
    %18 = arith.addi %6, %c512 : index
    cf.br ^bb3(%18 : index)
  ^bb8:  // pred: ^bb3
    %19 = arith.addi %3, %c2 : index
    cf.br ^bb1(%19 : index)
  ^bb9:  // pred: ^bb1
    gpu.barrier
    %20 = arith.muli %thread_id_y, %c16 : index
    %21 = arith.cmpi slt, %thread_id_x, %c0 : index
    %22 = arith.subi %c-1, %thread_id_x : index
    %23 = arith.select %21, %22, %thread_id_x : index
    %24 = arith.divsi %23, %c32 : index
    %25 = arith.subi %c-1, %24 : index
    %26 = arith.select %21, %25, %24 : index
    %27 = arith.muli %26, %c16 : index
    %28 = gpu.subgroup_mma_load_matrix %alloc[%20, %27] {leadDimension = 40 : index} : memref<32x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "COp">
    %alloc_0 = memref.alloc() : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %alloc_1 = memref.alloc() : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    cf.br ^bb10(%c0, %28 : index, !gpu.mma_matrix<16x16xf16, "COp">)
  ^bb10(%29: index, %30: !gpu.mma_matrix<16x16xf16, "COp">):  // 2 preds: ^bb9, ^bb28
    %31 = arith.cmpi slt, %29, %c128 : index
    cf.cond_br %31, ^bb11, ^bb29
  ^bb11:  // pred: ^bb10
    gpu.barrier
    cf.br ^bb12(%thread_id_y : index)
  ^bb12(%32: index):  // 2 preds: ^bb11, ^bb19
    %33 = arith.cmpi slt, %32, %c32 : index
    cf.cond_br %33, ^bb13, ^bb20(%thread_id_y : index)
  ^bb13:  // pred: ^bb12
    %34 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb14(%34 : index)
  ^bb14(%35: index):  // 2 preds: ^bb13, ^bb18
    %36 = arith.cmpi slt, %35, %c16 : index
    cf.cond_br %36, ^bb15, ^bb19
  ^bb15:  // pred: ^bb14
    %37 = arith.muli %workgroup_id_y, %c32 : index
    %38 = arith.addi %32, %37 : index
    %39 = arith.addi %29, %35 : index
    %40 = arith.cmpi slt, %29, %c0 : index
    %41 = arith.subi %c-1, %29 : index
    %42 = arith.select %40, %41, %29 : index
    %43 = arith.divsi %42, %c16 : index
    %44 = arith.subi %c-1, %43 : index
    %45 = arith.select %40, %44, %43 : index
    %46 = arith.remsi %45, %c3 : index
    %47 = arith.cmpi slt, %46, %c0 : index
    %48 = arith.addi %46, %c3 : index
    %49 = arith.select %47, %48, %46 : index
    cf.br ^bb16(%c0 : index)
  ^bb16(%50: index):  // 2 preds: ^bb15, ^bb17
    %51 = arith.cmpi slt, %50, %c8 : index
    cf.cond_br %51, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %52 = arith.addi %39, %50 : index
    %53 = memref.load %0[%38, %52] : memref<512x128xf16, #gpu.address_space<global>>
    %54 = arith.addi %35, %50 : index
    memref.store %53, %alloc_0[%49, %32, %54] : memref<3x32x24xf16, #gpu.address_space<workgroup>>
    %55 = arith.addi %50, %c1 : index
    cf.br ^bb16(%55 : index)
  ^bb18:  // pred: ^bb16
    %56 = arith.addi %35, %c512 : index
    cf.br ^bb14(%56 : index)
  ^bb19:  // pred: ^bb14
    %57 = arith.addi %32, %c2 : index
    cf.br ^bb12(%57 : index)
  ^bb20(%58: index):  // 2 preds: ^bb12, ^bb27
    %59 = arith.cmpi slt, %58, %c16 : index
    cf.cond_br %59, ^bb21, ^bb28
  ^bb21:  // pred: ^bb20
    %60 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb22(%60 : index)
  ^bb22(%61: index):  // 2 preds: ^bb21, ^bb26
    %62 = arith.cmpi slt, %61, %c32 : index
    cf.cond_br %62, ^bb23, ^bb27
  ^bb23:  // pred: ^bb22
    %63 = arith.addi %29, %58 : index
    %64 = arith.muli %workgroup_id_x, %c32 : index
    %65 = arith.addi %61, %64 : index
    %66 = arith.cmpi slt, %29, %c0 : index
    %67 = arith.subi %c-1, %29 : index
    %68 = arith.select %66, %67, %29 : index
    %69 = arith.divsi %68, %c16 : index
    %70 = arith.subi %c-1, %69 : index
    %71 = arith.select %66, %70, %69 : index
    %72 = arith.remsi %71, %c3 : index
    %73 = arith.cmpi slt, %72, %c0 : index
    %74 = arith.addi %72, %c3 : index
    %75 = arith.select %73, %74, %72 : index
    cf.br ^bb24(%c0 : index)
  ^bb24(%76: index):  // 2 preds: ^bb23, ^bb25
    %77 = arith.cmpi slt, %76, %c8 : index
    cf.cond_br %77, ^bb25, ^bb26
  ^bb25:  // pred: ^bb24
    %78 = arith.addi %65, %76 : index
    %79 = memref.load %1[%63, %78] : memref<128x512xf16, #gpu.address_space<global>>
    %80 = arith.addi %61, %76 : index
    memref.store %79, %alloc_1[%75, %58, %80] : memref<3x16x40xf16, #gpu.address_space<workgroup>>
    %81 = arith.addi %76, %c1 : index
    cf.br ^bb24(%81 : index)
  ^bb26:  // pred: ^bb24
    %82 = arith.addi %61, %c512 : index
    cf.br ^bb22(%82 : index)
  ^bb27:  // pred: ^bb22
    %83 = arith.addi %58, %c2 : index
    cf.br ^bb20(%83 : index)
  ^bb28:  // pred: ^bb20
    gpu.barrier
    %84 = arith.cmpi slt, %29, %c0 : index
    %85 = arith.subi %c-1, %29 : index
    %86 = arith.select %84, %85, %29 : index
    %87 = arith.divsi %86, %c16 : index
    %88 = arith.subi %c-1, %87 : index
    %89 = arith.select %84, %88, %87 : index
    %90 = arith.remsi %89, %c3 : index
    %91 = arith.cmpi slt, %90, %c0 : index
    %92 = arith.addi %90, %c3 : index
    %93 = arith.select %91, %92, %90 : index
    %94 = gpu.subgroup_mma_load_matrix %alloc_0[%93, %20, %c0] {leadDimension = 24 : index} : memref<3x32x24xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "AOp">
    %95 = gpu.subgroup_mma_load_matrix %alloc_1[%93, %c0, %27] {leadDimension = 40 : index} : memref<3x16x40xf16, #gpu.address_space<workgroup>> -> !gpu.mma_matrix<16x16xf16, "BOp">
    %96 = gpu.subgroup_mma_compute %94, %95, %30 : !gpu.mma_matrix<16x16xf16, "AOp">, !gpu.mma_matrix<16x16xf16, "BOp"> -> !gpu.mma_matrix<16x16xf16, "COp">
    %97 = arith.addi %29, %c16 : index
    cf.br ^bb10(%97, %96 : index, !gpu.mma_matrix<16x16xf16, "COp">)
  ^bb29:  // pred: ^bb10
    gpu.subgroup_mma_store_matrix %30, %alloc[%20, %27] {leadDimension = 40 : index} : !gpu.mma_matrix<16x16xf16, "COp">, memref<32x40xf16, #gpu.address_space<workgroup>>
    gpu.barrier
    cf.br ^bb30(%thread_id_y : index)
  ^bb30(%98: index):  // 2 preds: ^bb29, ^bb37
    %99 = arith.cmpi slt, %98, %c32 : index
    cf.cond_br %99, ^bb31, ^bb38
  ^bb31:  // pred: ^bb30
    %100 = arith.muli %thread_id_x, %c8 : index
    cf.br ^bb32(%100 : index)
  ^bb32(%101: index):  // 2 preds: ^bb31, ^bb36
    %102 = arith.cmpi slt, %101, %c32 : index
    cf.cond_br %102, ^bb33, ^bb37
  ^bb33:  // pred: ^bb32
    %103 = arith.muli %workgroup_id_y, %c32 : index
    %104 = arith.addi %98, %103 : index
    %105 = arith.muli %workgroup_id_x, %c32 : index
    %106 = arith.addi %101, %105 : index
    cf.br ^bb34(%c0 : index)
  ^bb34(%107: index):  // 2 preds: ^bb33, ^bb35
    %108 = arith.cmpi slt, %107, %c8 : index
    cf.cond_br %108, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %109 = arith.addi %101, %107 : index
    %110 = memref.load %alloc[%98, %109] : memref<32x40xf16, #gpu.address_space<workgroup>>
    %111 = arith.addi %106, %107 : index
    memref.store %110, %2[%104, %111] : memref<512x512xf16, #gpu.address_space<global>>
    %112 = arith.addi %107, %c1 : index
    cf.br ^bb34(%112 : index)
  ^bb36:  // pred: ^bb34
    %113 = arith.addi %101, %c512 : index
    cf.br ^bb32(%113 : index)
  ^bb37:  // pred: ^bb32
    %114 = arith.addi %98, %c2 : index
    cf.br ^bb30(%114 : index)
  ^bb38:  // pred: ^bb30
    gpu.barrier
    return
  }
}

// -----// IR Dump After ConvertToNVVMPass (iree-convert-to-nvvm) //----- //
module {
  llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
  llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
  llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
  llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
  llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
    %0 = llvm.mlir.constant(24 : index) : i32
    %1 = llvm.mlir.constant(40 : index) : i32
    %2 = llvm.mlir.constant(63 : index) : i64
    %3 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
    %4 = llvm.mlir.constant(0 : i64) : i64
    %5 = llvm.mlir.constant(0 : i64) : i64
    %6 = llvm.getelementptr %3[%4, %5] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
    %7 = llvm.mlir.constant(640 : index) : i64
    %8 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
    %9 = llvm.mlir.constant(0 : i64) : i64
    %10 = llvm.mlir.constant(3840 : i64) : i64
    %11 = llvm.getelementptr %8[%9, %10] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
    %12 = llvm.mlir.constant(768 : index) : i64
    %13 = llvm.mlir.constant(24 : index) : i64
    %14 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
    %15 = llvm.mlir.constant(0 : i64) : i64
    %16 = llvm.mlir.constant(8448 : i64) : i64
    %17 = llvm.getelementptr %14[%15, %16] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
    %18 = llvm.mlir.constant(0 : index) : i64
    %19 = llvm.mlir.constant(128 : index) : i64
    %20 = llvm.mlir.constant(16 : index) : i64
    %21 = llvm.mlir.constant(32 : index) : i64
    %22 = llvm.mlir.constant(2 : index) : i64
    %23 = llvm.mlir.constant(512 : index) : i64
    %24 = llvm.mlir.constant(1 : index) : i64
    %25 = llvm.mlir.constant(8 : index) : i64
    %26 = llvm.mlir.constant(-1 : index) : i64
    %27 = llvm.mlir.constant(3 : index) : i64
    %28 = llvm.mlir.constant(40 : index) : i64
    %29 = llvm.getelementptr %17[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
    %30 = llvm.getelementptr %11[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
    %31 = llvm.getelementptr %6[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
    %32 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
    %33 = llvm.and %32, %2 : i64
    %34 = llvm.icmp "eq" %33, %18 : i64
    llvm.intr.assume %34 : i1
    %35 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
    %36 = llvm.and %35, %2 : i64
    %37 = llvm.icmp "eq" %36, %18 : i64
    llvm.intr.assume %37 : i1
    %38 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
    %39 = llvm.and %38, %2 : i64
    %40 = llvm.icmp "eq" %39, %18 : i64
    llvm.intr.assume %40 : i1
    %41 = nvvm.read.ptx.sreg.ctaid.x : i32
    %42 = llvm.sext %41 : i32 to i64
    %43 = nvvm.read.ptx.sreg.ctaid.y : i32
    %44 = llvm.sext %43 : i32 to i64
    nvvm.barrier0
    %45 = nvvm.read.ptx.sreg.tid.x : i32
    %46 = llvm.sext %45 : i32 to i64
    %47 = nvvm.read.ptx.sreg.tid.y : i32
    %48 = llvm.sext %47 : i32 to i64
    llvm.br ^bb1(%48 : i64)
  ^bb1(%49: i64):  // 2 preds: ^bb0, ^bb8
    %50 = llvm.icmp "slt" %49, %21 : i64
    llvm.cond_br %50, ^bb2, ^bb9
  ^bb2:  // pred: ^bb1
    %51 = llvm.mul %46, %25 : i64
    llvm.br ^bb3(%51 : i64)
  ^bb3(%52: i64):  // 2 preds: ^bb2, ^bb7
    %53 = llvm.icmp "slt" %52, %21 : i64
    llvm.cond_br %53, ^bb4, ^bb8
  ^bb4:  // pred: ^bb3
    %54 = llvm.mul %44, %21 : i64
    %55 = llvm.add %49, %54 : i64
    %56 = llvm.mul %42, %21 : i64
    %57 = llvm.add %52, %56 : i64
    llvm.br ^bb5(%18 : i64)
  ^bb5(%58: i64):  // 2 preds: ^bb4, ^bb6
    %59 = llvm.icmp "slt" %58, %25 : i64
    llvm.cond_br %59, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %60 = llvm.add %57, %58 : i64
    %61 = llvm.mul %55, %23 : i64
    %62 = llvm.add %61, %60 : i64
    %63 = llvm.getelementptr %arg2[%62] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
    %64 = llvm.load %63 : !llvm.ptr<1> -> f16
    %65 = llvm.add %52, %58 : i64
    %66 = llvm.mul %49, %28 : i64
    %67 = llvm.add %66, %65 : i64
    %68 = llvm.getelementptr %29[%67] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    llvm.store %64, %68 : f16, !llvm.ptr<3>
    %69 = llvm.add %58, %24 : i64
    llvm.br ^bb5(%69 : i64)
  ^bb7:  // pred: ^bb5
    %70 = llvm.add %52, %23 : i64
    llvm.br ^bb3(%70 : i64)
  ^bb8:  // pred: ^bb3
    %71 = llvm.add %49, %22 : i64
    llvm.br ^bb1(%71 : i64)
  ^bb9:  // pred: ^bb1
    nvvm.barrier0
    %72 = llvm.mul %48, %20 : i64
    %73 = llvm.icmp "slt" %46, %18 : i64
    %74 = llvm.sub %26, %46 : i64
    %75 = llvm.select %73, %74, %46 : i1, i64
    %76 = llvm.sdiv %75, %21 : i64
    %77 = llvm.sub %26, %76 : i64
    %78 = llvm.select %73, %77, %76 : i1, i64
    %79 = llvm.mul %78, %20 : i64
    %80 = llvm.mul %72, %28 : i64
    %81 = llvm.add %80, %79 : i64
    %82 = llvm.getelementptr %29[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    %83 = nvvm.wmma.load %82, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
    llvm.br ^bb10(%18, %83 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
  ^bb10(%84: i64, %85: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
    %86 = llvm.icmp "slt" %84, %19 : i64
    llvm.cond_br %86, ^bb11, ^bb29
  ^bb11:  // pred: ^bb10
    nvvm.barrier0
    llvm.br ^bb12(%48 : i64)
  ^bb12(%87: i64):  // 2 preds: ^bb11, ^bb19
    %88 = llvm.icmp "slt" %87, %21 : i64
    llvm.cond_br %88, ^bb13, ^bb20(%48 : i64)
  ^bb13:  // pred: ^bb12
    %89 = llvm.mul %46, %25 : i64
    llvm.br ^bb14(%89 : i64)
  ^bb14(%90: i64):  // 2 preds: ^bb13, ^bb18
    %91 = llvm.icmp "slt" %90, %20 : i64
    llvm.cond_br %91, ^bb15, ^bb19
  ^bb15:  // pred: ^bb14
    %92 = llvm.mul %44, %21 : i64
    %93 = llvm.add %87, %92 : i64
    %94 = llvm.add %84, %90 : i64
    %95 = llvm.icmp "slt" %84, %18 : i64
    %96 = llvm.sub %26, %84 : i64
    %97 = llvm.select %95, %96, %84 : i1, i64
    %98 = llvm.sdiv %97, %20 : i64
    %99 = llvm.sub %26, %98 : i64
    %100 = llvm.select %95, %99, %98 : i1, i64
    %101 = llvm.srem %100, %27 : i64
    %102 = llvm.icmp "slt" %101, %18 : i64
    %103 = llvm.add %101, %27 : i64
    %104 = llvm.select %102, %103, %101 : i1, i64
    llvm.br ^bb16(%18 : i64)
  ^bb16(%105: i64):  // 2 preds: ^bb15, ^bb17
    %106 = llvm.icmp "slt" %105, %25 : i64
    llvm.cond_br %106, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %107 = llvm.add %94, %105 : i64
    %108 = llvm.mul %93, %19 : i64
    %109 = llvm.add %108, %107 : i64
    %110 = llvm.getelementptr %arg0[%109] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
    %111 = llvm.load %110 : !llvm.ptr<1> -> f16
    %112 = llvm.add %90, %105 : i64
    %113 = llvm.mul %104, %12 : i64
    %114 = llvm.mul %87, %13 : i64
    %115 = llvm.add %113, %114 : i64
    %116 = llvm.add %115, %112 : i64
    %117 = llvm.getelementptr %30[%116] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    llvm.store %111, %117 : f16, !llvm.ptr<3>
    %118 = llvm.add %105, %24 : i64
    llvm.br ^bb16(%118 : i64)
  ^bb18:  // pred: ^bb16
    %119 = llvm.add %90, %23 : i64
    llvm.br ^bb14(%119 : i64)
  ^bb19:  // pred: ^bb14
    %120 = llvm.add %87, %22 : i64
    llvm.br ^bb12(%120 : i64)
  ^bb20(%121: i64):  // 2 preds: ^bb12, ^bb27
    %122 = llvm.icmp "slt" %121, %20 : i64
    llvm.cond_br %122, ^bb21, ^bb28
  ^bb21:  // pred: ^bb20
    %123 = llvm.mul %46, %25 : i64
    llvm.br ^bb22(%123 : i64)
  ^bb22(%124: i64):  // 2 preds: ^bb21, ^bb26
    %125 = llvm.icmp "slt" %124, %21 : i64
    llvm.cond_br %125, ^bb23, ^bb27
  ^bb23:  // pred: ^bb22
    %126 = llvm.add %84, %121 : i64
    %127 = llvm.mul %42, %21 : i64
    %128 = llvm.add %124, %127 : i64
    %129 = llvm.icmp "slt" %84, %18 : i64
    %130 = llvm.sub %26, %84 : i64
    %131 = llvm.select %129, %130, %84 : i1, i64
    %132 = llvm.sdiv %131, %20 : i64
    %133 = llvm.sub %26, %132 : i64
    %134 = llvm.select %129, %133, %132 : i1, i64
    %135 = llvm.srem %134, %27 : i64
    %136 = llvm.icmp "slt" %135, %18 : i64
    %137 = llvm.add %135, %27 : i64
    %138 = llvm.select %136, %137, %135 : i1, i64
    llvm.br ^bb24(%18 : i64)
  ^bb24(%139: i64):  // 2 preds: ^bb23, ^bb25
    %140 = llvm.icmp "slt" %139, %25 : i64
    llvm.cond_br %140, ^bb25, ^bb26
  ^bb25:  // pred: ^bb24
    %141 = llvm.add %128, %139 : i64
    %142 = llvm.mul %126, %23 : i64
    %143 = llvm.add %142, %141 : i64
    %144 = llvm.getelementptr %arg1[%143] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
    %145 = llvm.load %144 : !llvm.ptr<1> -> f16
    %146 = llvm.add %124, %139 : i64
    %147 = llvm.mul %138, %7 : i64
    %148 = llvm.mul %121, %28 : i64
    %149 = llvm.add %147, %148 : i64
    %150 = llvm.add %149, %146 : i64
    %151 = llvm.getelementptr %31[%150] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    llvm.store %145, %151 : f16, !llvm.ptr<3>
    %152 = llvm.add %139, %24 : i64
    llvm.br ^bb24(%152 : i64)
  ^bb26:  // pred: ^bb24
    %153 = llvm.add %124, %23 : i64
    llvm.br ^bb22(%153 : i64)
  ^bb27:  // pred: ^bb22
    %154 = llvm.add %121, %22 : i64
    llvm.br ^bb20(%154 : i64)
  ^bb28:  // pred: ^bb20
    nvvm.barrier0
    %155 = llvm.icmp "slt" %84, %18 : i64
    %156 = llvm.sub %26, %84 : i64
    %157 = llvm.select %155, %156, %84 : i1, i64
    %158 = llvm.sdiv %157, %20 : i64
    %159 = llvm.sub %26, %158 : i64
    %160 = llvm.select %155, %159, %158 : i1, i64
    %161 = llvm.srem %160, %27 : i64
    %162 = llvm.icmp "slt" %161, %18 : i64
    %163 = llvm.add %161, %27 : i64
    %164 = llvm.select %162, %163, %161 : i1, i64
    %165 = llvm.mul %164, %12 : i64
    %166 = llvm.mul %72, %13 : i64
    %167 = llvm.add %165, %166 : i64
    %168 = llvm.add %167, %18 : i64
    %169 = llvm.getelementptr %30[%168] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    %170 = nvvm.wmma.load %169, %0 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
    %171 = llvm.mul %164, %7 : i64
    %172 = llvm.mul %18, %28 : i64
    %173 = llvm.add %171, %172 : i64
    %174 = llvm.add %173, %79 : i64
    %175 = llvm.getelementptr %31[%174] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    %176 = nvvm.wmma.load %175, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
    %177 = llvm.extractvalue %170[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %178 = llvm.extractvalue %170[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %179 = llvm.extractvalue %170[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %180 = llvm.extractvalue %170[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %181 = llvm.extractvalue %170[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %182 = llvm.extractvalue %170[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %183 = llvm.extractvalue %170[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %184 = llvm.extractvalue %170[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %185 = llvm.extractvalue %176[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %186 = llvm.extractvalue %176[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %187 = llvm.extractvalue %176[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %188 = llvm.extractvalue %176[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %189 = llvm.extractvalue %176[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %190 = llvm.extractvalue %176[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %191 = llvm.extractvalue %176[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %192 = llvm.extractvalue %176[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %193 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %194 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %195 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %196 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %197 = nvvm.wmma.mma %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
    %198 = llvm.add %84, %20 : i64
    llvm.br ^bb10(%198, %197 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
  ^bb29:  // pred: ^bb10
    %199 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %200 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %201 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %202 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
    %203 = llvm.mul %72, %28 : i64
    %204 = llvm.add %203, %79 : i64
    %205 = llvm.getelementptr %29[%204] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    nvvm.wmma.store %205, %1, %199, %200, %201, %202 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
    nvvm.barrier0
    llvm.br ^bb30(%48 : i64)
  ^bb30(%206: i64):  // 2 preds: ^bb29, ^bb37
    %207 = llvm.icmp "slt" %206, %21 : i64
    llvm.cond_br %207, ^bb31, ^bb38
  ^bb31:  // pred: ^bb30
    %208 = llvm.mul %46, %25 : i64
    llvm.br ^bb32(%208 : i64)
  ^bb32(%209: i64):  // 2 preds: ^bb31, ^bb36
    %210 = llvm.icmp "slt" %209, %21 : i64
    llvm.cond_br %210, ^bb33, ^bb37
  ^bb33:  // pred: ^bb32
    %211 = llvm.mul %44, %21 : i64
    %212 = llvm.add %206, %211 : i64
    %213 = llvm.mul %42, %21 : i64
    %214 = llvm.add %209, %213 : i64
    llvm.br ^bb34(%18 : i64)
  ^bb34(%215: i64):  // 2 preds: ^bb33, ^bb35
    %216 = llvm.icmp "slt" %215, %25 : i64
    llvm.cond_br %216, ^bb35, ^bb36
  ^bb35:  // pred: ^bb34
    %217 = llvm.add %209, %215 : i64
    %218 = llvm.mul %206, %28 : i64
    %219 = llvm.add %218, %217 : i64
    %220 = llvm.getelementptr %29[%219] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
    %221 = llvm.load %220 : !llvm.ptr<3> -> f16
    %222 = llvm.add %214, %215 : i64
    %223 = llvm.mul %212, %23 : i64
    %224 = llvm.add %223, %222 : i64
    %225 = llvm.getelementptr %arg2[%224] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
    llvm.store %221, %225 : f16, !llvm.ptr<1>
    %226 = llvm.add %215, %24 : i64
    llvm.br ^bb34(%226 : i64)
  ^bb36:  // pred: ^bb34
    %227 = llvm.add %209, %23 : i64
    llvm.br ^bb32(%227 : i64)
  ^bb37:  // pred: ^bb32
    %228 = llvm.add %206, %22 : i64
    llvm.br ^bb30(%228 : i64)
  ^bb38:  // pred: ^bb30
    nvvm.barrier0
    llvm.return
  }
}

// -----// IR Dump After TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @cuda_nvptx_fb target(<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>) {
  hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %c16 = arith.constant 16 : index
    %c1 = arith.constant 1 : index
    hal.return %c16, %c16, %c1 : index, index, index
  }
  builtin.module {
    llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
    llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
    llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
    llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
    llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
      %0 = llvm.mlir.constant(24 : index) : i32
      %1 = llvm.mlir.constant(40 : index) : i32
      %2 = llvm.mlir.constant(63 : index) : i64
      %3 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
      %4 = llvm.mlir.constant(0 : i64) : i64
      %5 = llvm.mlir.constant(0 : i64) : i64
      %6 = llvm.getelementptr %3[%4, %5] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
      %7 = llvm.mlir.constant(640 : index) : i64
      %8 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
      %9 = llvm.mlir.constant(0 : i64) : i64
      %10 = llvm.mlir.constant(3840 : i64) : i64
      %11 = llvm.getelementptr %8[%9, %10] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
      %12 = llvm.mlir.constant(768 : index) : i64
      %13 = llvm.mlir.constant(24 : index) : i64
      %14 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
      %15 = llvm.mlir.constant(0 : i64) : i64
      %16 = llvm.mlir.constant(8448 : i64) : i64
      %17 = llvm.getelementptr %14[%15, %16] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
      %18 = llvm.mlir.constant(0 : index) : i64
      %19 = llvm.mlir.constant(128 : index) : i64
      %20 = llvm.mlir.constant(16 : index) : i64
      %21 = llvm.mlir.constant(32 : index) : i64
      %22 = llvm.mlir.constant(2 : index) : i64
      %23 = llvm.mlir.constant(512 : index) : i64
      %24 = llvm.mlir.constant(1 : index) : i64
      %25 = llvm.mlir.constant(8 : index) : i64
      %26 = llvm.mlir.constant(-1 : index) : i64
      %27 = llvm.mlir.constant(3 : index) : i64
      %28 = llvm.mlir.constant(40 : index) : i64
      %29 = llvm.getelementptr %17[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
      %30 = llvm.getelementptr %11[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
      %31 = llvm.getelementptr %6[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
      %32 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
      %33 = llvm.and %32, %2 : i64
      %34 = llvm.icmp "eq" %33, %18 : i64
      llvm.intr.assume %34 : i1
      %35 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
      %36 = llvm.and %35, %2 : i64
      %37 = llvm.icmp "eq" %36, %18 : i64
      llvm.intr.assume %37 : i1
      %38 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
      %39 = llvm.and %38, %2 : i64
      %40 = llvm.icmp "eq" %39, %18 : i64
      llvm.intr.assume %40 : i1
      %41 = nvvm.read.ptx.sreg.ctaid.x : i32
      %42 = llvm.sext %41 : i32 to i64
      %43 = nvvm.read.ptx.sreg.ctaid.y : i32
      %44 = llvm.sext %43 : i32 to i64
      nvvm.barrier0
      %45 = nvvm.read.ptx.sreg.tid.x : i32
      %46 = llvm.sext %45 : i32 to i64
      %47 = nvvm.read.ptx.sreg.tid.y : i32
      %48 = llvm.sext %47 : i32 to i64
      llvm.br ^bb1(%48 : i64)
    ^bb1(%49: i64):  // 2 preds: ^bb0, ^bb8
      %50 = llvm.icmp "slt" %49, %21 : i64
      llvm.cond_br %50, ^bb2, ^bb9
    ^bb2:  // pred: ^bb1
      %51 = llvm.mul %46, %25 : i64
      llvm.br ^bb3(%51 : i64)
    ^bb3(%52: i64):  // 2 preds: ^bb2, ^bb7
      %53 = llvm.icmp "slt" %52, %21 : i64
      llvm.cond_br %53, ^bb4, ^bb8
    ^bb4:  // pred: ^bb3
      %54 = llvm.mul %44, %21 : i64
      %55 = llvm.add %49, %54 : i64
      %56 = llvm.mul %42, %21 : i64
      %57 = llvm.add %52, %56 : i64
      llvm.br ^bb5(%18 : i64)
    ^bb5(%58: i64):  // 2 preds: ^bb4, ^bb6
      %59 = llvm.icmp "slt" %58, %25 : i64
      llvm.cond_br %59, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %60 = llvm.add %57, %58 : i64
      %61 = llvm.mul %55, %23 : i64
      %62 = llvm.add %61, %60 : i64
      %63 = llvm.getelementptr %arg2[%62] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      %64 = llvm.load %63 : !llvm.ptr<1> -> f16
      %65 = llvm.add %52, %58 : i64
      %66 = llvm.mul %49, %28 : i64
      %67 = llvm.add %66, %65 : i64
      %68 = llvm.getelementptr %29[%67] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      llvm.store %64, %68 : f16, !llvm.ptr<3>
      %69 = llvm.add %58, %24 : i64
      llvm.br ^bb5(%69 : i64)
    ^bb7:  // pred: ^bb5
      %70 = llvm.add %52, %23 : i64
      llvm.br ^bb3(%70 : i64)
    ^bb8:  // pred: ^bb3
      %71 = llvm.add %49, %22 : i64
      llvm.br ^bb1(%71 : i64)
    ^bb9:  // pred: ^bb1
      nvvm.barrier0
      %72 = llvm.mul %48, %20 : i64
      %73 = llvm.icmp "slt" %46, %18 : i64
      %74 = llvm.sub %26, %46 : i64
      %75 = llvm.select %73, %74, %46 : i1, i64
      %76 = llvm.sdiv %75, %21 : i64
      %77 = llvm.sub %26, %76 : i64
      %78 = llvm.select %73, %77, %76 : i1, i64
      %79 = llvm.mul %78, %20 : i64
      %80 = llvm.mul %72, %28 : i64
      %81 = llvm.add %80, %79 : i64
      %82 = llvm.getelementptr %29[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %83 = nvvm.wmma.load %82, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      llvm.br ^bb10(%18, %83 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
    ^bb10(%84: i64, %85: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
      %86 = llvm.icmp "slt" %84, %19 : i64
      llvm.cond_br %86, ^bb11, ^bb29
    ^bb11:  // pred: ^bb10
      nvvm.barrier0
      llvm.br ^bb12(%48 : i64)
    ^bb12(%87: i64):  // 2 preds: ^bb11, ^bb19
      %88 = llvm.icmp "slt" %87, %21 : i64
      llvm.cond_br %88, ^bb13, ^bb20(%48 : i64)
    ^bb13:  // pred: ^bb12
      %89 = llvm.mul %46, %25 : i64
      llvm.br ^bb14(%89 : i64)
    ^bb14(%90: i64):  // 2 preds: ^bb13, ^bb18
      %91 = llvm.icmp "slt" %90, %20 : i64
      llvm.cond_br %91, ^bb15, ^bb19
    ^bb15:  // pred: ^bb14
      %92 = llvm.mul %44, %21 : i64
      %93 = llvm.add %87, %92 : i64
      %94 = llvm.add %84, %90 : i64
      %95 = llvm.icmp "slt" %84, %18 : i64
      %96 = llvm.sub %26, %84 : i64
      %97 = llvm.select %95, %96, %84 : i1, i64
      %98 = llvm.sdiv %97, %20 : i64
      %99 = llvm.sub %26, %98 : i64
      %100 = llvm.select %95, %99, %98 : i1, i64
      %101 = llvm.srem %100, %27 : i64
      %102 = llvm.icmp "slt" %101, %18 : i64
      %103 = llvm.add %101, %27 : i64
      %104 = llvm.select %102, %103, %101 : i1, i64
      llvm.br ^bb16(%18 : i64)
    ^bb16(%105: i64):  // 2 preds: ^bb15, ^bb17
      %106 = llvm.icmp "slt" %105, %25 : i64
      llvm.cond_br %106, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %107 = llvm.add %94, %105 : i64
      %108 = llvm.mul %93, %19 : i64
      %109 = llvm.add %108, %107 : i64
      %110 = llvm.getelementptr %arg0[%109] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      %111 = llvm.load %110 : !llvm.ptr<1> -> f16
      %112 = llvm.add %90, %105 : i64
      %113 = llvm.mul %104, %12 : i64
      %114 = llvm.mul %87, %13 : i64
      %115 = llvm.add %113, %114 : i64
      %116 = llvm.add %115, %112 : i64
      %117 = llvm.getelementptr %30[%116] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      llvm.store %111, %117 : f16, !llvm.ptr<3>
      %118 = llvm.add %105, %24 : i64
      llvm.br ^bb16(%118 : i64)
    ^bb18:  // pred: ^bb16
      %119 = llvm.add %90, %23 : i64
      llvm.br ^bb14(%119 : i64)
    ^bb19:  // pred: ^bb14
      %120 = llvm.add %87, %22 : i64
      llvm.br ^bb12(%120 : i64)
    ^bb20(%121: i64):  // 2 preds: ^bb12, ^bb27
      %122 = llvm.icmp "slt" %121, %20 : i64
      llvm.cond_br %122, ^bb21, ^bb28
    ^bb21:  // pred: ^bb20
      %123 = llvm.mul %46, %25 : i64
      llvm.br ^bb22(%123 : i64)
    ^bb22(%124: i64):  // 2 preds: ^bb21, ^bb26
      %125 = llvm.icmp "slt" %124, %21 : i64
      llvm.cond_br %125, ^bb23, ^bb27
    ^bb23:  // pred: ^bb22
      %126 = llvm.add %84, %121 : i64
      %127 = llvm.mul %42, %21 : i64
      %128 = llvm.add %124, %127 : i64
      %129 = llvm.icmp "slt" %84, %18 : i64
      %130 = llvm.sub %26, %84 : i64
      %131 = llvm.select %129, %130, %84 : i1, i64
      %132 = llvm.sdiv %131, %20 : i64
      %133 = llvm.sub %26, %132 : i64
      %134 = llvm.select %129, %133, %132 : i1, i64
      %135 = llvm.srem %134, %27 : i64
      %136 = llvm.icmp "slt" %135, %18 : i64
      %137 = llvm.add %135, %27 : i64
      %138 = llvm.select %136, %137, %135 : i1, i64
      llvm.br ^bb24(%18 : i64)
    ^bb24(%139: i64):  // 2 preds: ^bb23, ^bb25
      %140 = llvm.icmp "slt" %139, %25 : i64
      llvm.cond_br %140, ^bb25, ^bb26
    ^bb25:  // pred: ^bb24
      %141 = llvm.add %128, %139 : i64
      %142 = llvm.mul %126, %23 : i64
      %143 = llvm.add %142, %141 : i64
      %144 = llvm.getelementptr %arg1[%143] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      %145 = llvm.load %144 : !llvm.ptr<1> -> f16
      %146 = llvm.add %124, %139 : i64
      %147 = llvm.mul %138, %7 : i64
      %148 = llvm.mul %121, %28 : i64
      %149 = llvm.add %147, %148 : i64
      %150 = llvm.add %149, %146 : i64
      %151 = llvm.getelementptr %31[%150] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      llvm.store %145, %151 : f16, !llvm.ptr<3>
      %152 = llvm.add %139, %24 : i64
      llvm.br ^bb24(%152 : i64)
    ^bb26:  // pred: ^bb24
      %153 = llvm.add %124, %23 : i64
      llvm.br ^bb22(%153 : i64)
    ^bb27:  // pred: ^bb22
      %154 = llvm.add %121, %22 : i64
      llvm.br ^bb20(%154 : i64)
    ^bb28:  // pred: ^bb20
      nvvm.barrier0
      %155 = llvm.icmp "slt" %84, %18 : i64
      %156 = llvm.sub %26, %84 : i64
      %157 = llvm.select %155, %156, %84 : i1, i64
      %158 = llvm.sdiv %157, %20 : i64
      %159 = llvm.sub %26, %158 : i64
      %160 = llvm.select %155, %159, %158 : i1, i64
      %161 = llvm.srem %160, %27 : i64
      %162 = llvm.icmp "slt" %161, %18 : i64
      %163 = llvm.add %161, %27 : i64
      %164 = llvm.select %162, %163, %161 : i1, i64
      %165 = llvm.mul %164, %12 : i64
      %166 = llvm.mul %72, %13 : i64
      %167 = llvm.add %165, %166 : i64
      %168 = llvm.add %167, %18 : i64
      %169 = llvm.getelementptr %30[%168] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %170 = nvvm.wmma.load %169, %0 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      %171 = llvm.mul %164, %7 : i64
      %172 = llvm.mul %18, %28 : i64
      %173 = llvm.add %171, %172 : i64
      %174 = llvm.add %173, %79 : i64
      %175 = llvm.getelementptr %31[%174] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %176 = nvvm.wmma.load %175, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      %177 = llvm.extractvalue %170[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %178 = llvm.extractvalue %170[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %179 = llvm.extractvalue %170[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %180 = llvm.extractvalue %170[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %181 = llvm.extractvalue %170[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %182 = llvm.extractvalue %170[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %183 = llvm.extractvalue %170[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %184 = llvm.extractvalue %170[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %185 = llvm.extractvalue %176[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %186 = llvm.extractvalue %176[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %187 = llvm.extractvalue %176[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %188 = llvm.extractvalue %176[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %189 = llvm.extractvalue %176[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %190 = llvm.extractvalue %176[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %191 = llvm.extractvalue %176[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %192 = llvm.extractvalue %176[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %193 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %194 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %195 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %196 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %197 = nvvm.wmma.mma %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      %198 = llvm.add %84, %20 : i64
      llvm.br ^bb10(%198, %197 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
    ^bb29:  // pred: ^bb10
      %199 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %200 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %201 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %202 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %203 = llvm.mul %72, %28 : i64
      %204 = llvm.add %203, %79 : i64
      %205 = llvm.getelementptr %29[%204] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      nvvm.wmma.store %205, %1, %199, %200, %201, %202 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
      nvvm.barrier0
      llvm.br ^bb30(%48 : i64)
    ^bb30(%206: i64):  // 2 preds: ^bb29, ^bb37
      %207 = llvm.icmp "slt" %206, %21 : i64
      llvm.cond_br %207, ^bb31, ^bb38
    ^bb31:  // pred: ^bb30
      %208 = llvm.mul %46, %25 : i64
      llvm.br ^bb32(%208 : i64)
    ^bb32(%209: i64):  // 2 preds: ^bb31, ^bb36
      %210 = llvm.icmp "slt" %209, %21 : i64
      llvm.cond_br %210, ^bb33, ^bb37
    ^bb33:  // pred: ^bb32
      %211 = llvm.mul %44, %21 : i64
      %212 = llvm.add %206, %211 : i64
      %213 = llvm.mul %42, %21 : i64
      %214 = llvm.add %209, %213 : i64
      llvm.br ^bb34(%18 : i64)
    ^bb34(%215: i64):  // 2 preds: ^bb33, ^bb35
      %216 = llvm.icmp "slt" %215, %25 : i64
      llvm.cond_br %216, ^bb35, ^bb36
    ^bb35:  // pred: ^bb34
      %217 = llvm.add %209, %215 : i64
      %218 = llvm.mul %206, %28 : i64
      %219 = llvm.add %218, %217 : i64
      %220 = llvm.getelementptr %29[%219] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %221 = llvm.load %220 : !llvm.ptr<3> -> f16
      %222 = llvm.add %214, %215 : i64
      %223 = llvm.mul %212, %23 : i64
      %224 = llvm.add %223, %222 : i64
      %225 = llvm.getelementptr %arg2[%224] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      llvm.store %221, %225 : f16, !llvm.ptr<1>
      %226 = llvm.add %215, %24 : i64
      llvm.br ^bb34(%226 : i64)
    ^bb36:  // pred: ^bb34
      %227 = llvm.add %209, %23 : i64
      llvm.br ^bb32(%227 : i64)
    ^bb37:  // pred: ^bb32
      %228 = llvm.add %206, %22 : i64
      llvm.br ^bb30(%228 : i64)
    ^bb38:  // pred: ^bb30
      nvvm.barrier0
      llvm.return
    }
  }
}

// -----// IR Dump After TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
  hal.executable.variant public @cuda_nvptx_fb target(<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>) {
    hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device):
      %c16 = arith.constant 16 : index
      %c1 = arith.constant 1 : index
      hal.return %c16, %c16, %c1 : index, index, index
    }
    builtin.module {
      llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
      llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
      llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
      llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
      llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
        %0 = llvm.mlir.constant(24 : index) : i32
        %1 = llvm.mlir.constant(40 : index) : i32
        %2 = llvm.mlir.constant(63 : index) : i64
        %3 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
        %4 = llvm.mlir.constant(0 : i64) : i64
        %5 = llvm.mlir.constant(0 : i64) : i64
        %6 = llvm.getelementptr %3[%4, %5] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
        %7 = llvm.mlir.constant(640 : index) : i64
        %8 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
        %9 = llvm.mlir.constant(0 : i64) : i64
        %10 = llvm.mlir.constant(3840 : i64) : i64
        %11 = llvm.getelementptr %8[%9, %10] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
        %12 = llvm.mlir.constant(768 : index) : i64
        %13 = llvm.mlir.constant(24 : index) : i64
        %14 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
        %15 = llvm.mlir.constant(0 : i64) : i64
        %16 = llvm.mlir.constant(8448 : i64) : i64
        %17 = llvm.getelementptr %14[%15, %16] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
        %18 = llvm.mlir.constant(0 : index) : i64
        %19 = llvm.mlir.constant(128 : index) : i64
        %20 = llvm.mlir.constant(16 : index) : i64
        %21 = llvm.mlir.constant(32 : index) : i64
        %22 = llvm.mlir.constant(2 : index) : i64
        %23 = llvm.mlir.constant(512 : index) : i64
        %24 = llvm.mlir.constant(1 : index) : i64
        %25 = llvm.mlir.constant(8 : index) : i64
        %26 = llvm.mlir.constant(-1 : index) : i64
        %27 = llvm.mlir.constant(3 : index) : i64
        %28 = llvm.mlir.constant(40 : index) : i64
        %29 = llvm.getelementptr %17[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
        %30 = llvm.getelementptr %11[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
        %31 = llvm.getelementptr %6[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
        %32 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
        %33 = llvm.and %32, %2 : i64
        %34 = llvm.icmp "eq" %33, %18 : i64
        llvm.intr.assume %34 : i1
        %35 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
        %36 = llvm.and %35, %2 : i64
        %37 = llvm.icmp "eq" %36, %18 : i64
        llvm.intr.assume %37 : i1
        %38 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
        %39 = llvm.and %38, %2 : i64
        %40 = llvm.icmp "eq" %39, %18 : i64
        llvm.intr.assume %40 : i1
        %41 = nvvm.read.ptx.sreg.ctaid.x : i32
        %42 = llvm.sext %41 : i32 to i64
        %43 = nvvm.read.ptx.sreg.ctaid.y : i32
        %44 = llvm.sext %43 : i32 to i64
        nvvm.barrier0
        %45 = nvvm.read.ptx.sreg.tid.x : i32
        %46 = llvm.sext %45 : i32 to i64
        %47 = nvvm.read.ptx.sreg.tid.y : i32
        %48 = llvm.sext %47 : i32 to i64
        llvm.br ^bb1(%48 : i64)
      ^bb1(%49: i64):  // 2 preds: ^bb0, ^bb8
        %50 = llvm.icmp "slt" %49, %21 : i64
        llvm.cond_br %50, ^bb2, ^bb9
      ^bb2:  // pred: ^bb1
        %51 = llvm.mul %46, %25 : i64
        llvm.br ^bb3(%51 : i64)
      ^bb3(%52: i64):  // 2 preds: ^bb2, ^bb7
        %53 = llvm.icmp "slt" %52, %21 : i64
        llvm.cond_br %53, ^bb4, ^bb8
      ^bb4:  // pred: ^bb3
        %54 = llvm.mul %44, %21 : i64
        %55 = llvm.add %49, %54 : i64
        %56 = llvm.mul %42, %21 : i64
        %57 = llvm.add %52, %56 : i64
        llvm.br ^bb5(%18 : i64)
      ^bb5(%58: i64):  // 2 preds: ^bb4, ^bb6
        %59 = llvm.icmp "slt" %58, %25 : i64
        llvm.cond_br %59, ^bb6, ^bb7
      ^bb6:  // pred: ^bb5
        %60 = llvm.add %57, %58 : i64
        %61 = llvm.mul %55, %23 : i64
        %62 = llvm.add %61, %60 : i64
        %63 = llvm.getelementptr %arg2[%62] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        %64 = llvm.load %63 : !llvm.ptr<1> -> f16
        %65 = llvm.add %52, %58 : i64
        %66 = llvm.mul %49, %28 : i64
        %67 = llvm.add %66, %65 : i64
        %68 = llvm.getelementptr %29[%67] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        llvm.store %64, %68 : f16, !llvm.ptr<3>
        %69 = llvm.add %58, %24 : i64
        llvm.br ^bb5(%69 : i64)
      ^bb7:  // pred: ^bb5
        %70 = llvm.add %52, %23 : i64
        llvm.br ^bb3(%70 : i64)
      ^bb8:  // pred: ^bb3
        %71 = llvm.add %49, %22 : i64
        llvm.br ^bb1(%71 : i64)
      ^bb9:  // pred: ^bb1
        nvvm.barrier0
        %72 = llvm.mul %48, %20 : i64
        %73 = llvm.icmp "slt" %46, %18 : i64
        %74 = llvm.sub %26, %46 : i64
        %75 = llvm.select %73, %74, %46 : i1, i64
        %76 = llvm.sdiv %75, %21 : i64
        %77 = llvm.sub %26, %76 : i64
        %78 = llvm.select %73, %77, %76 : i1, i64
        %79 = llvm.mul %78, %20 : i64
        %80 = llvm.mul %72, %28 : i64
        %81 = llvm.add %80, %79 : i64
        %82 = llvm.getelementptr %29[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %83 = nvvm.wmma.load %82, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        llvm.br ^bb10(%18, %83 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
      ^bb10(%84: i64, %85: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
        %86 = llvm.icmp "slt" %84, %19 : i64
        llvm.cond_br %86, ^bb11, ^bb29
      ^bb11:  // pred: ^bb10
        nvvm.barrier0
        llvm.br ^bb12(%48 : i64)
      ^bb12(%87: i64):  // 2 preds: ^bb11, ^bb19
        %88 = llvm.icmp "slt" %87, %21 : i64
        llvm.cond_br %88, ^bb13, ^bb20(%48 : i64)
      ^bb13:  // pred: ^bb12
        %89 = llvm.mul %46, %25 : i64
        llvm.br ^bb14(%89 : i64)
      ^bb14(%90: i64):  // 2 preds: ^bb13, ^bb18
        %91 = llvm.icmp "slt" %90, %20 : i64
        llvm.cond_br %91, ^bb15, ^bb19
      ^bb15:  // pred: ^bb14
        %92 = llvm.mul %44, %21 : i64
        %93 = llvm.add %87, %92 : i64
        %94 = llvm.add %84, %90 : i64
        %95 = llvm.icmp "slt" %84, %18 : i64
        %96 = llvm.sub %26, %84 : i64
        %97 = llvm.select %95, %96, %84 : i1, i64
        %98 = llvm.sdiv %97, %20 : i64
        %99 = llvm.sub %26, %98 : i64
        %100 = llvm.select %95, %99, %98 : i1, i64
        %101 = llvm.srem %100, %27 : i64
        %102 = llvm.icmp "slt" %101, %18 : i64
        %103 = llvm.add %101, %27 : i64
        %104 = llvm.select %102, %103, %101 : i1, i64
        llvm.br ^bb16(%18 : i64)
      ^bb16(%105: i64):  // 2 preds: ^bb15, ^bb17
        %106 = llvm.icmp "slt" %105, %25 : i64
        llvm.cond_br %106, ^bb17, ^bb18
      ^bb17:  // pred: ^bb16
        %107 = llvm.add %94, %105 : i64
        %108 = llvm.mul %93, %19 : i64
        %109 = llvm.add %108, %107 : i64
        %110 = llvm.getelementptr %arg0[%109] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        %111 = llvm.load %110 : !llvm.ptr<1> -> f16
        %112 = llvm.add %90, %105 : i64
        %113 = llvm.mul %104, %12 : i64
        %114 = llvm.mul %87, %13 : i64
        %115 = llvm.add %113, %114 : i64
        %116 = llvm.add %115, %112 : i64
        %117 = llvm.getelementptr %30[%116] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        llvm.store %111, %117 : f16, !llvm.ptr<3>
        %118 = llvm.add %105, %24 : i64
        llvm.br ^bb16(%118 : i64)
      ^bb18:  // pred: ^bb16
        %119 = llvm.add %90, %23 : i64
        llvm.br ^bb14(%119 : i64)
      ^bb19:  // pred: ^bb14
        %120 = llvm.add %87, %22 : i64
        llvm.br ^bb12(%120 : i64)
      ^bb20(%121: i64):  // 2 preds: ^bb12, ^bb27
        %122 = llvm.icmp "slt" %121, %20 : i64
        llvm.cond_br %122, ^bb21, ^bb28
      ^bb21:  // pred: ^bb20
        %123 = llvm.mul %46, %25 : i64
        llvm.br ^bb22(%123 : i64)
      ^bb22(%124: i64):  // 2 preds: ^bb21, ^bb26
        %125 = llvm.icmp "slt" %124, %21 : i64
        llvm.cond_br %125, ^bb23, ^bb27
      ^bb23:  // pred: ^bb22
        %126 = llvm.add %84, %121 : i64
        %127 = llvm.mul %42, %21 : i64
        %128 = llvm.add %124, %127 : i64
        %129 = llvm.icmp "slt" %84, %18 : i64
        %130 = llvm.sub %26, %84 : i64
        %131 = llvm.select %129, %130, %84 : i1, i64
        %132 = llvm.sdiv %131, %20 : i64
        %133 = llvm.sub %26, %132 : i64
        %134 = llvm.select %129, %133, %132 : i1, i64
        %135 = llvm.srem %134, %27 : i64
        %136 = llvm.icmp "slt" %135, %18 : i64
        %137 = llvm.add %135, %27 : i64
        %138 = llvm.select %136, %137, %135 : i1, i64
        llvm.br ^bb24(%18 : i64)
      ^bb24(%139: i64):  // 2 preds: ^bb23, ^bb25
        %140 = llvm.icmp "slt" %139, %25 : i64
        llvm.cond_br %140, ^bb25, ^bb26
      ^bb25:  // pred: ^bb24
        %141 = llvm.add %128, %139 : i64
        %142 = llvm.mul %126, %23 : i64
        %143 = llvm.add %142, %141 : i64
        %144 = llvm.getelementptr %arg1[%143] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        %145 = llvm.load %144 : !llvm.ptr<1> -> f16
        %146 = llvm.add %124, %139 : i64
        %147 = llvm.mul %138, %7 : i64
        %148 = llvm.mul %121, %28 : i64
        %149 = llvm.add %147, %148 : i64
        %150 = llvm.add %149, %146 : i64
        %151 = llvm.getelementptr %31[%150] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        llvm.store %145, %151 : f16, !llvm.ptr<3>
        %152 = llvm.add %139, %24 : i64
        llvm.br ^bb24(%152 : i64)
      ^bb26:  // pred: ^bb24
        %153 = llvm.add %124, %23 : i64
        llvm.br ^bb22(%153 : i64)
      ^bb27:  // pred: ^bb22
        %154 = llvm.add %121, %22 : i64
        llvm.br ^bb20(%154 : i64)
      ^bb28:  // pred: ^bb20
        nvvm.barrier0
        %155 = llvm.icmp "slt" %84, %18 : i64
        %156 = llvm.sub %26, %84 : i64
        %157 = llvm.select %155, %156, %84 : i1, i64
        %158 = llvm.sdiv %157, %20 : i64
        %159 = llvm.sub %26, %158 : i64
        %160 = llvm.select %155, %159, %158 : i1, i64
        %161 = llvm.srem %160, %27 : i64
        %162 = llvm.icmp "slt" %161, %18 : i64
        %163 = llvm.add %161, %27 : i64
        %164 = llvm.select %162, %163, %161 : i1, i64
        %165 = llvm.mul %164, %12 : i64
        %166 = llvm.mul %72, %13 : i64
        %167 = llvm.add %165, %166 : i64
        %168 = llvm.add %167, %18 : i64
        %169 = llvm.getelementptr %30[%168] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %170 = nvvm.wmma.load %169, %0 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        %171 = llvm.mul %164, %7 : i64
        %172 = llvm.mul %18, %28 : i64
        %173 = llvm.add %171, %172 : i64
        %174 = llvm.add %173, %79 : i64
        %175 = llvm.getelementptr %31[%174] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %176 = nvvm.wmma.load %175, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        %177 = llvm.extractvalue %170[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %178 = llvm.extractvalue %170[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %179 = llvm.extractvalue %170[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %180 = llvm.extractvalue %170[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %181 = llvm.extractvalue %170[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %182 = llvm.extractvalue %170[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %183 = llvm.extractvalue %170[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %184 = llvm.extractvalue %170[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %185 = llvm.extractvalue %176[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %186 = llvm.extractvalue %176[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %187 = llvm.extractvalue %176[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %188 = llvm.extractvalue %176[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %189 = llvm.extractvalue %176[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %190 = llvm.extractvalue %176[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %191 = llvm.extractvalue %176[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %192 = llvm.extractvalue %176[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %193 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %194 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %195 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %196 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %197 = nvvm.wmma.mma %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        %198 = llvm.add %84, %20 : i64
        llvm.br ^bb10(%198, %197 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
      ^bb29:  // pred: ^bb10
        %199 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %200 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %201 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %202 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %203 = llvm.mul %72, %28 : i64
        %204 = llvm.add %203, %79 : i64
        %205 = llvm.getelementptr %29[%204] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        nvvm.wmma.store %205, %1, %199, %200, %201, %202 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
        nvvm.barrier0
        llvm.br ^bb30(%48 : i64)
      ^bb30(%206: i64):  // 2 preds: ^bb29, ^bb37
        %207 = llvm.icmp "slt" %206, %21 : i64
        llvm.cond_br %207, ^bb31, ^bb38
      ^bb31:  // pred: ^bb30
        %208 = llvm.mul %46, %25 : i64
        llvm.br ^bb32(%208 : i64)
      ^bb32(%209: i64):  // 2 preds: ^bb31, ^bb36
        %210 = llvm.icmp "slt" %209, %21 : i64
        llvm.cond_br %210, ^bb33, ^bb37
      ^bb33:  // pred: ^bb32
        %211 = llvm.mul %44, %21 : i64
        %212 = llvm.add %206, %211 : i64
        %213 = llvm.mul %42, %21 : i64
        %214 = llvm.add %209, %213 : i64
        llvm.br ^bb34(%18 : i64)
      ^bb34(%215: i64):  // 2 preds: ^bb33, ^bb35
        %216 = llvm.icmp "slt" %215, %25 : i64
        llvm.cond_br %216, ^bb35, ^bb36
      ^bb35:  // pred: ^bb34
        %217 = llvm.add %209, %215 : i64
        %218 = llvm.mul %206, %28 : i64
        %219 = llvm.add %218, %217 : i64
        %220 = llvm.getelementptr %29[%219] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %221 = llvm.load %220 : !llvm.ptr<3> -> f16
        %222 = llvm.add %214, %215 : i64
        %223 = llvm.mul %212, %23 : i64
        %224 = llvm.add %223, %222 : i64
        %225 = llvm.getelementptr %arg2[%224] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        llvm.store %221, %225 : f16, !llvm.ptr<1>
        %226 = llvm.add %215, %24 : i64
        llvm.br ^bb34(%226 : i64)
      ^bb36:  // pred: ^bb34
        %227 = llvm.add %209, %23 : i64
        llvm.br ^bb32(%227 : i64)
      ^bb37:  // pred: ^bb32
        %228 = llvm.add %206, %22 : i64
        llvm.br ^bb30(%228 : i64)
      ^bb38:  // pred: ^bb30
        nvvm.barrier0
        llvm.return
      }
    }
  }
}

// -----// IR Dump After ConvertToHALPass (iree-hal-conversion) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(24 : index) : i32
          %1 = llvm.mlir.constant(40 : index) : i32
          %2 = llvm.mlir.constant(63 : index) : i64
          %3 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %4 = llvm.mlir.constant(0 : i64) : i64
          %5 = llvm.mlir.constant(0 : i64) : i64
          %6 = llvm.getelementptr %3[%4, %5] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %7 = llvm.mlir.constant(640 : index) : i64
          %8 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %9 = llvm.mlir.constant(0 : i64) : i64
          %10 = llvm.mlir.constant(3840 : i64) : i64
          %11 = llvm.getelementptr %8[%9, %10] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(24 : index) : i64
          %14 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %15 = llvm.mlir.constant(0 : i64) : i64
          %16 = llvm.mlir.constant(8448 : i64) : i64
          %17 = llvm.getelementptr %14[%15, %16] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %18 = llvm.mlir.constant(0 : index) : i64
          %19 = llvm.mlir.constant(128 : index) : i64
          %20 = llvm.mlir.constant(16 : index) : i64
          %21 = llvm.mlir.constant(32 : index) : i64
          %22 = llvm.mlir.constant(2 : index) : i64
          %23 = llvm.mlir.constant(512 : index) : i64
          %24 = llvm.mlir.constant(1 : index) : i64
          %25 = llvm.mlir.constant(8 : index) : i64
          %26 = llvm.mlir.constant(-1 : index) : i64
          %27 = llvm.mlir.constant(3 : index) : i64
          %28 = llvm.mlir.constant(40 : index) : i64
          %29 = llvm.getelementptr %17[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %30 = llvm.getelementptr %11[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %31 = llvm.getelementptr %6[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %32 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %33 = llvm.and %32, %2 : i64
          %34 = llvm.icmp "eq" %33, %18 : i64
          llvm.intr.assume %34 : i1
          %35 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %36 = llvm.and %35, %2 : i64
          %37 = llvm.icmp "eq" %36, %18 : i64
          llvm.intr.assume %37 : i1
          %38 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %39 = llvm.and %38, %2 : i64
          %40 = llvm.icmp "eq" %39, %18 : i64
          llvm.intr.assume %40 : i1
          %41 = nvvm.read.ptx.sreg.ctaid.x : i32
          %42 = llvm.sext %41 : i32 to i64
          %43 = nvvm.read.ptx.sreg.ctaid.y : i32
          %44 = llvm.sext %43 : i32 to i64
          nvvm.barrier0
          %45 = nvvm.read.ptx.sreg.tid.x : i32
          %46 = llvm.sext %45 : i32 to i64
          %47 = nvvm.read.ptx.sreg.tid.y : i32
          %48 = llvm.sext %47 : i32 to i64
          llvm.br ^bb1(%48 : i64)
        ^bb1(%49: i64):  // 2 preds: ^bb0, ^bb8
          %50 = llvm.icmp "slt" %49, %21 : i64
          llvm.cond_br %50, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %51 = llvm.mul %46, %25 : i64
          llvm.br ^bb3(%51 : i64)
        ^bb3(%52: i64):  // 2 preds: ^bb2, ^bb7
          %53 = llvm.icmp "slt" %52, %21 : i64
          llvm.cond_br %53, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %54 = llvm.mul %44, %21 : i64
          %55 = llvm.add %49, %54 : i64
          %56 = llvm.mul %42, %21 : i64
          %57 = llvm.add %52, %56 : i64
          llvm.br ^bb5(%18 : i64)
        ^bb5(%58: i64):  // 2 preds: ^bb4, ^bb6
          %59 = llvm.icmp "slt" %58, %25 : i64
          llvm.cond_br %59, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %60 = llvm.add %57, %58 : i64
          %61 = llvm.mul %55, %23 : i64
          %62 = llvm.add %61, %60 : i64
          %63 = llvm.getelementptr %arg2[%62] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %64 = llvm.load %63 : !llvm.ptr<1> -> f16
          %65 = llvm.add %52, %58 : i64
          %66 = llvm.mul %49, %28 : i64
          %67 = llvm.add %66, %65 : i64
          %68 = llvm.getelementptr %29[%67] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %64, %68 : f16, !llvm.ptr<3>
          %69 = llvm.add %58, %24 : i64
          llvm.br ^bb5(%69 : i64)
        ^bb7:  // pred: ^bb5
          %70 = llvm.add %52, %23 : i64
          llvm.br ^bb3(%70 : i64)
        ^bb8:  // pred: ^bb3
          %71 = llvm.add %49, %22 : i64
          llvm.br ^bb1(%71 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %72 = llvm.mul %48, %20 : i64
          %73 = llvm.icmp "slt" %46, %18 : i64
          %74 = llvm.sub %26, %46 : i64
          %75 = llvm.select %73, %74, %46 : i1, i64
          %76 = llvm.sdiv %75, %21 : i64
          %77 = llvm.sub %26, %76 : i64
          %78 = llvm.select %73, %77, %76 : i1, i64
          %79 = llvm.mul %78, %20 : i64
          %80 = llvm.mul %72, %28 : i64
          %81 = llvm.add %80, %79 : i64
          %82 = llvm.getelementptr %29[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %83 = nvvm.wmma.load %82, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%18, %83 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%84: i64, %85: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %86 = llvm.icmp "slt" %84, %19 : i64
          llvm.cond_br %86, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%48 : i64)
        ^bb12(%87: i64):  // 2 preds: ^bb11, ^bb19
          %88 = llvm.icmp "slt" %87, %21 : i64
          llvm.cond_br %88, ^bb13, ^bb20(%48 : i64)
        ^bb13:  // pred: ^bb12
          %89 = llvm.mul %46, %25 : i64
          llvm.br ^bb14(%89 : i64)
        ^bb14(%90: i64):  // 2 preds: ^bb13, ^bb18
          %91 = llvm.icmp "slt" %90, %20 : i64
          llvm.cond_br %91, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %92 = llvm.mul %44, %21 : i64
          %93 = llvm.add %87, %92 : i64
          %94 = llvm.add %84, %90 : i64
          %95 = llvm.icmp "slt" %84, %18 : i64
          %96 = llvm.sub %26, %84 : i64
          %97 = llvm.select %95, %96, %84 : i1, i64
          %98 = llvm.sdiv %97, %20 : i64
          %99 = llvm.sub %26, %98 : i64
          %100 = llvm.select %95, %99, %98 : i1, i64
          %101 = llvm.srem %100, %27 : i64
          %102 = llvm.icmp "slt" %101, %18 : i64
          %103 = llvm.add %101, %27 : i64
          %104 = llvm.select %102, %103, %101 : i1, i64
          llvm.br ^bb16(%18 : i64)
        ^bb16(%105: i64):  // 2 preds: ^bb15, ^bb17
          %106 = llvm.icmp "slt" %105, %25 : i64
          llvm.cond_br %106, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %107 = llvm.add %94, %105 : i64
          %108 = llvm.mul %93, %19 : i64
          %109 = llvm.add %108, %107 : i64
          %110 = llvm.getelementptr %arg0[%109] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %111 = llvm.load %110 : !llvm.ptr<1> -> f16
          %112 = llvm.add %90, %105 : i64
          %113 = llvm.mul %104, %12 : i64
          %114 = llvm.mul %87, %13 : i64
          %115 = llvm.add %113, %114 : i64
          %116 = llvm.add %115, %112 : i64
          %117 = llvm.getelementptr %30[%116] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %111, %117 : f16, !llvm.ptr<3>
          %118 = llvm.add %105, %24 : i64
          llvm.br ^bb16(%118 : i64)
        ^bb18:  // pred: ^bb16
          %119 = llvm.add %90, %23 : i64
          llvm.br ^bb14(%119 : i64)
        ^bb19:  // pred: ^bb14
          %120 = llvm.add %87, %22 : i64
          llvm.br ^bb12(%120 : i64)
        ^bb20(%121: i64):  // 2 preds: ^bb12, ^bb27
          %122 = llvm.icmp "slt" %121, %20 : i64
          llvm.cond_br %122, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %123 = llvm.mul %46, %25 : i64
          llvm.br ^bb22(%123 : i64)
        ^bb22(%124: i64):  // 2 preds: ^bb21, ^bb26
          %125 = llvm.icmp "slt" %124, %21 : i64
          llvm.cond_br %125, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %126 = llvm.add %84, %121 : i64
          %127 = llvm.mul %42, %21 : i64
          %128 = llvm.add %124, %127 : i64
          %129 = llvm.icmp "slt" %84, %18 : i64
          %130 = llvm.sub %26, %84 : i64
          %131 = llvm.select %129, %130, %84 : i1, i64
          %132 = llvm.sdiv %131, %20 : i64
          %133 = llvm.sub %26, %132 : i64
          %134 = llvm.select %129, %133, %132 : i1, i64
          %135 = llvm.srem %134, %27 : i64
          %136 = llvm.icmp "slt" %135, %18 : i64
          %137 = llvm.add %135, %27 : i64
          %138 = llvm.select %136, %137, %135 : i1, i64
          llvm.br ^bb24(%18 : i64)
        ^bb24(%139: i64):  // 2 preds: ^bb23, ^bb25
          %140 = llvm.icmp "slt" %139, %25 : i64
          llvm.cond_br %140, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %141 = llvm.add %128, %139 : i64
          %142 = llvm.mul %126, %23 : i64
          %143 = llvm.add %142, %141 : i64
          %144 = llvm.getelementptr %arg1[%143] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %145 = llvm.load %144 : !llvm.ptr<1> -> f16
          %146 = llvm.add %124, %139 : i64
          %147 = llvm.mul %138, %7 : i64
          %148 = llvm.mul %121, %28 : i64
          %149 = llvm.add %147, %148 : i64
          %150 = llvm.add %149, %146 : i64
          %151 = llvm.getelementptr %31[%150] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %145, %151 : f16, !llvm.ptr<3>
          %152 = llvm.add %139, %24 : i64
          llvm.br ^bb24(%152 : i64)
        ^bb26:  // pred: ^bb24
          %153 = llvm.add %124, %23 : i64
          llvm.br ^bb22(%153 : i64)
        ^bb27:  // pred: ^bb22
          %154 = llvm.add %121, %22 : i64
          llvm.br ^bb20(%154 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %155 = llvm.icmp "slt" %84, %18 : i64
          %156 = llvm.sub %26, %84 : i64
          %157 = llvm.select %155, %156, %84 : i1, i64
          %158 = llvm.sdiv %157, %20 : i64
          %159 = llvm.sub %26, %158 : i64
          %160 = llvm.select %155, %159, %158 : i1, i64
          %161 = llvm.srem %160, %27 : i64
          %162 = llvm.icmp "slt" %161, %18 : i64
          %163 = llvm.add %161, %27 : i64
          %164 = llvm.select %162, %163, %161 : i1, i64
          %165 = llvm.mul %164, %12 : i64
          %166 = llvm.mul %72, %13 : i64
          %167 = llvm.add %165, %166 : i64
          %168 = llvm.add %167, %18 : i64
          %169 = llvm.getelementptr %30[%168] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %170 = nvvm.wmma.load %169, %0 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %171 = llvm.mul %164, %7 : i64
          %172 = llvm.mul %18, %28 : i64
          %173 = llvm.add %171, %172 : i64
          %174 = llvm.add %173, %79 : i64
          %175 = llvm.getelementptr %31[%174] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %176 = nvvm.wmma.load %175, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %177 = llvm.extractvalue %170[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %170[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %170[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %170[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %170[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %170[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %170[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %170[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %176[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %176[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %176[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %176[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = llvm.extractvalue %176[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %190 = llvm.extractvalue %176[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %191 = llvm.extractvalue %176[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %176[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %196 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %197 = nvvm.wmma.mma %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %198 = llvm.add %84, %20 : i64
          llvm.br ^bb10(%198, %197 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %199 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %200 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %201 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %202 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %203 = llvm.mul %72, %28 : i64
          %204 = llvm.add %203, %79 : i64
          %205 = llvm.getelementptr %29[%204] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %205, %1, %199, %200, %201, %202 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%48 : i64)
        ^bb30(%206: i64):  // 2 preds: ^bb29, ^bb37
          %207 = llvm.icmp "slt" %206, %21 : i64
          llvm.cond_br %207, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %208 = llvm.mul %46, %25 : i64
          llvm.br ^bb32(%208 : i64)
        ^bb32(%209: i64):  // 2 preds: ^bb31, ^bb36
          %210 = llvm.icmp "slt" %209, %21 : i64
          llvm.cond_br %210, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %211 = llvm.mul %44, %21 : i64
          %212 = llvm.add %206, %211 : i64
          %213 = llvm.mul %42, %21 : i64
          %214 = llvm.add %209, %213 : i64
          llvm.br ^bb34(%18 : i64)
        ^bb34(%215: i64):  // 2 preds: ^bb33, ^bb35
          %216 = llvm.icmp "slt" %215, %25 : i64
          llvm.cond_br %216, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %217 = llvm.add %209, %215 : i64
          %218 = llvm.mul %206, %28 : i64
          %219 = llvm.add %218, %217 : i64
          %220 = llvm.getelementptr %29[%219] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %221 = llvm.load %220 : !llvm.ptr<3> -> f16
          %222 = llvm.add %214, %215 : i64
          %223 = llvm.mul %212, %23 : i64
          %224 = llvm.add %223, %222 : i64
          %225 = llvm.getelementptr %arg2[%224] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %221, %225 : f16, !llvm.ptr<1>
          %226 = llvm.add %215, %24 : i64
          llvm.br ^bb34(%226 : i64)
        ^bb36:  // pred: ^bb34
          %227 = llvm.add %209, %23 : i64
          llvm.br ^bb32(%227 : i64)
        ^bb37:  // pred: ^bb32
          %228 = llvm.add %206, %22 : i64
          llvm.br ^bb30(%228 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %__device_0_1 = util.global.load immutable @__device_0 : !hal.device
    %allocator_2 = hal.device.allocator<%__device_0_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    %__device_0_4 = util.global.load immutable @__device_0 : !hal.device
    %allocator_5 = hal.device.allocator<%__device_0_4 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %__device_0_6 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %c0_7 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = hal.device.memoize<%__device_0_6 : !hal.device> affinity(%c-1_i64) -> !hal.command_buffer {
      %c3 = arith.constant 3 : index
      %cmd = hal.command_buffer.create device(%__device_0_6 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
      %2 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %c16 = arith.constant 16 : index
      %c1_13 = arith.constant 1 : index
      %exe = hal.executable.lookup device(%2 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1_13]) bindings([
        (%c0_7 : index)[%c0, %c131072], 
        (%c1 : index)[%c0, %c131072], 
        (%c2 : index)[%c0, %c524288]
      ]) flags("None")
      hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
      hal.command_buffer.finalize<%cmd : !hal.command_buffer>
      hal.return %cmd : !hal.command_buffer
    }
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%__device_0_6 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0_6 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands(%0) bindings([
      (%buffer : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_3 : !hal.buffer)[%c0_7, %c524288]
    ])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %dense_row_major_8 = hal.encoding_type<dense_row_major> : i32
    %element_type_f16_9 = hal.element_type<f16> : i32
    %c512_10 = arith.constant 512 : index
    %c512_11 = arith.constant 512 : index
    %c0_12 = arith.constant 0 : index
    %view = hal.buffer_view.create buffer(%buffer_3 : !hal.buffer)[%c0_12, %c524288] shape([%c512_10, %c512_11]) type(%element_type_f16_9) encoding(%dense_row_major_8) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineMemoizeRegionsPass (iree-hal-outline-memoize-regions) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(24 : index) : i32
          %1 = llvm.mlir.constant(40 : index) : i32
          %2 = llvm.mlir.constant(63 : index) : i64
          %3 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %4 = llvm.mlir.constant(0 : i64) : i64
          %5 = llvm.mlir.constant(0 : i64) : i64
          %6 = llvm.getelementptr %3[%4, %5] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %7 = llvm.mlir.constant(640 : index) : i64
          %8 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %9 = llvm.mlir.constant(0 : i64) : i64
          %10 = llvm.mlir.constant(3840 : i64) : i64
          %11 = llvm.getelementptr %8[%9, %10] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(24 : index) : i64
          %14 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %15 = llvm.mlir.constant(0 : i64) : i64
          %16 = llvm.mlir.constant(8448 : i64) : i64
          %17 = llvm.getelementptr %14[%15, %16] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %18 = llvm.mlir.constant(0 : index) : i64
          %19 = llvm.mlir.constant(128 : index) : i64
          %20 = llvm.mlir.constant(16 : index) : i64
          %21 = llvm.mlir.constant(32 : index) : i64
          %22 = llvm.mlir.constant(2 : index) : i64
          %23 = llvm.mlir.constant(512 : index) : i64
          %24 = llvm.mlir.constant(1 : index) : i64
          %25 = llvm.mlir.constant(8 : index) : i64
          %26 = llvm.mlir.constant(-1 : index) : i64
          %27 = llvm.mlir.constant(3 : index) : i64
          %28 = llvm.mlir.constant(40 : index) : i64
          %29 = llvm.getelementptr %17[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %30 = llvm.getelementptr %11[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %31 = llvm.getelementptr %6[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %32 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %33 = llvm.and %32, %2 : i64
          %34 = llvm.icmp "eq" %33, %18 : i64
          llvm.intr.assume %34 : i1
          %35 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %36 = llvm.and %35, %2 : i64
          %37 = llvm.icmp "eq" %36, %18 : i64
          llvm.intr.assume %37 : i1
          %38 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %39 = llvm.and %38, %2 : i64
          %40 = llvm.icmp "eq" %39, %18 : i64
          llvm.intr.assume %40 : i1
          %41 = nvvm.read.ptx.sreg.ctaid.x : i32
          %42 = llvm.sext %41 : i32 to i64
          %43 = nvvm.read.ptx.sreg.ctaid.y : i32
          %44 = llvm.sext %43 : i32 to i64
          nvvm.barrier0
          %45 = nvvm.read.ptx.sreg.tid.x : i32
          %46 = llvm.sext %45 : i32 to i64
          %47 = nvvm.read.ptx.sreg.tid.y : i32
          %48 = llvm.sext %47 : i32 to i64
          llvm.br ^bb1(%48 : i64)
        ^bb1(%49: i64):  // 2 preds: ^bb0, ^bb8
          %50 = llvm.icmp "slt" %49, %21 : i64
          llvm.cond_br %50, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %51 = llvm.mul %46, %25 : i64
          llvm.br ^bb3(%51 : i64)
        ^bb3(%52: i64):  // 2 preds: ^bb2, ^bb7
          %53 = llvm.icmp "slt" %52, %21 : i64
          llvm.cond_br %53, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %54 = llvm.mul %44, %21 : i64
          %55 = llvm.add %49, %54 : i64
          %56 = llvm.mul %42, %21 : i64
          %57 = llvm.add %52, %56 : i64
          llvm.br ^bb5(%18 : i64)
        ^bb5(%58: i64):  // 2 preds: ^bb4, ^bb6
          %59 = llvm.icmp "slt" %58, %25 : i64
          llvm.cond_br %59, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %60 = llvm.add %57, %58 : i64
          %61 = llvm.mul %55, %23 : i64
          %62 = llvm.add %61, %60 : i64
          %63 = llvm.getelementptr %arg2[%62] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %64 = llvm.load %63 : !llvm.ptr<1> -> f16
          %65 = llvm.add %52, %58 : i64
          %66 = llvm.mul %49, %28 : i64
          %67 = llvm.add %66, %65 : i64
          %68 = llvm.getelementptr %29[%67] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %64, %68 : f16, !llvm.ptr<3>
          %69 = llvm.add %58, %24 : i64
          llvm.br ^bb5(%69 : i64)
        ^bb7:  // pred: ^bb5
          %70 = llvm.add %52, %23 : i64
          llvm.br ^bb3(%70 : i64)
        ^bb8:  // pred: ^bb3
          %71 = llvm.add %49, %22 : i64
          llvm.br ^bb1(%71 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %72 = llvm.mul %48, %20 : i64
          %73 = llvm.icmp "slt" %46, %18 : i64
          %74 = llvm.sub %26, %46 : i64
          %75 = llvm.select %73, %74, %46 : i1, i64
          %76 = llvm.sdiv %75, %21 : i64
          %77 = llvm.sub %26, %76 : i64
          %78 = llvm.select %73, %77, %76 : i1, i64
          %79 = llvm.mul %78, %20 : i64
          %80 = llvm.mul %72, %28 : i64
          %81 = llvm.add %80, %79 : i64
          %82 = llvm.getelementptr %29[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %83 = nvvm.wmma.load %82, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%18, %83 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%84: i64, %85: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %86 = llvm.icmp "slt" %84, %19 : i64
          llvm.cond_br %86, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%48 : i64)
        ^bb12(%87: i64):  // 2 preds: ^bb11, ^bb19
          %88 = llvm.icmp "slt" %87, %21 : i64
          llvm.cond_br %88, ^bb13, ^bb20(%48 : i64)
        ^bb13:  // pred: ^bb12
          %89 = llvm.mul %46, %25 : i64
          llvm.br ^bb14(%89 : i64)
        ^bb14(%90: i64):  // 2 preds: ^bb13, ^bb18
          %91 = llvm.icmp "slt" %90, %20 : i64
          llvm.cond_br %91, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %92 = llvm.mul %44, %21 : i64
          %93 = llvm.add %87, %92 : i64
          %94 = llvm.add %84, %90 : i64
          %95 = llvm.icmp "slt" %84, %18 : i64
          %96 = llvm.sub %26, %84 : i64
          %97 = llvm.select %95, %96, %84 : i1, i64
          %98 = llvm.sdiv %97, %20 : i64
          %99 = llvm.sub %26, %98 : i64
          %100 = llvm.select %95, %99, %98 : i1, i64
          %101 = llvm.srem %100, %27 : i64
          %102 = llvm.icmp "slt" %101, %18 : i64
          %103 = llvm.add %101, %27 : i64
          %104 = llvm.select %102, %103, %101 : i1, i64
          llvm.br ^bb16(%18 : i64)
        ^bb16(%105: i64):  // 2 preds: ^bb15, ^bb17
          %106 = llvm.icmp "slt" %105, %25 : i64
          llvm.cond_br %106, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %107 = llvm.add %94, %105 : i64
          %108 = llvm.mul %93, %19 : i64
          %109 = llvm.add %108, %107 : i64
          %110 = llvm.getelementptr %arg0[%109] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %111 = llvm.load %110 : !llvm.ptr<1> -> f16
          %112 = llvm.add %90, %105 : i64
          %113 = llvm.mul %104, %12 : i64
          %114 = llvm.mul %87, %13 : i64
          %115 = llvm.add %113, %114 : i64
          %116 = llvm.add %115, %112 : i64
          %117 = llvm.getelementptr %30[%116] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %111, %117 : f16, !llvm.ptr<3>
          %118 = llvm.add %105, %24 : i64
          llvm.br ^bb16(%118 : i64)
        ^bb18:  // pred: ^bb16
          %119 = llvm.add %90, %23 : i64
          llvm.br ^bb14(%119 : i64)
        ^bb19:  // pred: ^bb14
          %120 = llvm.add %87, %22 : i64
          llvm.br ^bb12(%120 : i64)
        ^bb20(%121: i64):  // 2 preds: ^bb12, ^bb27
          %122 = llvm.icmp "slt" %121, %20 : i64
          llvm.cond_br %122, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %123 = llvm.mul %46, %25 : i64
          llvm.br ^bb22(%123 : i64)
        ^bb22(%124: i64):  // 2 preds: ^bb21, ^bb26
          %125 = llvm.icmp "slt" %124, %21 : i64
          llvm.cond_br %125, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %126 = llvm.add %84, %121 : i64
          %127 = llvm.mul %42, %21 : i64
          %128 = llvm.add %124, %127 : i64
          %129 = llvm.icmp "slt" %84, %18 : i64
          %130 = llvm.sub %26, %84 : i64
          %131 = llvm.select %129, %130, %84 : i1, i64
          %132 = llvm.sdiv %131, %20 : i64
          %133 = llvm.sub %26, %132 : i64
          %134 = llvm.select %129, %133, %132 : i1, i64
          %135 = llvm.srem %134, %27 : i64
          %136 = llvm.icmp "slt" %135, %18 : i64
          %137 = llvm.add %135, %27 : i64
          %138 = llvm.select %136, %137, %135 : i1, i64
          llvm.br ^bb24(%18 : i64)
        ^bb24(%139: i64):  // 2 preds: ^bb23, ^bb25
          %140 = llvm.icmp "slt" %139, %25 : i64
          llvm.cond_br %140, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %141 = llvm.add %128, %139 : i64
          %142 = llvm.mul %126, %23 : i64
          %143 = llvm.add %142, %141 : i64
          %144 = llvm.getelementptr %arg1[%143] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %145 = llvm.load %144 : !llvm.ptr<1> -> f16
          %146 = llvm.add %124, %139 : i64
          %147 = llvm.mul %138, %7 : i64
          %148 = llvm.mul %121, %28 : i64
          %149 = llvm.add %147, %148 : i64
          %150 = llvm.add %149, %146 : i64
          %151 = llvm.getelementptr %31[%150] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %145, %151 : f16, !llvm.ptr<3>
          %152 = llvm.add %139, %24 : i64
          llvm.br ^bb24(%152 : i64)
        ^bb26:  // pred: ^bb24
          %153 = llvm.add %124, %23 : i64
          llvm.br ^bb22(%153 : i64)
        ^bb27:  // pred: ^bb22
          %154 = llvm.add %121, %22 : i64
          llvm.br ^bb20(%154 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %155 = llvm.icmp "slt" %84, %18 : i64
          %156 = llvm.sub %26, %84 : i64
          %157 = llvm.select %155, %156, %84 : i1, i64
          %158 = llvm.sdiv %157, %20 : i64
          %159 = llvm.sub %26, %158 : i64
          %160 = llvm.select %155, %159, %158 : i1, i64
          %161 = llvm.srem %160, %27 : i64
          %162 = llvm.icmp "slt" %161, %18 : i64
          %163 = llvm.add %161, %27 : i64
          %164 = llvm.select %162, %163, %161 : i1, i64
          %165 = llvm.mul %164, %12 : i64
          %166 = llvm.mul %72, %13 : i64
          %167 = llvm.add %165, %166 : i64
          %168 = llvm.add %167, %18 : i64
          %169 = llvm.getelementptr %30[%168] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %170 = nvvm.wmma.load %169, %0 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %171 = llvm.mul %164, %7 : i64
          %172 = llvm.mul %18, %28 : i64
          %173 = llvm.add %171, %172 : i64
          %174 = llvm.add %173, %79 : i64
          %175 = llvm.getelementptr %31[%174] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %176 = nvvm.wmma.load %175, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %177 = llvm.extractvalue %170[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %170[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %170[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %170[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %170[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %170[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %170[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %170[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %176[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %176[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %176[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %176[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = llvm.extractvalue %176[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %190 = llvm.extractvalue %176[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %191 = llvm.extractvalue %176[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %176[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %196 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %197 = nvvm.wmma.mma %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %198 = llvm.add %84, %20 : i64
          llvm.br ^bb10(%198, %197 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %199 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %200 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %201 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %202 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %203 = llvm.mul %72, %28 : i64
          %204 = llvm.add %203, %79 : i64
          %205 = llvm.getelementptr %29[%204] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %205, %1, %199, %200, %201, %202 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%48 : i64)
        ^bb30(%206: i64):  // 2 preds: ^bb29, ^bb37
          %207 = llvm.icmp "slt" %206, %21 : i64
          llvm.cond_br %207, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %208 = llvm.mul %46, %25 : i64
          llvm.br ^bb32(%208 : i64)
        ^bb32(%209: i64):  // 2 preds: ^bb31, ^bb36
          %210 = llvm.icmp "slt" %209, %21 : i64
          llvm.cond_br %210, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %211 = llvm.mul %44, %21 : i64
          %212 = llvm.add %206, %211 : i64
          %213 = llvm.mul %42, %21 : i64
          %214 = llvm.add %209, %213 : i64
          llvm.br ^bb34(%18 : i64)
        ^bb34(%215: i64):  // 2 preds: ^bb33, ^bb35
          %216 = llvm.icmp "slt" %215, %25 : i64
          llvm.cond_br %216, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %217 = llvm.add %209, %215 : i64
          %218 = llvm.mul %206, %28 : i64
          %219 = llvm.add %218, %217 : i64
          %220 = llvm.getelementptr %29[%219] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %221 = llvm.load %220 : !llvm.ptr<3> -> f16
          %222 = llvm.add %214, %215 : i64
          %223 = llvm.mul %212, %23 : i64
          %224 = llvm.add %223, %222 : i64
          %225 = llvm.getelementptr %arg2[%224] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %221, %225 : f16, !llvm.ptr<1>
          %226 = llvm.add %215, %24 : i64
          llvm.br ^bb34(%226 : i64)
        ^bb36:  // pred: ^bb34
          %227 = llvm.add %209, %23 : i64
          llvm.br ^bb32(%227 : i64)
        ^bb37:  // pred: ^bb32
          %228 = llvm.add %206, %22 : i64
          llvm.br ^bb30(%228 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c0_0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %c3 = arith.constant 3 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %c16 = arith.constant 16 : index
    %c1_1 = arith.constant 1 : index
    %exe = hal.executable.lookup device(%0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1_1]) bindings([
      (%c0 : index)[%c0_0, %c131072], 
      (%c1 : index)[%c0_0, %c131072], 
      (%c2 : index)[%c0_0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %0 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %1 = scf.if %0 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      %2 = util.null : !hal.command_buffer
      scf.yield %2 : !hal.command_buffer
    }
    util.return %1 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %__device_0_1 = util.global.load immutable @__device_0 : !hal.device
    %allocator_2 = hal.device.allocator<%__device_0_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    %__device_0_4 = util.global.load immutable @__device_0 : !hal.device
    %allocator_5 = hal.device.allocator<%__device_0_4 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %__device_0_6 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %c0_7 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0_6, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%__device_0_6 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0_6 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands(%0) bindings([
      (%buffer : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_3 : !hal.buffer)[%c0_7, %c524288]
    ])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %dense_row_major_8 = hal.encoding_type<dense_row_major> : i32
    %element_type_f16_9 = hal.element_type<f16> : i32
    %c512_10 = arith.constant 512 : index
    %c512_11 = arith.constant 512 : index
    %c0_12 = arith.constant 0 : index
    %view = hal.buffer_view.create buffer(%buffer_3 : !hal.buffer)[%c0_12, %c524288] shape([%c512_10, %c512_11]) type(%element_type_f16_9) encoding(%dense_row_major_8) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FixupLegacySyncPass (iree-hal-fixup-legacy-sync) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(24 : index) : i32
          %1 = llvm.mlir.constant(40 : index) : i32
          %2 = llvm.mlir.constant(63 : index) : i64
          %3 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %4 = llvm.mlir.constant(0 : i64) : i64
          %5 = llvm.mlir.constant(0 : i64) : i64
          %6 = llvm.getelementptr %3[%4, %5] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %7 = llvm.mlir.constant(640 : index) : i64
          %8 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %9 = llvm.mlir.constant(0 : i64) : i64
          %10 = llvm.mlir.constant(3840 : i64) : i64
          %11 = llvm.getelementptr %8[%9, %10] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(24 : index) : i64
          %14 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %15 = llvm.mlir.constant(0 : i64) : i64
          %16 = llvm.mlir.constant(8448 : i64) : i64
          %17 = llvm.getelementptr %14[%15, %16] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %18 = llvm.mlir.constant(0 : index) : i64
          %19 = llvm.mlir.constant(128 : index) : i64
          %20 = llvm.mlir.constant(16 : index) : i64
          %21 = llvm.mlir.constant(32 : index) : i64
          %22 = llvm.mlir.constant(2 : index) : i64
          %23 = llvm.mlir.constant(512 : index) : i64
          %24 = llvm.mlir.constant(1 : index) : i64
          %25 = llvm.mlir.constant(8 : index) : i64
          %26 = llvm.mlir.constant(-1 : index) : i64
          %27 = llvm.mlir.constant(3 : index) : i64
          %28 = llvm.mlir.constant(40 : index) : i64
          %29 = llvm.getelementptr %17[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %30 = llvm.getelementptr %11[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %31 = llvm.getelementptr %6[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %32 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %33 = llvm.and %32, %2 : i64
          %34 = llvm.icmp "eq" %33, %18 : i64
          llvm.intr.assume %34 : i1
          %35 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %36 = llvm.and %35, %2 : i64
          %37 = llvm.icmp "eq" %36, %18 : i64
          llvm.intr.assume %37 : i1
          %38 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %39 = llvm.and %38, %2 : i64
          %40 = llvm.icmp "eq" %39, %18 : i64
          llvm.intr.assume %40 : i1
          %41 = nvvm.read.ptx.sreg.ctaid.x : i32
          %42 = llvm.sext %41 : i32 to i64
          %43 = nvvm.read.ptx.sreg.ctaid.y : i32
          %44 = llvm.sext %43 : i32 to i64
          nvvm.barrier0
          %45 = nvvm.read.ptx.sreg.tid.x : i32
          %46 = llvm.sext %45 : i32 to i64
          %47 = nvvm.read.ptx.sreg.tid.y : i32
          %48 = llvm.sext %47 : i32 to i64
          llvm.br ^bb1(%48 : i64)
        ^bb1(%49: i64):  // 2 preds: ^bb0, ^bb8
          %50 = llvm.icmp "slt" %49, %21 : i64
          llvm.cond_br %50, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %51 = llvm.mul %46, %25 : i64
          llvm.br ^bb3(%51 : i64)
        ^bb3(%52: i64):  // 2 preds: ^bb2, ^bb7
          %53 = llvm.icmp "slt" %52, %21 : i64
          llvm.cond_br %53, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %54 = llvm.mul %44, %21 : i64
          %55 = llvm.add %49, %54 : i64
          %56 = llvm.mul %42, %21 : i64
          %57 = llvm.add %52, %56 : i64
          llvm.br ^bb5(%18 : i64)
        ^bb5(%58: i64):  // 2 preds: ^bb4, ^bb6
          %59 = llvm.icmp "slt" %58, %25 : i64
          llvm.cond_br %59, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %60 = llvm.add %57, %58 : i64
          %61 = llvm.mul %55, %23 : i64
          %62 = llvm.add %61, %60 : i64
          %63 = llvm.getelementptr %arg2[%62] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %64 = llvm.load %63 : !llvm.ptr<1> -> f16
          %65 = llvm.add %52, %58 : i64
          %66 = llvm.mul %49, %28 : i64
          %67 = llvm.add %66, %65 : i64
          %68 = llvm.getelementptr %29[%67] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %64, %68 : f16, !llvm.ptr<3>
          %69 = llvm.add %58, %24 : i64
          llvm.br ^bb5(%69 : i64)
        ^bb7:  // pred: ^bb5
          %70 = llvm.add %52, %23 : i64
          llvm.br ^bb3(%70 : i64)
        ^bb8:  // pred: ^bb3
          %71 = llvm.add %49, %22 : i64
          llvm.br ^bb1(%71 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %72 = llvm.mul %48, %20 : i64
          %73 = llvm.icmp "slt" %46, %18 : i64
          %74 = llvm.sub %26, %46 : i64
          %75 = llvm.select %73, %74, %46 : i1, i64
          %76 = llvm.sdiv %75, %21 : i64
          %77 = llvm.sub %26, %76 : i64
          %78 = llvm.select %73, %77, %76 : i1, i64
          %79 = llvm.mul %78, %20 : i64
          %80 = llvm.mul %72, %28 : i64
          %81 = llvm.add %80, %79 : i64
          %82 = llvm.getelementptr %29[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %83 = nvvm.wmma.load %82, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%18, %83 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%84: i64, %85: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %86 = llvm.icmp "slt" %84, %19 : i64
          llvm.cond_br %86, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%48 : i64)
        ^bb12(%87: i64):  // 2 preds: ^bb11, ^bb19
          %88 = llvm.icmp "slt" %87, %21 : i64
          llvm.cond_br %88, ^bb13, ^bb20(%48 : i64)
        ^bb13:  // pred: ^bb12
          %89 = llvm.mul %46, %25 : i64
          llvm.br ^bb14(%89 : i64)
        ^bb14(%90: i64):  // 2 preds: ^bb13, ^bb18
          %91 = llvm.icmp "slt" %90, %20 : i64
          llvm.cond_br %91, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %92 = llvm.mul %44, %21 : i64
          %93 = llvm.add %87, %92 : i64
          %94 = llvm.add %84, %90 : i64
          %95 = llvm.icmp "slt" %84, %18 : i64
          %96 = llvm.sub %26, %84 : i64
          %97 = llvm.select %95, %96, %84 : i1, i64
          %98 = llvm.sdiv %97, %20 : i64
          %99 = llvm.sub %26, %98 : i64
          %100 = llvm.select %95, %99, %98 : i1, i64
          %101 = llvm.srem %100, %27 : i64
          %102 = llvm.icmp "slt" %101, %18 : i64
          %103 = llvm.add %101, %27 : i64
          %104 = llvm.select %102, %103, %101 : i1, i64
          llvm.br ^bb16(%18 : i64)
        ^bb16(%105: i64):  // 2 preds: ^bb15, ^bb17
          %106 = llvm.icmp "slt" %105, %25 : i64
          llvm.cond_br %106, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %107 = llvm.add %94, %105 : i64
          %108 = llvm.mul %93, %19 : i64
          %109 = llvm.add %108, %107 : i64
          %110 = llvm.getelementptr %arg0[%109] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %111 = llvm.load %110 : !llvm.ptr<1> -> f16
          %112 = llvm.add %90, %105 : i64
          %113 = llvm.mul %104, %12 : i64
          %114 = llvm.mul %87, %13 : i64
          %115 = llvm.add %113, %114 : i64
          %116 = llvm.add %115, %112 : i64
          %117 = llvm.getelementptr %30[%116] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %111, %117 : f16, !llvm.ptr<3>
          %118 = llvm.add %105, %24 : i64
          llvm.br ^bb16(%118 : i64)
        ^bb18:  // pred: ^bb16
          %119 = llvm.add %90, %23 : i64
          llvm.br ^bb14(%119 : i64)
        ^bb19:  // pred: ^bb14
          %120 = llvm.add %87, %22 : i64
          llvm.br ^bb12(%120 : i64)
        ^bb20(%121: i64):  // 2 preds: ^bb12, ^bb27
          %122 = llvm.icmp "slt" %121, %20 : i64
          llvm.cond_br %122, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %123 = llvm.mul %46, %25 : i64
          llvm.br ^bb22(%123 : i64)
        ^bb22(%124: i64):  // 2 preds: ^bb21, ^bb26
          %125 = llvm.icmp "slt" %124, %21 : i64
          llvm.cond_br %125, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %126 = llvm.add %84, %121 : i64
          %127 = llvm.mul %42, %21 : i64
          %128 = llvm.add %124, %127 : i64
          %129 = llvm.icmp "slt" %84, %18 : i64
          %130 = llvm.sub %26, %84 : i64
          %131 = llvm.select %129, %130, %84 : i1, i64
          %132 = llvm.sdiv %131, %20 : i64
          %133 = llvm.sub %26, %132 : i64
          %134 = llvm.select %129, %133, %132 : i1, i64
          %135 = llvm.srem %134, %27 : i64
          %136 = llvm.icmp "slt" %135, %18 : i64
          %137 = llvm.add %135, %27 : i64
          %138 = llvm.select %136, %137, %135 : i1, i64
          llvm.br ^bb24(%18 : i64)
        ^bb24(%139: i64):  // 2 preds: ^bb23, ^bb25
          %140 = llvm.icmp "slt" %139, %25 : i64
          llvm.cond_br %140, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %141 = llvm.add %128, %139 : i64
          %142 = llvm.mul %126, %23 : i64
          %143 = llvm.add %142, %141 : i64
          %144 = llvm.getelementptr %arg1[%143] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %145 = llvm.load %144 : !llvm.ptr<1> -> f16
          %146 = llvm.add %124, %139 : i64
          %147 = llvm.mul %138, %7 : i64
          %148 = llvm.mul %121, %28 : i64
          %149 = llvm.add %147, %148 : i64
          %150 = llvm.add %149, %146 : i64
          %151 = llvm.getelementptr %31[%150] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %145, %151 : f16, !llvm.ptr<3>
          %152 = llvm.add %139, %24 : i64
          llvm.br ^bb24(%152 : i64)
        ^bb26:  // pred: ^bb24
          %153 = llvm.add %124, %23 : i64
          llvm.br ^bb22(%153 : i64)
        ^bb27:  // pred: ^bb22
          %154 = llvm.add %121, %22 : i64
          llvm.br ^bb20(%154 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %155 = llvm.icmp "slt" %84, %18 : i64
          %156 = llvm.sub %26, %84 : i64
          %157 = llvm.select %155, %156, %84 : i1, i64
          %158 = llvm.sdiv %157, %20 : i64
          %159 = llvm.sub %26, %158 : i64
          %160 = llvm.select %155, %159, %158 : i1, i64
          %161 = llvm.srem %160, %27 : i64
          %162 = llvm.icmp "slt" %161, %18 : i64
          %163 = llvm.add %161, %27 : i64
          %164 = llvm.select %162, %163, %161 : i1, i64
          %165 = llvm.mul %164, %12 : i64
          %166 = llvm.mul %72, %13 : i64
          %167 = llvm.add %165, %166 : i64
          %168 = llvm.add %167, %18 : i64
          %169 = llvm.getelementptr %30[%168] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %170 = nvvm.wmma.load %169, %0 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %171 = llvm.mul %164, %7 : i64
          %172 = llvm.mul %18, %28 : i64
          %173 = llvm.add %171, %172 : i64
          %174 = llvm.add %173, %79 : i64
          %175 = llvm.getelementptr %31[%174] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %176 = nvvm.wmma.load %175, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %177 = llvm.extractvalue %170[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %170[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %170[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %170[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %170[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %170[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %170[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %170[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %176[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %176[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %176[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %176[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = llvm.extractvalue %176[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %190 = llvm.extractvalue %176[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %191 = llvm.extractvalue %176[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %176[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %196 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %197 = nvvm.wmma.mma %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %198 = llvm.add %84, %20 : i64
          llvm.br ^bb10(%198, %197 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %199 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %200 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %201 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %202 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %203 = llvm.mul %72, %28 : i64
          %204 = llvm.add %203, %79 : i64
          %205 = llvm.getelementptr %29[%204] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %205, %1, %199, %200, %201, %202 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%48 : i64)
        ^bb30(%206: i64):  // 2 preds: ^bb29, ^bb37
          %207 = llvm.icmp "slt" %206, %21 : i64
          llvm.cond_br %207, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %208 = llvm.mul %46, %25 : i64
          llvm.br ^bb32(%208 : i64)
        ^bb32(%209: i64):  // 2 preds: ^bb31, ^bb36
          %210 = llvm.icmp "slt" %209, %21 : i64
          llvm.cond_br %210, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %211 = llvm.mul %44, %21 : i64
          %212 = llvm.add %206, %211 : i64
          %213 = llvm.mul %42, %21 : i64
          %214 = llvm.add %209, %213 : i64
          llvm.br ^bb34(%18 : i64)
        ^bb34(%215: i64):  // 2 preds: ^bb33, ^bb35
          %216 = llvm.icmp "slt" %215, %25 : i64
          llvm.cond_br %216, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %217 = llvm.add %209, %215 : i64
          %218 = llvm.mul %206, %28 : i64
          %219 = llvm.add %218, %217 : i64
          %220 = llvm.getelementptr %29[%219] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %221 = llvm.load %220 : !llvm.ptr<3> -> f16
          %222 = llvm.add %214, %215 : i64
          %223 = llvm.mul %212, %23 : i64
          %224 = llvm.add %223, %222 : i64
          %225 = llvm.getelementptr %arg2[%224] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %221, %225 : f16, !llvm.ptr<1>
          %226 = llvm.add %215, %24 : i64
          llvm.br ^bb34(%226 : i64)
        ^bb36:  // pred: ^bb34
          %227 = llvm.add %209, %23 : i64
          llvm.br ^bb32(%227 : i64)
        ^bb37:  // pred: ^bb32
          %228 = llvm.add %206, %22 : i64
          llvm.br ^bb30(%228 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c0_0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %c3 = arith.constant 3 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %c16 = arith.constant 16 : index
    %c1_1 = arith.constant 1 : index
    %exe = hal.executable.lookup device(%0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1_1]) bindings([
      (%c0 : index)[%c0_0, %c131072], 
      (%c1 : index)[%c0_0, %c131072], 
      (%c2 : index)[%c0_0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %0 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %1 = scf.if %0 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      %2 = util.null : !hal.command_buffer
      scf.yield %2 : !hal.command_buffer
    }
    util.return %1 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %__device_0_1 = util.global.load immutable @__device_0 : !hal.device
    %allocator_2 = hal.device.allocator<%__device_0_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    %__device_0_4 = util.global.load immutable @__device_0 : !hal.device
    %allocator_5 = hal.device.allocator<%__device_0_4 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %__device_0_6 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %c0_7 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0_6, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%__device_0_6 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0_6 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands(%0) bindings([
      (%buffer : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_3 : !hal.buffer)[%c0_7, %c524288]
    ])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %dense_row_major_8 = hal.encoding_type<dense_row_major> : i32
    %element_type_f16_9 = hal.element_type<f16> : i32
    %c512_10 = arith.constant 512 : index
    %c512_11 = arith.constant 512 : index
    %c0_12 = arith.constant 0 : index
    %view = hal.buffer_view.create buffer(%buffer_3 : !hal.buffer)[%c0_12, %c524288] shape([%c512_10, %c512_11]) type(%element_type_f16_9) encoding(%dense_row_major_8) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(24 : index) : i32
          %1 = llvm.mlir.constant(40 : index) : i32
          %2 = llvm.mlir.constant(63 : index) : i64
          %3 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %4 = llvm.mlir.constant(0 : i64) : i64
          %5 = llvm.mlir.constant(0 : i64) : i64
          %6 = llvm.getelementptr %3[%4, %5] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %7 = llvm.mlir.constant(640 : index) : i64
          %8 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %9 = llvm.mlir.constant(0 : i64) : i64
          %10 = llvm.mlir.constant(3840 : i64) : i64
          %11 = llvm.getelementptr %8[%9, %10] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(24 : index) : i64
          %14 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %15 = llvm.mlir.constant(0 : i64) : i64
          %16 = llvm.mlir.constant(8448 : i64) : i64
          %17 = llvm.getelementptr %14[%15, %16] : (!llvm.ptr<3>, i64, i64) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %18 = llvm.mlir.constant(0 : index) : i64
          %19 = llvm.mlir.constant(128 : index) : i64
          %20 = llvm.mlir.constant(16 : index) : i64
          %21 = llvm.mlir.constant(32 : index) : i64
          %22 = llvm.mlir.constant(2 : index) : i64
          %23 = llvm.mlir.constant(512 : index) : i64
          %24 = llvm.mlir.constant(1 : index) : i64
          %25 = llvm.mlir.constant(8 : index) : i64
          %26 = llvm.mlir.constant(-1 : index) : i64
          %27 = llvm.mlir.constant(3 : index) : i64
          %28 = llvm.mlir.constant(40 : index) : i64
          %29 = llvm.getelementptr %17[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %30 = llvm.getelementptr %11[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %31 = llvm.getelementptr %6[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %32 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %33 = llvm.and %32, %2 : i64
          %34 = llvm.icmp "eq" %33, %18 : i64
          llvm.intr.assume %34 : i1
          %35 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %36 = llvm.and %35, %2 : i64
          %37 = llvm.icmp "eq" %36, %18 : i64
          llvm.intr.assume %37 : i1
          %38 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %39 = llvm.and %38, %2 : i64
          %40 = llvm.icmp "eq" %39, %18 : i64
          llvm.intr.assume %40 : i1
          %41 = nvvm.read.ptx.sreg.ctaid.x : i32
          %42 = llvm.sext %41 : i32 to i64
          %43 = nvvm.read.ptx.sreg.ctaid.y : i32
          %44 = llvm.sext %43 : i32 to i64
          nvvm.barrier0
          %45 = nvvm.read.ptx.sreg.tid.x : i32
          %46 = llvm.sext %45 : i32 to i64
          %47 = nvvm.read.ptx.sreg.tid.y : i32
          %48 = llvm.sext %47 : i32 to i64
          llvm.br ^bb1(%48 : i64)
        ^bb1(%49: i64):  // 2 preds: ^bb0, ^bb8
          %50 = llvm.icmp "slt" %49, %21 : i64
          llvm.cond_br %50, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %51 = llvm.mul %46, %25 : i64
          llvm.br ^bb3(%51 : i64)
        ^bb3(%52: i64):  // 2 preds: ^bb2, ^bb7
          %53 = llvm.icmp "slt" %52, %21 : i64
          llvm.cond_br %53, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %54 = llvm.mul %44, %21 : i64
          %55 = llvm.add %49, %54 : i64
          %56 = llvm.mul %42, %21 : i64
          %57 = llvm.add %52, %56 : i64
          llvm.br ^bb5(%18 : i64)
        ^bb5(%58: i64):  // 2 preds: ^bb4, ^bb6
          %59 = llvm.icmp "slt" %58, %25 : i64
          llvm.cond_br %59, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %60 = llvm.add %57, %58 : i64
          %61 = llvm.mul %55, %23 : i64
          %62 = llvm.add %61, %60 : i64
          %63 = llvm.getelementptr %arg2[%62] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %64 = llvm.load %63 : !llvm.ptr<1> -> f16
          %65 = llvm.add %52, %58 : i64
          %66 = llvm.mul %49, %28 : i64
          %67 = llvm.add %66, %65 : i64
          %68 = llvm.getelementptr %29[%67] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %64, %68 : f16, !llvm.ptr<3>
          %69 = llvm.add %58, %24 : i64
          llvm.br ^bb5(%69 : i64)
        ^bb7:  // pred: ^bb5
          %70 = llvm.add %52, %23 : i64
          llvm.br ^bb3(%70 : i64)
        ^bb8:  // pred: ^bb3
          %71 = llvm.add %49, %22 : i64
          llvm.br ^bb1(%71 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %72 = llvm.mul %48, %20 : i64
          %73 = llvm.icmp "slt" %46, %18 : i64
          %74 = llvm.sub %26, %46 : i64
          %75 = llvm.select %73, %74, %46 : i1, i64
          %76 = llvm.sdiv %75, %21 : i64
          %77 = llvm.sub %26, %76 : i64
          %78 = llvm.select %73, %77, %76 : i1, i64
          %79 = llvm.mul %78, %20 : i64
          %80 = llvm.mul %72, %28 : i64
          %81 = llvm.add %80, %79 : i64
          %82 = llvm.getelementptr %29[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %83 = nvvm.wmma.load %82, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%18, %83 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%84: i64, %85: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %86 = llvm.icmp "slt" %84, %19 : i64
          llvm.cond_br %86, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%48 : i64)
        ^bb12(%87: i64):  // 2 preds: ^bb11, ^bb19
          %88 = llvm.icmp "slt" %87, %21 : i64
          llvm.cond_br %88, ^bb13, ^bb20(%48 : i64)
        ^bb13:  // pred: ^bb12
          %89 = llvm.mul %46, %25 : i64
          llvm.br ^bb14(%89 : i64)
        ^bb14(%90: i64):  // 2 preds: ^bb13, ^bb18
          %91 = llvm.icmp "slt" %90, %20 : i64
          llvm.cond_br %91, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %92 = llvm.mul %44, %21 : i64
          %93 = llvm.add %87, %92 : i64
          %94 = llvm.add %84, %90 : i64
          %95 = llvm.icmp "slt" %84, %18 : i64
          %96 = llvm.sub %26, %84 : i64
          %97 = llvm.select %95, %96, %84 : i1, i64
          %98 = llvm.sdiv %97, %20 : i64
          %99 = llvm.sub %26, %98 : i64
          %100 = llvm.select %95, %99, %98 : i1, i64
          %101 = llvm.srem %100, %27 : i64
          %102 = llvm.icmp "slt" %101, %18 : i64
          %103 = llvm.add %101, %27 : i64
          %104 = llvm.select %102, %103, %101 : i1, i64
          llvm.br ^bb16(%18 : i64)
        ^bb16(%105: i64):  // 2 preds: ^bb15, ^bb17
          %106 = llvm.icmp "slt" %105, %25 : i64
          llvm.cond_br %106, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %107 = llvm.add %94, %105 : i64
          %108 = llvm.mul %93, %19 : i64
          %109 = llvm.add %108, %107 : i64
          %110 = llvm.getelementptr %arg0[%109] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %111 = llvm.load %110 : !llvm.ptr<1> -> f16
          %112 = llvm.add %90, %105 : i64
          %113 = llvm.mul %104, %12 : i64
          %114 = llvm.mul %87, %13 : i64
          %115 = llvm.add %113, %114 : i64
          %116 = llvm.add %115, %112 : i64
          %117 = llvm.getelementptr %30[%116] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %111, %117 : f16, !llvm.ptr<3>
          %118 = llvm.add %105, %24 : i64
          llvm.br ^bb16(%118 : i64)
        ^bb18:  // pred: ^bb16
          %119 = llvm.add %90, %23 : i64
          llvm.br ^bb14(%119 : i64)
        ^bb19:  // pred: ^bb14
          %120 = llvm.add %87, %22 : i64
          llvm.br ^bb12(%120 : i64)
        ^bb20(%121: i64):  // 2 preds: ^bb12, ^bb27
          %122 = llvm.icmp "slt" %121, %20 : i64
          llvm.cond_br %122, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %123 = llvm.mul %46, %25 : i64
          llvm.br ^bb22(%123 : i64)
        ^bb22(%124: i64):  // 2 preds: ^bb21, ^bb26
          %125 = llvm.icmp "slt" %124, %21 : i64
          llvm.cond_br %125, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %126 = llvm.add %84, %121 : i64
          %127 = llvm.mul %42, %21 : i64
          %128 = llvm.add %124, %127 : i64
          %129 = llvm.icmp "slt" %84, %18 : i64
          %130 = llvm.sub %26, %84 : i64
          %131 = llvm.select %129, %130, %84 : i1, i64
          %132 = llvm.sdiv %131, %20 : i64
          %133 = llvm.sub %26, %132 : i64
          %134 = llvm.select %129, %133, %132 : i1, i64
          %135 = llvm.srem %134, %27 : i64
          %136 = llvm.icmp "slt" %135, %18 : i64
          %137 = llvm.add %135, %27 : i64
          %138 = llvm.select %136, %137, %135 : i1, i64
          llvm.br ^bb24(%18 : i64)
        ^bb24(%139: i64):  // 2 preds: ^bb23, ^bb25
          %140 = llvm.icmp "slt" %139, %25 : i64
          llvm.cond_br %140, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %141 = llvm.add %128, %139 : i64
          %142 = llvm.mul %126, %23 : i64
          %143 = llvm.add %142, %141 : i64
          %144 = llvm.getelementptr %arg1[%143] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %145 = llvm.load %144 : !llvm.ptr<1> -> f16
          %146 = llvm.add %124, %139 : i64
          %147 = llvm.mul %138, %7 : i64
          %148 = llvm.mul %121, %28 : i64
          %149 = llvm.add %147, %148 : i64
          %150 = llvm.add %149, %146 : i64
          %151 = llvm.getelementptr %31[%150] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %145, %151 : f16, !llvm.ptr<3>
          %152 = llvm.add %139, %24 : i64
          llvm.br ^bb24(%152 : i64)
        ^bb26:  // pred: ^bb24
          %153 = llvm.add %124, %23 : i64
          llvm.br ^bb22(%153 : i64)
        ^bb27:  // pred: ^bb22
          %154 = llvm.add %121, %22 : i64
          llvm.br ^bb20(%154 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %155 = llvm.icmp "slt" %84, %18 : i64
          %156 = llvm.sub %26, %84 : i64
          %157 = llvm.select %155, %156, %84 : i1, i64
          %158 = llvm.sdiv %157, %20 : i64
          %159 = llvm.sub %26, %158 : i64
          %160 = llvm.select %155, %159, %158 : i1, i64
          %161 = llvm.srem %160, %27 : i64
          %162 = llvm.icmp "slt" %161, %18 : i64
          %163 = llvm.add %161, %27 : i64
          %164 = llvm.select %162, %163, %161 : i1, i64
          %165 = llvm.mul %164, %12 : i64
          %166 = llvm.mul %72, %13 : i64
          %167 = llvm.add %165, %166 : i64
          %168 = llvm.add %167, %18 : i64
          %169 = llvm.getelementptr %30[%168] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %170 = nvvm.wmma.load %169, %0 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %171 = llvm.mul %164, %7 : i64
          %172 = llvm.mul %18, %28 : i64
          %173 = llvm.add %171, %172 : i64
          %174 = llvm.add %173, %79 : i64
          %175 = llvm.getelementptr %31[%174] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %176 = nvvm.wmma.load %175, %1 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %177 = llvm.extractvalue %170[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %170[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %170[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %170[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %170[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %170[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %170[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %170[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %176[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %176[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %176[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %176[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = llvm.extractvalue %176[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %190 = llvm.extractvalue %176[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %191 = llvm.extractvalue %176[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %176[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %196 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %197 = nvvm.wmma.mma %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %198 = llvm.add %84, %20 : i64
          llvm.br ^bb10(%198, %197 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %199 = llvm.extractvalue %85[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %200 = llvm.extractvalue %85[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %201 = llvm.extractvalue %85[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %202 = llvm.extractvalue %85[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %203 = llvm.mul %72, %28 : i64
          %204 = llvm.add %203, %79 : i64
          %205 = llvm.getelementptr %29[%204] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %205, %1, %199, %200, %201, %202 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%48 : i64)
        ^bb30(%206: i64):  // 2 preds: ^bb29, ^bb37
          %207 = llvm.icmp "slt" %206, %21 : i64
          llvm.cond_br %207, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %208 = llvm.mul %46, %25 : i64
          llvm.br ^bb32(%208 : i64)
        ^bb32(%209: i64):  // 2 preds: ^bb31, ^bb36
          %210 = llvm.icmp "slt" %209, %21 : i64
          llvm.cond_br %210, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %211 = llvm.mul %44, %21 : i64
          %212 = llvm.add %206, %211 : i64
          %213 = llvm.mul %42, %21 : i64
          %214 = llvm.add %209, %213 : i64
          llvm.br ^bb34(%18 : i64)
        ^bb34(%215: i64):  // 2 preds: ^bb33, ^bb35
          %216 = llvm.icmp "slt" %215, %25 : i64
          llvm.cond_br %216, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %217 = llvm.add %209, %215 : i64
          %218 = llvm.mul %206, %28 : i64
          %219 = llvm.add %218, %217 : i64
          %220 = llvm.getelementptr %29[%219] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %221 = llvm.load %220 : !llvm.ptr<3> -> f16
          %222 = llvm.add %214, %215 : i64
          %223 = llvm.mul %212, %23 : i64
          %224 = llvm.add %223, %222 : i64
          %225 = llvm.getelementptr %arg2[%224] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %221, %225 : f16, !llvm.ptr<1>
          %226 = llvm.add %215, %24 : i64
          llvm.br ^bb34(%226 : i64)
        ^bb36:  // pred: ^bb34
          %227 = llvm.add %209, %23 : i64
          llvm.br ^bb32(%227 : i64)
        ^bb37:  // pred: ^bb32
          %228 = llvm.add %206, %22 : i64
          llvm.br ^bb30(%228 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c0_0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %c3 = arith.constant 3 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %c16 = arith.constant 16 : index
    %c1_1 = arith.constant 1 : index
    %exe = hal.executable.lookup device(%0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1_1]) bindings([
      (%c0 : index)[%c0_0, %c131072], 
      (%c1 : index)[%c0_0, %c131072], 
      (%c2 : index)[%c0_0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %0 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %1 = scf.if %0 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      %2 = util.null : !hal.command_buffer
      scf.yield %2 : !hal.command_buffer
    }
    util.return %1 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %__device_0_1 = util.global.load immutable @__device_0 : !hal.device
    %allocator_2 = hal.device.allocator<%__device_0_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    %__device_0_4 = util.global.load immutable @__device_0 : !hal.device
    %allocator_5 = hal.device.allocator<%__device_0_4 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %__device_0_6 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %c0_7 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0_6, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%__device_0_6 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0_6 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands(%0) bindings([
      (%buffer : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0_7, %c131072], 
      (%buffer_3 : !hal.buffer)[%c0_7, %c524288]
    ])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %dense_row_major_8 = hal.encoding_type<dense_row_major> : i32
    %element_type_f16_9 = hal.element_type<f16> : i32
    %c512_10 = arith.constant 512 : index
    %c512_11 = arith.constant 512 : index
    %c0_12 = arith.constant 0 : index
    %view = hal.buffer_view.create buffer(%buffer_3 : !hal.buffer)[%c0_12, %c524288] shape([%c512_10, %c512_11]) type(%element_type_f16_9) encoding(%dense_row_major_8) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
  %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
  %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
  %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
  %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %0 = util.null : !hal.command_buffer
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  %__device_0_1 = util.global.load immutable @__device_0 : !hal.device
  %allocator_2 = hal.device.allocator<%__device_0_1 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  %__device_0_4 = util.global.load immutable @__device_0 : !hal.device
  %allocator_5 = hal.device.allocator<%__device_0_4 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %__device_0_6 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0_6, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0_6 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0_6 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_3 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %dense_row_major_7 = hal.encoding_type<dense_row_major> : i32
  %element_type_f16_8 = hal.element_type<f16> : i32
  %view = hal.buffer_view.create buffer(%buffer_3 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16_8) encoding(%dense_row_major_7) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After LLVMGPULinkExecutablesPass (iree-llvmgpu-link-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
  hal.executable.variant public @cuda_nvptx_fb target(<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>) {
    hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device):
      %c16 = arith.constant 16 : index
      %c1 = arith.constant 1 : index
      hal.return %c16, %c16, %c1 : index, index, index
    }
    builtin.module {
      llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
      llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
      llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
      llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
      llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
        %0 = llvm.mlir.constant(40 : index) : i64
        %1 = llvm.mlir.constant(3 : index) : i64
        %2 = llvm.mlir.constant(-1 : index) : i64
        %3 = llvm.mlir.constant(8 : index) : i64
        %4 = llvm.mlir.constant(1 : index) : i64
        %5 = llvm.mlir.constant(512 : index) : i64
        %6 = llvm.mlir.constant(2 : index) : i64
        %7 = llvm.mlir.constant(32 : index) : i64
        %8 = llvm.mlir.constant(16 : index) : i64
        %9 = llvm.mlir.constant(128 : index) : i64
        %10 = llvm.mlir.constant(0 : index) : i64
        %11 = llvm.mlir.constant(24 : index) : i64
        %12 = llvm.mlir.constant(768 : index) : i64
        %13 = llvm.mlir.constant(640 : index) : i64
        %14 = llvm.mlir.constant(24 : index) : i32
        %15 = llvm.mlir.constant(40 : index) : i32
        %16 = llvm.mlir.constant(63 : index) : i64
        %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
        %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
        %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
        %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
        %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
        %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
        %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
        %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
        %25 = llvm.and %24, %16 : i64
        %26 = llvm.icmp "eq" %25, %10 : i64
        llvm.intr.assume %26 : i1
        %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
        %28 = llvm.and %27, %16 : i64
        %29 = llvm.icmp "eq" %28, %10 : i64
        llvm.intr.assume %29 : i1
        %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
        %31 = llvm.and %30, %16 : i64
        %32 = llvm.icmp "eq" %31, %10 : i64
        llvm.intr.assume %32 : i1
        %33 = nvvm.read.ptx.sreg.ctaid.x : i32
        %34 = llvm.sext %33 : i32 to i64
        %35 = nvvm.read.ptx.sreg.ctaid.y : i32
        %36 = llvm.sext %35 : i32 to i64
        nvvm.barrier0
        %37 = nvvm.read.ptx.sreg.tid.x : i32
        %38 = llvm.sext %37 : i32 to i64
        %39 = nvvm.read.ptx.sreg.tid.y : i32
        %40 = llvm.sext %39 : i32 to i64
        llvm.br ^bb1(%40 : i64)
      ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
        %42 = llvm.icmp "slt" %41, %7 : i64
        llvm.cond_br %42, ^bb2, ^bb9
      ^bb2:  // pred: ^bb1
        %43 = llvm.mul %38, %3 : i64
        llvm.br ^bb3(%43 : i64)
      ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
        %45 = llvm.icmp "slt" %44, %7 : i64
        llvm.cond_br %45, ^bb4, ^bb8
      ^bb4:  // pred: ^bb3
        %46 = llvm.mul %36, %7 : i64
        %47 = llvm.add %41, %46 : i64
        %48 = llvm.mul %34, %7 : i64
        %49 = llvm.add %44, %48 : i64
        llvm.br ^bb5(%10 : i64)
      ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
        %51 = llvm.icmp "slt" %50, %3 : i64
        llvm.cond_br %51, ^bb6, ^bb7
      ^bb6:  // pred: ^bb5
        %52 = llvm.add %49, %50 : i64
        %53 = llvm.mul %47, %5 : i64
        %54 = llvm.add %53, %52 : i64
        %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        %56 = llvm.load %55 : !llvm.ptr<1> -> f16
        %57 = llvm.add %44, %50 : i64
        %58 = llvm.mul %41, %0 : i64
        %59 = llvm.add %58, %57 : i64
        %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        llvm.store %56, %60 : f16, !llvm.ptr<3>
        %61 = llvm.add %50, %4 : i64
        llvm.br ^bb5(%61 : i64)
      ^bb7:  // pred: ^bb5
        %62 = llvm.add %44, %5 : i64
        llvm.br ^bb3(%62 : i64)
      ^bb8:  // pred: ^bb3
        %63 = llvm.add %41, %6 : i64
        llvm.br ^bb1(%63 : i64)
      ^bb9:  // pred: ^bb1
        nvvm.barrier0
        %64 = llvm.mul %40, %8 : i64
        %65 = llvm.icmp "slt" %38, %10 : i64
        %66 = llvm.sub %2, %38 : i64
        %67 = llvm.select %65, %66, %38 : i1, i64
        %68 = llvm.sdiv %67, %7 : i64
        %69 = llvm.sub %2, %68 : i64
        %70 = llvm.select %65, %69, %68 : i1, i64
        %71 = llvm.mul %70, %8 : i64
        %72 = llvm.mul %64, %0 : i64
        %73 = llvm.add %72, %71 : i64
        %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
      ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
        %78 = llvm.icmp "slt" %76, %9 : i64
        llvm.cond_br %78, ^bb11, ^bb29
      ^bb11:  // pred: ^bb10
        nvvm.barrier0
        llvm.br ^bb12(%40 : i64)
      ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
        %80 = llvm.icmp "slt" %79, %7 : i64
        llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
      ^bb13:  // pred: ^bb12
        %81 = llvm.mul %38, %3 : i64
        llvm.br ^bb14(%81 : i64)
      ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
        %83 = llvm.icmp "slt" %82, %8 : i64
        llvm.cond_br %83, ^bb15, ^bb19
      ^bb15:  // pred: ^bb14
        %84 = llvm.mul %36, %7 : i64
        %85 = llvm.add %79, %84 : i64
        %86 = llvm.add %76, %82 : i64
        %87 = llvm.icmp "slt" %76, %10 : i64
        %88 = llvm.sub %2, %76 : i64
        %89 = llvm.select %87, %88, %76 : i1, i64
        %90 = llvm.sdiv %89, %8 : i64
        %91 = llvm.sub %2, %90 : i64
        %92 = llvm.select %87, %91, %90 : i1, i64
        %93 = llvm.srem %92, %1 : i64
        %94 = llvm.icmp "slt" %93, %10 : i64
        %95 = llvm.add %93, %1 : i64
        %96 = llvm.select %94, %95, %93 : i1, i64
        llvm.br ^bb16(%10 : i64)
      ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
        %98 = llvm.icmp "slt" %97, %3 : i64
        llvm.cond_br %98, ^bb17, ^bb18
      ^bb17:  // pred: ^bb16
        %99 = llvm.add %86, %97 : i64
        %100 = llvm.mul %85, %9 : i64
        %101 = llvm.add %100, %99 : i64
        %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        %103 = llvm.load %102 : !llvm.ptr<1> -> f16
        %104 = llvm.add %82, %97 : i64
        %105 = llvm.mul %96, %12 : i64
        %106 = llvm.mul %79, %11 : i64
        %107 = llvm.add %105, %106 : i64
        %108 = llvm.add %107, %104 : i64
        %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        llvm.store %103, %109 : f16, !llvm.ptr<3>
        %110 = llvm.add %97, %4 : i64
        llvm.br ^bb16(%110 : i64)
      ^bb18:  // pred: ^bb16
        %111 = llvm.add %82, %5 : i64
        llvm.br ^bb14(%111 : i64)
      ^bb19:  // pred: ^bb14
        %112 = llvm.add %79, %6 : i64
        llvm.br ^bb12(%112 : i64)
      ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
        %114 = llvm.icmp "slt" %113, %8 : i64
        llvm.cond_br %114, ^bb21, ^bb28
      ^bb21:  // pred: ^bb20
        %115 = llvm.mul %38, %3 : i64
        llvm.br ^bb22(%115 : i64)
      ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
        %117 = llvm.icmp "slt" %116, %7 : i64
        llvm.cond_br %117, ^bb23, ^bb27
      ^bb23:  // pred: ^bb22
        %118 = llvm.add %76, %113 : i64
        %119 = llvm.mul %34, %7 : i64
        %120 = llvm.add %116, %119 : i64
        %121 = llvm.icmp "slt" %76, %10 : i64
        %122 = llvm.sub %2, %76 : i64
        %123 = llvm.select %121, %122, %76 : i1, i64
        %124 = llvm.sdiv %123, %8 : i64
        %125 = llvm.sub %2, %124 : i64
        %126 = llvm.select %121, %125, %124 : i1, i64
        %127 = llvm.srem %126, %1 : i64
        %128 = llvm.icmp "slt" %127, %10 : i64
        %129 = llvm.add %127, %1 : i64
        %130 = llvm.select %128, %129, %127 : i1, i64
        llvm.br ^bb24(%10 : i64)
      ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
        %132 = llvm.icmp "slt" %131, %3 : i64
        llvm.cond_br %132, ^bb25, ^bb26
      ^bb25:  // pred: ^bb24
        %133 = llvm.add %120, %131 : i64
        %134 = llvm.mul %118, %5 : i64
        %135 = llvm.add %134, %133 : i64
        %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        %137 = llvm.load %136 : !llvm.ptr<1> -> f16
        %138 = llvm.add %116, %131 : i64
        %139 = llvm.mul %130, %13 : i64
        %140 = llvm.mul %113, %0 : i64
        %141 = llvm.add %139, %140 : i64
        %142 = llvm.add %141, %138 : i64
        %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        llvm.store %137, %143 : f16, !llvm.ptr<3>
        %144 = llvm.add %131, %4 : i64
        llvm.br ^bb24(%144 : i64)
      ^bb26:  // pred: ^bb24
        %145 = llvm.add %116, %5 : i64
        llvm.br ^bb22(%145 : i64)
      ^bb27:  // pred: ^bb22
        %146 = llvm.add %113, %6 : i64
        llvm.br ^bb20(%146 : i64)
      ^bb28:  // pred: ^bb20
        nvvm.barrier0
        %147 = llvm.icmp "slt" %76, %10 : i64
        %148 = llvm.sub %2, %76 : i64
        %149 = llvm.select %147, %148, %76 : i1, i64
        %150 = llvm.sdiv %149, %8 : i64
        %151 = llvm.sub %2, %150 : i64
        %152 = llvm.select %147, %151, %150 : i1, i64
        %153 = llvm.srem %152, %1 : i64
        %154 = llvm.icmp "slt" %153, %10 : i64
        %155 = llvm.add %153, %1 : i64
        %156 = llvm.select %154, %155, %153 : i1, i64
        %157 = llvm.mul %156, %12 : i64
        %158 = llvm.mul %64, %11 : i64
        %159 = llvm.add %157, %158 : i64
        %160 = llvm.add %159, %10 : i64
        %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        %163 = llvm.mul %156, %13 : i64
        %164 = llvm.mul %10, %0 : i64
        %165 = llvm.add %163, %164 : i64
        %166 = llvm.add %165, %71 : i64
        %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
        %190 = llvm.add %76, %8 : i64
        llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
      ^bb29:  // pred: ^bb10
        %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
        %195 = llvm.mul %64, %0 : i64
        %196 = llvm.add %195, %71 : i64
        %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
        nvvm.barrier0
        llvm.br ^bb30(%40 : i64)
      ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
        %199 = llvm.icmp "slt" %198, %7 : i64
        llvm.cond_br %199, ^bb31, ^bb38
      ^bb31:  // pred: ^bb30
        %200 = llvm.mul %38, %3 : i64
        llvm.br ^bb32(%200 : i64)
      ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
        %202 = llvm.icmp "slt" %201, %7 : i64
        llvm.cond_br %202, ^bb33, ^bb37
      ^bb33:  // pred: ^bb32
        %203 = llvm.mul %36, %7 : i64
        %204 = llvm.add %198, %203 : i64
        %205 = llvm.mul %34, %7 : i64
        %206 = llvm.add %201, %205 : i64
        llvm.br ^bb34(%10 : i64)
      ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
        %208 = llvm.icmp "slt" %207, %3 : i64
        llvm.cond_br %208, ^bb35, ^bb36
      ^bb35:  // pred: ^bb34
        %209 = llvm.add %201, %207 : i64
        %210 = llvm.mul %198, %0 : i64
        %211 = llvm.add %210, %209 : i64
        %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
        %213 = llvm.load %212 : !llvm.ptr<3> -> f16
        %214 = llvm.add %206, %207 : i64
        %215 = llvm.mul %204, %5 : i64
        %216 = llvm.add %215, %214 : i64
        %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
        llvm.store %213, %217 : f16, !llvm.ptr<1>
        %218 = llvm.add %207, %4 : i64
        llvm.br ^bb34(%218 : i64)
      ^bb36:  // pred: ^bb34
        %219 = llvm.add %201, %5 : i64
        llvm.br ^bb32(%219 : i64)
      ^bb37:  // pred: ^bb32
        %220 = llvm.add %198, %6 : i64
        llvm.br ^bb30(%220 : i64)
      ^bb38:  // pred: ^bb30
        nvvm.barrier0
        llvm.return
      }
    }
  }
}

// -----// IR Dump After LLVMGPUAssignConstantOrdinalsPass (iree-llvmgpu-assign-constant-ordinals) //----- //
hal.executable.variant public @cuda_nvptx_fb target(<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>) {
  hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %c16 = arith.constant 16 : index
    %c1 = arith.constant 1 : index
    hal.return %c16, %c16, %c1 : index, index, index
  }
  builtin.module {
    llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
    llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
    llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
    llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
    llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
      %0 = llvm.mlir.constant(40 : index) : i64
      %1 = llvm.mlir.constant(3 : index) : i64
      %2 = llvm.mlir.constant(-1 : index) : i64
      %3 = llvm.mlir.constant(8 : index) : i64
      %4 = llvm.mlir.constant(1 : index) : i64
      %5 = llvm.mlir.constant(512 : index) : i64
      %6 = llvm.mlir.constant(2 : index) : i64
      %7 = llvm.mlir.constant(32 : index) : i64
      %8 = llvm.mlir.constant(16 : index) : i64
      %9 = llvm.mlir.constant(128 : index) : i64
      %10 = llvm.mlir.constant(0 : index) : i64
      %11 = llvm.mlir.constant(24 : index) : i64
      %12 = llvm.mlir.constant(768 : index) : i64
      %13 = llvm.mlir.constant(640 : index) : i64
      %14 = llvm.mlir.constant(24 : index) : i32
      %15 = llvm.mlir.constant(40 : index) : i32
      %16 = llvm.mlir.constant(63 : index) : i64
      %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
      %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
      %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
      %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
      %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
      %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
      %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
      %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
      %25 = llvm.and %24, %16 : i64
      %26 = llvm.icmp "eq" %25, %10 : i64
      llvm.intr.assume %26 : i1
      %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
      %28 = llvm.and %27, %16 : i64
      %29 = llvm.icmp "eq" %28, %10 : i64
      llvm.intr.assume %29 : i1
      %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
      %31 = llvm.and %30, %16 : i64
      %32 = llvm.icmp "eq" %31, %10 : i64
      llvm.intr.assume %32 : i1
      %33 = nvvm.read.ptx.sreg.ctaid.x : i32
      %34 = llvm.sext %33 : i32 to i64
      %35 = nvvm.read.ptx.sreg.ctaid.y : i32
      %36 = llvm.sext %35 : i32 to i64
      nvvm.barrier0
      %37 = nvvm.read.ptx.sreg.tid.x : i32
      %38 = llvm.sext %37 : i32 to i64
      %39 = nvvm.read.ptx.sreg.tid.y : i32
      %40 = llvm.sext %39 : i32 to i64
      llvm.br ^bb1(%40 : i64)
    ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
      %42 = llvm.icmp "slt" %41, %7 : i64
      llvm.cond_br %42, ^bb2, ^bb9
    ^bb2:  // pred: ^bb1
      %43 = llvm.mul %38, %3 : i64
      llvm.br ^bb3(%43 : i64)
    ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
      %45 = llvm.icmp "slt" %44, %7 : i64
      llvm.cond_br %45, ^bb4, ^bb8
    ^bb4:  // pred: ^bb3
      %46 = llvm.mul %36, %7 : i64
      %47 = llvm.add %41, %46 : i64
      %48 = llvm.mul %34, %7 : i64
      %49 = llvm.add %44, %48 : i64
      llvm.br ^bb5(%10 : i64)
    ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
      %51 = llvm.icmp "slt" %50, %3 : i64
      llvm.cond_br %51, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %52 = llvm.add %49, %50 : i64
      %53 = llvm.mul %47, %5 : i64
      %54 = llvm.add %53, %52 : i64
      %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      %56 = llvm.load %55 : !llvm.ptr<1> -> f16
      %57 = llvm.add %44, %50 : i64
      %58 = llvm.mul %41, %0 : i64
      %59 = llvm.add %58, %57 : i64
      %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      llvm.store %56, %60 : f16, !llvm.ptr<3>
      %61 = llvm.add %50, %4 : i64
      llvm.br ^bb5(%61 : i64)
    ^bb7:  // pred: ^bb5
      %62 = llvm.add %44, %5 : i64
      llvm.br ^bb3(%62 : i64)
    ^bb8:  // pred: ^bb3
      %63 = llvm.add %41, %6 : i64
      llvm.br ^bb1(%63 : i64)
    ^bb9:  // pred: ^bb1
      nvvm.barrier0
      %64 = llvm.mul %40, %8 : i64
      %65 = llvm.icmp "slt" %38, %10 : i64
      %66 = llvm.sub %2, %38 : i64
      %67 = llvm.select %65, %66, %38 : i1, i64
      %68 = llvm.sdiv %67, %7 : i64
      %69 = llvm.sub %2, %68 : i64
      %70 = llvm.select %65, %69, %68 : i1, i64
      %71 = llvm.mul %70, %8 : i64
      %72 = llvm.mul %64, %0 : i64
      %73 = llvm.add %72, %71 : i64
      %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
    ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
      %78 = llvm.icmp "slt" %76, %9 : i64
      llvm.cond_br %78, ^bb11, ^bb29
    ^bb11:  // pred: ^bb10
      nvvm.barrier0
      llvm.br ^bb12(%40 : i64)
    ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
      %80 = llvm.icmp "slt" %79, %7 : i64
      llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
    ^bb13:  // pred: ^bb12
      %81 = llvm.mul %38, %3 : i64
      llvm.br ^bb14(%81 : i64)
    ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
      %83 = llvm.icmp "slt" %82, %8 : i64
      llvm.cond_br %83, ^bb15, ^bb19
    ^bb15:  // pred: ^bb14
      %84 = llvm.mul %36, %7 : i64
      %85 = llvm.add %79, %84 : i64
      %86 = llvm.add %76, %82 : i64
      %87 = llvm.icmp "slt" %76, %10 : i64
      %88 = llvm.sub %2, %76 : i64
      %89 = llvm.select %87, %88, %76 : i1, i64
      %90 = llvm.sdiv %89, %8 : i64
      %91 = llvm.sub %2, %90 : i64
      %92 = llvm.select %87, %91, %90 : i1, i64
      %93 = llvm.srem %92, %1 : i64
      %94 = llvm.icmp "slt" %93, %10 : i64
      %95 = llvm.add %93, %1 : i64
      %96 = llvm.select %94, %95, %93 : i1, i64
      llvm.br ^bb16(%10 : i64)
    ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
      %98 = llvm.icmp "slt" %97, %3 : i64
      llvm.cond_br %98, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %99 = llvm.add %86, %97 : i64
      %100 = llvm.mul %85, %9 : i64
      %101 = llvm.add %100, %99 : i64
      %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      %103 = llvm.load %102 : !llvm.ptr<1> -> f16
      %104 = llvm.add %82, %97 : i64
      %105 = llvm.mul %96, %12 : i64
      %106 = llvm.mul %79, %11 : i64
      %107 = llvm.add %105, %106 : i64
      %108 = llvm.add %107, %104 : i64
      %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      llvm.store %103, %109 : f16, !llvm.ptr<3>
      %110 = llvm.add %97, %4 : i64
      llvm.br ^bb16(%110 : i64)
    ^bb18:  // pred: ^bb16
      %111 = llvm.add %82, %5 : i64
      llvm.br ^bb14(%111 : i64)
    ^bb19:  // pred: ^bb14
      %112 = llvm.add %79, %6 : i64
      llvm.br ^bb12(%112 : i64)
    ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
      %114 = llvm.icmp "slt" %113, %8 : i64
      llvm.cond_br %114, ^bb21, ^bb28
    ^bb21:  // pred: ^bb20
      %115 = llvm.mul %38, %3 : i64
      llvm.br ^bb22(%115 : i64)
    ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
      %117 = llvm.icmp "slt" %116, %7 : i64
      llvm.cond_br %117, ^bb23, ^bb27
    ^bb23:  // pred: ^bb22
      %118 = llvm.add %76, %113 : i64
      %119 = llvm.mul %34, %7 : i64
      %120 = llvm.add %116, %119 : i64
      %121 = llvm.icmp "slt" %76, %10 : i64
      %122 = llvm.sub %2, %76 : i64
      %123 = llvm.select %121, %122, %76 : i1, i64
      %124 = llvm.sdiv %123, %8 : i64
      %125 = llvm.sub %2, %124 : i64
      %126 = llvm.select %121, %125, %124 : i1, i64
      %127 = llvm.srem %126, %1 : i64
      %128 = llvm.icmp "slt" %127, %10 : i64
      %129 = llvm.add %127, %1 : i64
      %130 = llvm.select %128, %129, %127 : i1, i64
      llvm.br ^bb24(%10 : i64)
    ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
      %132 = llvm.icmp "slt" %131, %3 : i64
      llvm.cond_br %132, ^bb25, ^bb26
    ^bb25:  // pred: ^bb24
      %133 = llvm.add %120, %131 : i64
      %134 = llvm.mul %118, %5 : i64
      %135 = llvm.add %134, %133 : i64
      %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      %137 = llvm.load %136 : !llvm.ptr<1> -> f16
      %138 = llvm.add %116, %131 : i64
      %139 = llvm.mul %130, %13 : i64
      %140 = llvm.mul %113, %0 : i64
      %141 = llvm.add %139, %140 : i64
      %142 = llvm.add %141, %138 : i64
      %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      llvm.store %137, %143 : f16, !llvm.ptr<3>
      %144 = llvm.add %131, %4 : i64
      llvm.br ^bb24(%144 : i64)
    ^bb26:  // pred: ^bb24
      %145 = llvm.add %116, %5 : i64
      llvm.br ^bb22(%145 : i64)
    ^bb27:  // pred: ^bb22
      %146 = llvm.add %113, %6 : i64
      llvm.br ^bb20(%146 : i64)
    ^bb28:  // pred: ^bb20
      nvvm.barrier0
      %147 = llvm.icmp "slt" %76, %10 : i64
      %148 = llvm.sub %2, %76 : i64
      %149 = llvm.select %147, %148, %76 : i1, i64
      %150 = llvm.sdiv %149, %8 : i64
      %151 = llvm.sub %2, %150 : i64
      %152 = llvm.select %147, %151, %150 : i1, i64
      %153 = llvm.srem %152, %1 : i64
      %154 = llvm.icmp "slt" %153, %10 : i64
      %155 = llvm.add %153, %1 : i64
      %156 = llvm.select %154, %155, %153 : i1, i64
      %157 = llvm.mul %156, %12 : i64
      %158 = llvm.mul %64, %11 : i64
      %159 = llvm.add %157, %158 : i64
      %160 = llvm.add %159, %10 : i64
      %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      %163 = llvm.mul %156, %13 : i64
      %164 = llvm.mul %10, %0 : i64
      %165 = llvm.add %163, %164 : i64
      %166 = llvm.add %165, %71 : i64
      %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
      %190 = llvm.add %76, %8 : i64
      llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
    ^bb29:  // pred: ^bb10
      %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
      %195 = llvm.mul %64, %0 : i64
      %196 = llvm.add %195, %71 : i64
      %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
      nvvm.barrier0
      llvm.br ^bb30(%40 : i64)
    ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
      %199 = llvm.icmp "slt" %198, %7 : i64
      llvm.cond_br %199, ^bb31, ^bb38
    ^bb31:  // pred: ^bb30
      %200 = llvm.mul %38, %3 : i64
      llvm.br ^bb32(%200 : i64)
    ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
      %202 = llvm.icmp "slt" %201, %7 : i64
      llvm.cond_br %202, ^bb33, ^bb37
    ^bb33:  // pred: ^bb32
      %203 = llvm.mul %36, %7 : i64
      %204 = llvm.add %198, %203 : i64
      %205 = llvm.mul %34, %7 : i64
      %206 = llvm.add %201, %205 : i64
      llvm.br ^bb34(%10 : i64)
    ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
      %208 = llvm.icmp "slt" %207, %3 : i64
      llvm.cond_br %208, ^bb35, ^bb36
    ^bb35:  // pred: ^bb34
      %209 = llvm.add %201, %207 : i64
      %210 = llvm.mul %198, %0 : i64
      %211 = llvm.add %210, %209 : i64
      %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
      %213 = llvm.load %212 : !llvm.ptr<3> -> f16
      %214 = llvm.add %206, %207 : i64
      %215 = llvm.mul %204, %5 : i64
      %216 = llvm.add %215, %214 : i64
      %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
      llvm.store %213, %217 : f16, !llvm.ptr<1>
      %218 = llvm.add %207, %4 : i64
      llvm.br ^bb34(%218 : i64)
    ^bb36:  // pred: ^bb34
      %219 = llvm.add %201, %5 : i64
      llvm.br ^bb32(%219 : i64)
    ^bb37:  // pred: ^bb32
      %220 = llvm.add %198, %6 : i64
      llvm.br ^bb30(%220 : i64)
    ^bb38:  // pred: ^bb30
      nvvm.barrier0
      llvm.return
    }
  }
}

// -----// IR Dump After LinkTargetExecutablesPass (iree-hal-link-target-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After LinkExecutablesPass (iree-hal-link-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb::@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveExportOrdinalsPass (iree-hal-resolve-export-ordinals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %exe = hal.executable.lookup device(%arg0 : !hal.device) executable(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0) : !hal.executable
    %c0_0 = arith.constant 0 : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%c0_0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeResourceCachesPass (iree-hal-materialize-resource-caches) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
      scf.yield %executable : !hal.executable
    }
    default {
      %c14_i32 = arith.constant 14 : i32
      util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %c0_0 = arith.constant 0 : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0_0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After MemoizeDeviceQueriesPass (iree-hal-memoize-device-queries) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok : i1
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %ok, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok : i1
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok : i1
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %0 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
      scf.yield %executable : !hal.executable
    }
    default {
      %c14_i32 = arith.constant 14 : i32
      util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %c0_0 = arith.constant 0 : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0_0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %ok, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok : i1
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %ok, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok : i1
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.global.store %ok, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok : i1
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.global.store %ok, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb_ok : i1
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0 = util.global.load @__device_0 : !hal.device
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = scf.index_switch %1 -> !hal.executable 
  case 0 {
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    scf.yield %executable : !hal.executable
  }
  default {
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    scf.yield %0 : !hal.executable
  }
  util.global.store %2, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0 = util.global.load @__device_0 : !hal.device
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = scf.index_switch %1 -> !hal.executable 
  case 0 {
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    scf.yield %executable : !hal.executable
  }
  default {
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    scf.yield %0 : !hal.executable
  }
  util.global.store %2, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = scf.index_switch %1 -> !hal.executable 
  case 0 {
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    scf.yield %executable : !hal.executable
  }
  default {
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    scf.yield %0 : !hal.executable
  }
  util.global.store %2, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  %3 = scf.if %2 -> (!hal.executable) {
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    scf.yield %executable : !hal.executable
  } else {
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    scf.yield %0 : !hal.executable
  }
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.null : !hal.command_buffer
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    %3 = scf.if %2 -> (!hal.executable) {
      %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
      scf.yield %executable : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      scf.yield %0 : !hal.executable
    }
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_cuda = #hal.device.target<"cuda", [#executable_target_cuda_nvptx_fb]> : !hal.device
module {
  util.global private @__device_0 = #device_target_cuda
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    %3 = scf.if %2 -> (!hal.executable) {
      %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
      scf.yield %executable : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      scf.yield %0 : !hal.executable
    }
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  %3 = scf.if %2 -> (!hal.executable) {
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    scf.yield %executable : !hal.executable
  } else {
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    scf.yield %0 : !hal.executable
  }
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = scf.if %1 -> (!hal.command_buffer) {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  } else {
    scf.yield %0 : !hal.command_buffer
  }
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After InitializeDevicesPass (iree-hal-initialize-devices) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    %1:3 = scf.while (%arg0 = %c0, %arg1 = %c0, %arg2 = %0) : (index, index, !hal.device) -> (index, index, !hal.device) {
      %4 = util.cmp.eq %arg2, %0 : !hal.device
      %5 = arith.cmpi slt, %arg0, %device_count : index
      %6 = arith.andi %4, %5 : i1
      scf.condition(%6) %arg0, %arg1, %arg2 : index, index, !hal.device
    } do {
    ^bb0(%arg0: index, %arg1: index, %arg2: !hal.device):
      %device_n = hal.devices.get %arg0 : !hal.device
      %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
      %4 = scf.if %value -> (i1) {
        %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
        scf.yield %value_1 : i1
      } else {
        %false = arith.constant false
        scf.yield %false : i1
      }
      %5 = arith.cmpi eq, %arg1, %c0 : index
      %6 = arith.select %4, %c1, %c0 : index
      %7 = arith.addi %arg1, %6 : index
      %8 = arith.andi %4, %5 : i1
      %9 = arith.select %8, %device_n, %0 : !hal.device
      %10 = arith.addi %arg0, %c1 : index
      scf.yield %10, %7, %9 : index, index, !hal.device
    }
    %2 = util.null : !hal.device
    %3 = util.cmp.eq %1#2, %2 : !hal.device
    scf.if %3 {
      %c18_i32 = arith.constant 18 : i32
      util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    }
    util.global.store %1#2, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    %3 = scf.if %2 -> (!hal.executable) {
      %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
      scf.yield %executable : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      scf.yield %0 : !hal.executable
    }
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    %1:3 = scf.while (%arg0 = %c0, %arg1 = %c0, %arg2 = %0) : (index, index, !hal.device) -> (index, index, !hal.device) {
      %3 = util.cmp.eq %arg2, %0 : !hal.device
      %4 = arith.cmpi slt, %arg0, %device_count : index
      %5 = arith.andi %3, %4 : i1
      scf.condition(%5) %arg0, %arg1, %arg2 : index, index, !hal.device
    } do {
    ^bb0(%arg0: index, %arg1: index, %arg2: !hal.device):
      %device_n = hal.devices.get %arg0 : !hal.device
      %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
      %3 = scf.if %value -> (i1) {
        %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
        scf.yield %value_1 : i1
      } else {
        scf.yield %false : i1
      }
      %4 = arith.cmpi eq, %arg1, %c0 : index
      %5 = arith.select %3, %c1, %c0 : index
      %6 = arith.addi %arg1, %5 : index
      %7 = arith.andi %3, %4 : i1
      %8 = arith.select %7, %device_n, %0 : !hal.device
      %9 = arith.addi %arg0, %c1 : index
      scf.yield %9, %6, %8 : index, index, !hal.device
    }
    %2 = util.cmp.eq %1#2, %0 : !hal.device
    scf.if %2 {
      util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    }
    util.global.store %1#2, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    %3 = scf.if %2 -> (!hal.executable) {
      %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
      scf.yield %executable : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      scf.yield %0 : !hal.executable
    }
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    %1:3 = scf.while (%arg0 = %c0, %arg1 = %c0, %arg2 = %0) : (index, index, !hal.device) -> (index, index, !hal.device) {
      %3 = util.cmp.eq %arg2, %0 : !hal.device
      %4 = arith.cmpi slt, %arg0, %device_count : index
      %5 = arith.andi %3, %4 : i1
      scf.condition(%5) %arg0, %arg1, %arg2 : index, index, !hal.device
    } do {
    ^bb0(%arg0: index, %arg1: index, %arg2: !hal.device):
      %device_n = hal.devices.get %arg0 : !hal.device
      %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
      %3 = scf.if %value -> (i1) {
        %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
        scf.yield %value_1 : i1
      } else {
        scf.yield %false : i1
      }
      %4 = arith.cmpi eq, %arg1, %c0 : index
      %5 = arith.select %3, %c1, %c0 : index
      %6 = arith.addi %arg1, %5 : index
      %7 = arith.andi %3, %4 : i1
      %8 = arith.select %7, %device_n, %0 : !hal.device
      %9 = arith.addi %arg0, %c1 : index
      scf.yield %9, %6, %8 : index, index, !hal.device
    }
    %2 = util.cmp.eq %1#2, %0 : !hal.device
    scf.if %2 {
      util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    }
    util.global.store %1#2, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    %3 = scf.if %2 -> (!hal.executable) {
      %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
      scf.yield %executable : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      scf.yield %0 : !hal.executable
    }
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb target(#executable_target_cuda_nvptx_fb) {
      hal.executable.export public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16 ordinal(0) layout(#pipeline_layout) attributes {workgroup_local_memory = 11008 : index, workgroup_size = [64 : index, 2 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c16, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.mlir.global external @__dynamic_shared_memory__() {addr_space = 3 : i32, alignment = 16 : i64} : !llvm.array<0 x i8>
        llvm.mlir.global private @__shared_memory___1() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<32 x array<40 x f16>>
        llvm.mlir.global private @__shared_memory___0() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<32 x array<24 x f16>>>
        llvm.mlir.global private @__shared_memory__() {addr_space = 3 : i32, alignment = 2 : i64} : !llvm.array<3 x array<16 x array<40 x f16>>>
        llvm.func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(40 : index) : i64
          %1 = llvm.mlir.constant(3 : index) : i64
          %2 = llvm.mlir.constant(-1 : index) : i64
          %3 = llvm.mlir.constant(8 : index) : i64
          %4 = llvm.mlir.constant(1 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(2 : index) : i64
          %7 = llvm.mlir.constant(32 : index) : i64
          %8 = llvm.mlir.constant(16 : index) : i64
          %9 = llvm.mlir.constant(128 : index) : i64
          %10 = llvm.mlir.constant(0 : index) : i64
          %11 = llvm.mlir.constant(24 : index) : i64
          %12 = llvm.mlir.constant(768 : index) : i64
          %13 = llvm.mlir.constant(640 : index) : i64
          %14 = llvm.mlir.constant(24 : index) : i32
          %15 = llvm.mlir.constant(40 : index) : i32
          %16 = llvm.mlir.constant(63 : index) : i64
          %17 = llvm.mlir.addressof @__dynamic_shared_memory__ : !llvm.ptr<3>
          %18 = llvm.getelementptr %17[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %19 = llvm.getelementptr %17[0, 3840] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %20 = llvm.getelementptr %17[0, 8448] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<0 x i8>
          %21 = llvm.getelementptr %20[0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<32 x array<40 x f16>>
          %22 = llvm.getelementptr %19[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<32 x array<24 x f16>>>
          %23 = llvm.getelementptr %18[0, 0, 0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<3 x array<16 x array<40 x f16>>>
          %24 = llvm.ptrtoint %arg0 : !llvm.ptr<1> to i64
          %25 = llvm.and %24, %16 : i64
          %26 = llvm.icmp "eq" %25, %10 : i64
          llvm.intr.assume %26 : i1
          %27 = llvm.ptrtoint %arg1 : !llvm.ptr<1> to i64
          %28 = llvm.and %27, %16 : i64
          %29 = llvm.icmp "eq" %28, %10 : i64
          llvm.intr.assume %29 : i1
          %30 = llvm.ptrtoint %arg2 : !llvm.ptr<1> to i64
          %31 = llvm.and %30, %16 : i64
          %32 = llvm.icmp "eq" %31, %10 : i64
          llvm.intr.assume %32 : i1
          %33 = nvvm.read.ptx.sreg.ctaid.x : i32
          %34 = llvm.sext %33 : i32 to i64
          %35 = nvvm.read.ptx.sreg.ctaid.y : i32
          %36 = llvm.sext %35 : i32 to i64
          nvvm.barrier0
          %37 = nvvm.read.ptx.sreg.tid.x : i32
          %38 = llvm.sext %37 : i32 to i64
          %39 = nvvm.read.ptx.sreg.tid.y : i32
          %40 = llvm.sext %39 : i32 to i64
          llvm.br ^bb1(%40 : i64)
        ^bb1(%41: i64):  // 2 preds: ^bb0, ^bb8
          %42 = llvm.icmp "slt" %41, %7 : i64
          llvm.cond_br %42, ^bb2, ^bb9
        ^bb2:  // pred: ^bb1
          %43 = llvm.mul %38, %3 : i64
          llvm.br ^bb3(%43 : i64)
        ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb7
          %45 = llvm.icmp "slt" %44, %7 : i64
          llvm.cond_br %45, ^bb4, ^bb8
        ^bb4:  // pred: ^bb3
          %46 = llvm.mul %36, %7 : i64
          %47 = llvm.add %41, %46 : i64
          %48 = llvm.mul %34, %7 : i64
          %49 = llvm.add %44, %48 : i64
          llvm.br ^bb5(%10 : i64)
        ^bb5(%50: i64):  // 2 preds: ^bb4, ^bb6
          %51 = llvm.icmp "slt" %50, %3 : i64
          llvm.cond_br %51, ^bb6, ^bb7
        ^bb6:  // pred: ^bb5
          %52 = llvm.add %49, %50 : i64
          %53 = llvm.mul %47, %5 : i64
          %54 = llvm.add %53, %52 : i64
          %55 = llvm.getelementptr %arg2[%54] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %56 = llvm.load %55 : !llvm.ptr<1> -> f16
          %57 = llvm.add %44, %50 : i64
          %58 = llvm.mul %41, %0 : i64
          %59 = llvm.add %58, %57 : i64
          %60 = llvm.getelementptr %21[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %56, %60 : f16, !llvm.ptr<3>
          %61 = llvm.add %50, %4 : i64
          llvm.br ^bb5(%61 : i64)
        ^bb7:  // pred: ^bb5
          %62 = llvm.add %44, %5 : i64
          llvm.br ^bb3(%62 : i64)
        ^bb8:  // pred: ^bb3
          %63 = llvm.add %41, %6 : i64
          llvm.br ^bb1(%63 : i64)
        ^bb9:  // pred: ^bb1
          nvvm.barrier0
          %64 = llvm.mul %40, %8 : i64
          %65 = llvm.icmp "slt" %38, %10 : i64
          %66 = llvm.sub %2, %38 : i64
          %67 = llvm.select %65, %66, %38 : i1, i64
          %68 = llvm.sdiv %67, %7 : i64
          %69 = llvm.sub %2, %68 : i64
          %70 = llvm.select %65, %69, %68 : i1, i64
          %71 = llvm.mul %70, %8 : i64
          %72 = llvm.mul %64, %0 : i64
          %73 = llvm.add %72, %71 : i64
          %74 = llvm.getelementptr %21[%73] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %75 = nvvm.wmma.load %74, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<c>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          llvm.br ^bb10(%10, %75 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb10(%76: i64, %77: !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>):  // 2 preds: ^bb9, ^bb28
          %78 = llvm.icmp "slt" %76, %9 : i64
          llvm.cond_br %78, ^bb11, ^bb29
        ^bb11:  // pred: ^bb10
          nvvm.barrier0
          llvm.br ^bb12(%40 : i64)
        ^bb12(%79: i64):  // 2 preds: ^bb11, ^bb19
          %80 = llvm.icmp "slt" %79, %7 : i64
          llvm.cond_br %80, ^bb13, ^bb20(%40 : i64)
        ^bb13:  // pred: ^bb12
          %81 = llvm.mul %38, %3 : i64
          llvm.br ^bb14(%81 : i64)
        ^bb14(%82: i64):  // 2 preds: ^bb13, ^bb18
          %83 = llvm.icmp "slt" %82, %8 : i64
          llvm.cond_br %83, ^bb15, ^bb19
        ^bb15:  // pred: ^bb14
          %84 = llvm.mul %36, %7 : i64
          %85 = llvm.add %79, %84 : i64
          %86 = llvm.add %76, %82 : i64
          %87 = llvm.icmp "slt" %76, %10 : i64
          %88 = llvm.sub %2, %76 : i64
          %89 = llvm.select %87, %88, %76 : i1, i64
          %90 = llvm.sdiv %89, %8 : i64
          %91 = llvm.sub %2, %90 : i64
          %92 = llvm.select %87, %91, %90 : i1, i64
          %93 = llvm.srem %92, %1 : i64
          %94 = llvm.icmp "slt" %93, %10 : i64
          %95 = llvm.add %93, %1 : i64
          %96 = llvm.select %94, %95, %93 : i1, i64
          llvm.br ^bb16(%10 : i64)
        ^bb16(%97: i64):  // 2 preds: ^bb15, ^bb17
          %98 = llvm.icmp "slt" %97, %3 : i64
          llvm.cond_br %98, ^bb17, ^bb18
        ^bb17:  // pred: ^bb16
          %99 = llvm.add %86, %97 : i64
          %100 = llvm.mul %85, %9 : i64
          %101 = llvm.add %100, %99 : i64
          %102 = llvm.getelementptr %arg0[%101] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %103 = llvm.load %102 : !llvm.ptr<1> -> f16
          %104 = llvm.add %82, %97 : i64
          %105 = llvm.mul %96, %12 : i64
          %106 = llvm.mul %79, %11 : i64
          %107 = llvm.add %105, %106 : i64
          %108 = llvm.add %107, %104 : i64
          %109 = llvm.getelementptr %22[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %103, %109 : f16, !llvm.ptr<3>
          %110 = llvm.add %97, %4 : i64
          llvm.br ^bb16(%110 : i64)
        ^bb18:  // pred: ^bb16
          %111 = llvm.add %82, %5 : i64
          llvm.br ^bb14(%111 : i64)
        ^bb19:  // pred: ^bb14
          %112 = llvm.add %79, %6 : i64
          llvm.br ^bb12(%112 : i64)
        ^bb20(%113: i64):  // 2 preds: ^bb12, ^bb27
          %114 = llvm.icmp "slt" %113, %8 : i64
          llvm.cond_br %114, ^bb21, ^bb28
        ^bb21:  // pred: ^bb20
          %115 = llvm.mul %38, %3 : i64
          llvm.br ^bb22(%115 : i64)
        ^bb22(%116: i64):  // 2 preds: ^bb21, ^bb26
          %117 = llvm.icmp "slt" %116, %7 : i64
          llvm.cond_br %117, ^bb23, ^bb27
        ^bb23:  // pred: ^bb22
          %118 = llvm.add %76, %113 : i64
          %119 = llvm.mul %34, %7 : i64
          %120 = llvm.add %116, %119 : i64
          %121 = llvm.icmp "slt" %76, %10 : i64
          %122 = llvm.sub %2, %76 : i64
          %123 = llvm.select %121, %122, %76 : i1, i64
          %124 = llvm.sdiv %123, %8 : i64
          %125 = llvm.sub %2, %124 : i64
          %126 = llvm.select %121, %125, %124 : i1, i64
          %127 = llvm.srem %126, %1 : i64
          %128 = llvm.icmp "slt" %127, %10 : i64
          %129 = llvm.add %127, %1 : i64
          %130 = llvm.select %128, %129, %127 : i1, i64
          llvm.br ^bb24(%10 : i64)
        ^bb24(%131: i64):  // 2 preds: ^bb23, ^bb25
          %132 = llvm.icmp "slt" %131, %3 : i64
          llvm.cond_br %132, ^bb25, ^bb26
        ^bb25:  // pred: ^bb24
          %133 = llvm.add %120, %131 : i64
          %134 = llvm.mul %118, %5 : i64
          %135 = llvm.add %134, %133 : i64
          %136 = llvm.getelementptr %arg1[%135] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          %137 = llvm.load %136 : !llvm.ptr<1> -> f16
          %138 = llvm.add %116, %131 : i64
          %139 = llvm.mul %130, %13 : i64
          %140 = llvm.mul %113, %0 : i64
          %141 = llvm.add %139, %140 : i64
          %142 = llvm.add %141, %138 : i64
          %143 = llvm.getelementptr %23[%142] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          llvm.store %137, %143 : f16, !llvm.ptr<3>
          %144 = llvm.add %131, %4 : i64
          llvm.br ^bb24(%144 : i64)
        ^bb26:  // pred: ^bb24
          %145 = llvm.add %116, %5 : i64
          llvm.br ^bb22(%145 : i64)
        ^bb27:  // pred: ^bb22
          %146 = llvm.add %113, %6 : i64
          llvm.br ^bb20(%146 : i64)
        ^bb28:  // pred: ^bb20
          nvvm.barrier0
          %147 = llvm.icmp "slt" %76, %10 : i64
          %148 = llvm.sub %2, %76 : i64
          %149 = llvm.select %147, %148, %76 : i1, i64
          %150 = llvm.sdiv %149, %8 : i64
          %151 = llvm.sub %2, %150 : i64
          %152 = llvm.select %147, %151, %150 : i1, i64
          %153 = llvm.srem %152, %1 : i64
          %154 = llvm.icmp "slt" %153, %10 : i64
          %155 = llvm.add %153, %1 : i64
          %156 = llvm.select %154, %155, %153 : i1, i64
          %157 = llvm.mul %156, %12 : i64
          %158 = llvm.mul %64, %11 : i64
          %159 = llvm.add %157, %158 : i64
          %160 = llvm.add %159, %10 : i64
          %161 = llvm.getelementptr %22[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %162 = nvvm.wmma.load %161, %14 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<a>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %163 = llvm.mul %156, %13 : i64
          %164 = llvm.mul %10, %0 : i64
          %165 = llvm.add %163, %164 : i64
          %166 = llvm.add %165, %71 : i64
          %167 = llvm.getelementptr %23[%166] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %168 = nvvm.wmma.load %167, %15 {eltype = #nvvm.mma_type<f16>, frag = #nvvm.mma_frag<b>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (!llvm.ptr<3>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %169 = llvm.extractvalue %162[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %170 = llvm.extractvalue %162[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %171 = llvm.extractvalue %162[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %172 = llvm.extractvalue %162[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %173 = llvm.extractvalue %162[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %174 = llvm.extractvalue %162[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %175 = llvm.extractvalue %162[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %176 = llvm.extractvalue %162[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %177 = llvm.extractvalue %168[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %178 = llvm.extractvalue %168[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %179 = llvm.extractvalue %168[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %180 = llvm.extractvalue %168[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %181 = llvm.extractvalue %168[4] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %182 = llvm.extractvalue %168[5] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %183 = llvm.extractvalue %168[6] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %184 = llvm.extractvalue %168[7] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %185 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %186 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %187 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %188 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %189 = nvvm.wmma.mma %169, %170, %171, %172, %173, %174, %175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188 {eltypeA = #nvvm.mma_type<f16>, eltypeB = #nvvm.mma_type<f16>, k = 16 : i32, layoutA = #nvvm.mma_layout<row>, layoutB = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : (vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>) -> !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>
          %190 = llvm.add %76, %8 : i64
          llvm.br ^bb10(%190, %189 : i64, !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)>)
        ^bb29:  // pred: ^bb10
          %191 = llvm.extractvalue %77[0] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %192 = llvm.extractvalue %77[1] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %193 = llvm.extractvalue %77[2] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %194 = llvm.extractvalue %77[3] : !llvm.struct<(vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>)> 
          %195 = llvm.mul %64, %0 : i64
          %196 = llvm.add %195, %71 : i64
          %197 = llvm.getelementptr %21[%196] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          nvvm.wmma.store %197, %15, %191, %192, %193, %194 {eltype = #nvvm.mma_type<f16>, k = 16 : i32, layout = #nvvm.mma_layout<row>, m = 16 : i32, n = 16 : i32} : !llvm.ptr<3>, vector<2xf16>, vector<2xf16>, vector<2xf16>, vector<2xf16>
          nvvm.barrier0
          llvm.br ^bb30(%40 : i64)
        ^bb30(%198: i64):  // 2 preds: ^bb29, ^bb37
          %199 = llvm.icmp "slt" %198, %7 : i64
          llvm.cond_br %199, ^bb31, ^bb38
        ^bb31:  // pred: ^bb30
          %200 = llvm.mul %38, %3 : i64
          llvm.br ^bb32(%200 : i64)
        ^bb32(%201: i64):  // 2 preds: ^bb31, ^bb36
          %202 = llvm.icmp "slt" %201, %7 : i64
          llvm.cond_br %202, ^bb33, ^bb37
        ^bb33:  // pred: ^bb32
          %203 = llvm.mul %36, %7 : i64
          %204 = llvm.add %198, %203 : i64
          %205 = llvm.mul %34, %7 : i64
          %206 = llvm.add %201, %205 : i64
          llvm.br ^bb34(%10 : i64)
        ^bb34(%207: i64):  // 2 preds: ^bb33, ^bb35
          %208 = llvm.icmp "slt" %207, %3 : i64
          llvm.cond_br %208, ^bb35, ^bb36
        ^bb35:  // pred: ^bb34
          %209 = llvm.add %201, %207 : i64
          %210 = llvm.mul %198, %0 : i64
          %211 = llvm.add %210, %209 : i64
          %212 = llvm.getelementptr %21[%211] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, f16
          %213 = llvm.load %212 : !llvm.ptr<3> -> f16
          %214 = llvm.add %206, %207 : i64
          %215 = llvm.mul %204, %5 : i64
          %216 = llvm.add %215, %214 : i64
          %217 = llvm.getelementptr %arg2[%216] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, f16
          llvm.store %213, %217 : f16, !llvm.ptr<1>
          %218 = llvm.add %207, %4 : i64
          llvm.br ^bb34(%218 : i64)
        ^bb36:  // pred: ^bb34
          %219 = llvm.add %201, %5 : i64
          llvm.br ^bb32(%219 : i64)
        ^bb37:  // pred: ^bb32
          %220 = llvm.add %198, %6 : i64
          llvm.br ^bb30(%220 : i64)
        ^bb38:  // pred: ^bb30
          nvvm.barrier0
          llvm.return
        }
      }
    }
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = scf.if %1 -> (!hal.command_buffer) {
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
      scf.yield %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    } else {
      scf.yield %0 : !hal.command_buffer
    }
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb6
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2(%1, %2, %3 : index, index, !hal.device), ^bb7
^bb2(%7: index, %8: index, %9: !hal.device):  // pred: ^bb1
  %device_n = hal.devices.get %7 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb5(%value_1 : i1)
^bb4:  // pred: ^bb2
  cf.br ^bb5(%false : i1)
^bb5(%10: i1):  // 2 preds: ^bb3, ^bb4
  cf.br ^bb6
^bb6:  // pred: ^bb5
  %11 = arith.cmpi eq, %8, %c0 : index
  %12 = arith.select %10, %c1, %c0 : index
  %13 = arith.addi %8, %12 : index
  %14 = arith.andi %10, %11 : i1
  %15 = arith.select %14, %device_n, %0 : !hal.device
  %16 = arith.addi %7, %c1 : index
  cf.br ^bb1(%16, %13, %15 : index, index, !hal.device)
^bb7:  // pred: ^bb1
  %17 = util.cmp.eq %3, %0 : !hal.device
  cf.cond_br %17, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb9
^bb9:  // 2 preds: ^bb7, ^bb8
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  cf.br ^bb4
^bb4:  // pred: ^bb3
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SerializeTargetExecutablesPass (iree-hal-serialize-target-executables) //----- //
hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
  hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
}

// -----// IR Dump After SerializeExecutablesPass (iree-hal-serialize-executables) //----- //
hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
  hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  cf.br ^bb3(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer)
^bb2:  // pred: ^bb0
  cf.br ^bb3(%0 : !hal.command_buffer)
^bb3(%2: !hal.command_buffer):  // 2 preds: ^bb1, ^bb2
  cf.br ^bb4
^bb4:  // pred: ^bb3
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb6
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2(%1, %2, %3 : index, index, !hal.device), ^bb7
  ^bb2(%7: index, %8: index, %9: !hal.device):  // pred: ^bb1
    %device_n = hal.devices.get %7 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb5(%value_1 : i1)
  ^bb4:  // pred: ^bb2
    cf.br ^bb5(%false : i1)
  ^bb5(%10: i1):  // 2 preds: ^bb3, ^bb4
    cf.br ^bb6
  ^bb6:  // pred: ^bb5
    %11 = arith.cmpi eq, %8, %c0 : index
    %12 = arith.select %10, %c1, %c0 : index
    %13 = arith.addi %8, %12 : index
    %14 = arith.andi %10, %11 : i1
    %15 = arith.select %14, %device_n, %0 : !hal.device
    %16 = arith.addi %7, %c1 : index
    cf.br ^bb1(%16, %13, %15 : index, index, !hal.device)
  ^bb7:  // pred: ^bb1
    %17 = util.cmp.eq %3, %0 : !hal.device
    cf.cond_br %17, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    cf.br ^bb3(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer)
  ^bb2:  // pred: ^bb0
    cf.br ^bb3(%0 : !hal.command_buffer)
  ^bb3(%2: !hal.command_buffer):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb6
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2(%1, %2, %3 : index, index, !hal.device), ^bb7
  ^bb2(%7: index, %8: index, %9: !hal.device):  // pred: ^bb1
    %device_n = hal.devices.get %7 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb5(%value_1 : i1)
  ^bb4:  // pred: ^bb2
    cf.br ^bb5(%false : i1)
  ^bb5(%10: i1):  // 2 preds: ^bb3, ^bb4
    cf.br ^bb6
  ^bb6:  // pred: ^bb5
    %11 = arith.cmpi eq, %8, %c0 : index
    %12 = arith.select %10, %c1, %c0 : index
    %13 = arith.addi %8, %12 : index
    %14 = arith.andi %10, %11 : i1
    %15 = arith.select %14, %device_n, %0 : !hal.device
    %16 = arith.addi %7, %c1 : index
    cf.br ^bb1(%16, %13, %15 : index, index, !hal.device)
  ^bb7:  // pred: ^bb1
    %17 = util.cmp.eq %3, %0 : !hal.device
    cf.cond_br %17, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    cf.br ^bb3(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer)
  ^bb2:  // pred: ^bb0
    cf.br ^bb3(%0 : !hal.command_buffer)
  ^bb3(%2: !hal.command_buffer):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2(%1, %2 : index, index), ^bb5
^bb2(%7: index, %8: index):  // pred: ^bb1
  %device_n = hal.devices.get %7 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%9: i1):  // 2 preds: ^bb2, ^bb3
  %10 = arith.cmpi eq, %8, %c0 : index
  %11 = arith.select %9, %c1, %c0 : index
  %12 = arith.addi %8, %11 : index
  %13 = arith.andi %9, %10 : i1
  %14 = arith.select %13, %device_n, %0 : !hal.device
  %15 = arith.addi %7, %c1 : index
  cf.br ^bb1(%15, %12, %14 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  %16 = util.cmp.eq %3, %0 : !hal.device
  cf.cond_br %16, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2(%1, %2 : index, index), ^bb5
^bb2(%7: index, %8: index):  // pred: ^bb1
  %device_n = hal.devices.get %7 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%9: i1):  // 2 preds: ^bb2, ^bb3
  %10 = arith.cmpi eq, %8, %c0 : index
  %11 = arith.select %9, %c1, %c0 : index
  %12 = arith.addi %8, %11 : index
  %13 = arith.andi %9, %10 : i1
  %14 = arith.select %13, %device_n, %0 : !hal.device
  %15 = arith.addi %7, %c1 : index
  cf.br ^bb1(%15, %12, %14 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2(%1, %2 : index, index), ^bb5
^bb2(%7: index, %8: index):  // pred: ^bb1
  %device_n = hal.devices.get %7 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%9: i1):  // 2 preds: ^bb2, ^bb3
  %10 = arith.cmpi eq, %8, %c0 : index
  %11 = arith.select %9, %c1, %c0 : index
  %12 = arith.addi %8, %11 : index
  %13 = arith.andi %9, %10 : i1
  %14 = arith.select %13, %device_n, %0 : !hal.device
  %15 = arith.addi %7, %c1 : index
  cf.br ^bb1(%15, %12, %14 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  cf.cond_br %1, ^bb1, ^bb2(%0 : !hal.command_buffer)
^bb1:  // pred: ^bb0
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  cf.br ^bb2(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer)
^bb2(%2: !hal.command_buffer):  // 2 preds: ^bb0, ^bb1
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  cf.cond_br %1, ^bb1, ^bb2(%0 : !hal.command_buffer)
^bb1:  // pred: ^bb0
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  cf.br ^bb2(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer)
^bb2(%2: !hal.command_buffer):  // 2 preds: ^bb0, ^bb1
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %0 = util.null : !hal.command_buffer
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  cf.cond_br %1, ^bb1, ^bb2(%0 : !hal.command_buffer)
^bb1:  // pred: ^bb0
  cf.br ^bb2(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer)
^bb2(%2: !hal.command_buffer):  // 2 preds: ^bb0, ^bb1
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
  %2 = arith.select %1, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, %0 : !hal.command_buffer
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = arith.select %1, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, %0 : !hal.command_buffer
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%arg0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%arg1) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%arg0: !hal.device, %arg1: i64) -> !hal.command_buffer {
    %0 = util.null : !hal.command_buffer
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %1 = util.cmp.eq %arg0, %__device_0 : !hal.device
    %2 = arith.select %1, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, %0 : !hal.command_buffer
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup(%__device_0, %c-1_i64) : (!hal.device, i64) -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index, iree.fixedpoint.modified} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() -> !hal.command_buffer {
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %0 = util.null : !hal.command_buffer
    %__device_0_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %1 = util.cmp.eq %__device_0, %__device_0_0 : !hal.device
    %2 = arith.select %1, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, %0 : !hal.command_buffer
    util.return %2 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %1 = util.cmp.eq %__device_0, %__device_0_0 : !hal.device
  %2 = arith.select %1, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, %0 : !hal.command_buffer
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() -> !hal.command_buffer {
  %0 = util.null : !hal.command_buffer
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %1 = util.cmp.eq %__device_0, %__device_0 : !hal.device
  %2 = arith.select %1, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, %0 : !hal.command_buffer
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() -> !hal.command_buffer {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %0 = util.null : !hal.command_buffer
  %1 = util.cmp.eq %__device_0, %__device_0 : !hal.device
  %2 = arith.select %1, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, %0 : !hal.command_buffer
  util.return %2 : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() -> !hal.command_buffer {
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {iree.fixedpoint.iteration = 1 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() -> !hal.command_buffer {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module attributes {iree.fixedpoint.iteration = 1 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() -> !hal.command_buffer {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %1 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%1) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module attributes {iree.fixedpoint.iteration = 1 : index, iree.fixedpoint.modified} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() {
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> ()
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() {
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() {
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() {
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() {
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> ()
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> ()
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> ()
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> ()
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {iree.fixedpoint.iteration = 2 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() {
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> ()
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module attributes {iree.fixedpoint.iteration = 2 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() {
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_lookup() : () -> ()
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module attributes {iree.fixedpoint.iteration = 2 : index, iree.fixedpoint.modified} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {iree.fixedpoint.iteration = 3 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module attributes {iree.fixedpoint.iteration = 3 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module attributes {iree.fixedpoint.iteration = 3 : index} {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %0 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
  %4 = util.cmp.eq %3, %0 : !hal.device
  %5 = arith.cmpi slt, %1, %device_count : index
  %6 = arith.andi %4, %5 : i1
  cf.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %1 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
  %8 = arith.cmpi eq, %2, %c0 : index
  %9 = arith.select %7, %c1, %c0 : index
  %10 = arith.addi %2, %9 : index
  %11 = arith.andi %7, %8 : i1
  %12 = arith.select %11, %device_n, %0 : !hal.device
  %13 = arith.addi %1, %c1 : index
  cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %4, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %3, @__device_0 : !hal.device
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c14_i32 = arith.constant 14 : i32
  %0 = util.null : !hal.executable
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0 = util.global.load @__device_0 : !hal.device
  %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb3(%executable : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.initializer {
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok, %value = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %0 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0 = util.global.load @__device_0 : !hal.device
    %1 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %executable = hal.executable.create device(%__device_0 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb3(%executable : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.initializer {
    %0 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %0, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %0 : index, index, !hal.device)
  ^bb1(%1: index, %2: index, %3: !hal.device):  // 2 preds: ^bb0, ^bb4
    %4 = util.cmp.eq %3, %0 : !hal.device
    %5 = arith.cmpi slt, %1, %device_count : index
    %6 = arith.andi %4, %5 : i1
    cf.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %1 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%7: i1):  // 2 preds: ^bb2, ^bb3
    %8 = arith.cmpi eq, %2, %c0 : index
    %9 = arith.select %7, %c1, %c0 : index
    %10 = arith.addi %2, %9 : index
    %11 = arith.andi %7, %8 : i1
    %12 = arith.select %11, %device_n, %0 : !hal.device
    %13 = arith.addi %1, %c1 : index
    cf.br ^bb1(%13, %10, %12 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %4, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %3, @__device_0 : !hal.device
    cf.br ^bb8
  ^bb8:  // pred: ^bb7
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %c-1 = arith.constant -1 : index
    %c0_4 = arith.constant 0 : index
    %c14_i32 = arith.constant 14 : i32
    %14 = util.null : !hal.executable
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0_5 = util.global.load @__device_0 : !hal.device
    %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0_4, %c-1 : index
    %16 = arith.cmpi eq, %15, %c0_4 : index
    cf.cond_br %16, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %executable = hal.executable.create device(%__device_0_5 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb11(%executable : !hal.executable)
  ^bb10:  // pred: ^bb8
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb11(%14 : !hal.executable)
  ^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
    util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    cf.br ^bb12
  ^bb12:  // pred: ^bb11
    %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  cf.br ^bb8
^bb8:  // pred: ^bb7
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb11(%executable : !hal.executable)
^bb10:  // pred: ^bb8
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb11(%0 : !hal.executable)
^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  cf.br ^bb12
^bb12:  // pred: ^bb11
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  cf.br ^bb8
^bb8:  // pred: ^bb7
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb11(%executable : !hal.executable)
^bb10:  // pred: ^bb8
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb11(%0 : !hal.executable)
^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  cf.br ^bb12
^bb12:  // pred: ^bb11
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  cf.br ^bb8
^bb8:  // pred: ^bb7
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb11(%executable : !hal.executable)
^bb10:  // pred: ^bb8
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb11(%0 : !hal.executable)
^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  cf.br ^bb12
^bb12:  // pred: ^bb11
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  cf.br ^bb8
^bb8:  // pred: ^bb7
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb11(%executable : !hal.executable)
^bb10:  // pred: ^bb8
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb11(%0 : !hal.executable)
^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  cf.br ^bb12
^bb12:  // pred: ^bb11
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  cf.br ^bb8
^bb8:  // pred: ^bb7
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb11(%executable : !hal.executable)
^bb10:  // pred: ^bb8
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb11(%0 : !hal.executable)
^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  cf.br ^bb12
^bb12:  // pred: ^bb11
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  cf.br ^bb8
^bb8:  // pred: ^bb7
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb11(%executable : !hal.executable)
^bb10:  // pred: ^bb8
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb11(%0 : !hal.executable)
^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  cf.br ^bb12
^bb12:  // pred: ^bb11
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
  ^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
    %5 = util.cmp.eq %4, %1 : !hal.device
    %6 = arith.cmpi slt, %2, %device_count : index
    %7 = arith.andi %5, %6 : i1
    cf.cond_br %7, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %2 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
    %9 = arith.cmpi eq, %3, %c0 : index
    %10 = arith.select %8, %c1, %c0 : index
    %11 = arith.addi %3, %10 : index
    %12 = arith.andi %8, %9 : i1
    %13 = arith.select %12, %device_n, %1 : !hal.device
    %14 = arith.addi %2, %c1 : index
    cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %4, @__device_0 : !hal.device
    cf.br ^bb8
  ^bb8:  // pred: ^bb7
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0_4 = util.global.load @__device_0 : !hal.device
    %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %16 = arith.cmpi eq, %15, %c0 : index
    cf.cond_br %16, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb11(%executable : !hal.executable)
  ^bb10:  // pred: ^bb8
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb11(%0 : !hal.executable)
  ^bb11(%17: !hal.executable):  // 2 preds: ^bb9, ^bb10
    util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    cf.br ^bb12
  ^bb12:  // pred: ^bb11
    %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb10(%executable : !hal.executable)
^bb9:  // pred: ^bb7
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb10(%0 : !hal.executable)
^bb10(%17: !hal.executable):  // 2 preds: ^bb8, ^bb9
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After CSE (cse) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  util.global.store %4, @__device_0 : !hal.device
  %__device_0 = util.global.load @__device_0 : !hal.device
  %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  %__device_0_4 = util.global.load @__device_0 : !hal.device
  %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  cf.cond_br %16, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb10(%executable : !hal.executable)
^bb9:  // pred: ^bb7
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb10(%0 : !hal.executable)
^bb10(%17: !hal.executable):  // 2 preds: ^bb8, ^bb9
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After CSE (cse) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
  ^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
    %5 = util.cmp.eq %4, %1 : !hal.device
    %6 = arith.cmpi slt, %2, %device_count : index
    %7 = arith.andi %5, %6 : i1
    cf.cond_br %7, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %2 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
    %9 = arith.cmpi eq, %3, %c0 : index
    %10 = arith.select %8, %c1, %c0 : index
    %11 = arith.addi %3, %10 : index
    %12 = arith.andi %8, %9 : i1
    %13 = arith.select %12, %device_n, %1 : !hal.device
    %14 = arith.addi %2, %c1 : index
    cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    util.global.store %4, @__device_0 : !hal.device
    %__device_0 = util.global.load @__device_0 : !hal.device
    %ok_2, %value_3 = hal.device.query<%__device_0 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0_query_0_hal_executable_format_cuda_nvptx_fb = util.global.load @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
    %__device_0_4 = util.global.load @__device_0 : !hal.device
    %15 = arith.select %__device_0_query_0_hal_executable_format_cuda_nvptx_fb, %c0, %c-1 : index
    %16 = arith.cmpi eq, %15, %c0 : index
    cf.cond_br %16, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %executable = hal.executable.create device(%__device_0_4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb10(%executable : !hal.executable)
  ^bb9:  // pred: ^bb7
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb10(%0 : !hal.executable)
  ^bb10(%17: !hal.executable):  // 2 preds: ^bb8, ^bb9
    util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.global private @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c-1_i64 = arith.constant -1 : i64
    %c16 = arith.constant 16 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c-1_i64 = arith.constant -1 : i64
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  %ok_2, %value_3 = hal.device.query<%4 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %15 = arith.select %value_3, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  util.global.store %4, @__device_0 : !hal.device
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  cf.cond_br %16, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %executable = hal.executable.create device(%4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb10(%executable : !hal.executable)
^bb9:  // pred: ^bb7
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb10(%0 : !hal.executable)
^bb10(%17: !hal.executable):  // 2 preds: ^bb8, ^bb9
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.initializer {
  %0 = util.null : !hal.executable
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c18_i32 = arith.constant 18 : i32
  %false = arith.constant false
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %1 = util.null : !hal.device
  %device_count = hal.devices.count : index
  cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
  %5 = util.cmp.eq %4, %1 : !hal.device
  %6 = arith.cmpi slt, %2, %device_count : index
  %7 = arith.andi %5, %6 : i1
  cf.cond_br %7, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %device_n = hal.devices.get %2 : !hal.device
  %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
  cf.cond_br %value, ^bb3, ^bb4(%false : i1)
^bb3:  // pred: ^bb2
  %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.br ^bb4(%value_1 : i1)
^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
  %9 = arith.cmpi eq, %3, %c0 : index
  %10 = arith.select %8, %c1, %c0 : index
  %11 = arith.addi %3, %10 : index
  %12 = arith.andi %8, %9 : i1
  %13 = arith.select %12, %device_n, %1 : !hal.device
  %14 = arith.addi %2, %c1 : index
  cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
^bb5:  // pred: ^bb1
  cf.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  cf.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  %ok_2, %value_3 = hal.device.query<%4 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %15 = arith.select %value_3, %c0, %c-1 : index
  %16 = arith.cmpi eq, %15, %c0 : index
  util.global.store %4, @__device_0 : !hal.device
  util.global.store %value_3, @__device_0_query_0_hal_executable_format_cuda_nvptx_fb : i1
  cf.cond_br %16, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %executable = hal.executable.create device(%4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
  cf.br ^bb10(%executable : !hal.executable)
^bb9:  // pred: ^bb7
  util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  cf.br ^bb10(%0 : !hal.executable)
^bb10(%17: !hal.executable):  // 2 preds: ^bb8, ^bb9
  util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
  util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %c-1_i64 = arith.constant -1 : i64
  %c16 = arith.constant 16 : index
  %c3 = arith.constant 3 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c16 = arith.constant 16 : index
  %c-1_i64 = arith.constant -1 : i64
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
    (%c0 : index)[%c0, %c131072], 
    (%c1 : index)[%c0, %c131072], 
    (%c2 : index)[%c0, %c524288]
  ]) flags("None")
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  util.return %cmd : !hal.command_buffer
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %c-1_i32 = arith.constant -1 : i32
  %0 = util.null : !hal.fence
  %c-1_i64 = arith.constant -1 : i64
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %c512 = arith.constant 512 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c524288 = arith.constant 524288 : index
  %c-1_i64 = arith.constant -1 : i64
  %0 = util.null : !hal.fence
  %c-1_i32 = arith.constant -1 : i32
  %__device_0 = util.global.load immutable @__device_0 : !hal.device
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
    (%buffer : !hal.buffer)[%c0, %c131072], 
    (%buffer_0 : !hal.buffer)[%c0, %c131072], 
    (%buffer_1 : !hal.buffer)[%c0, %c524288]
  ])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
  ^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
    %5 = util.cmp.eq %4, %1 : !hal.device
    %6 = arith.cmpi slt, %2, %device_count : index
    %7 = arith.andi %5, %6 : i1
    cf.cond_br %7, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %2 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
    %9 = arith.cmpi eq, %3, %c0 : index
    %10 = arith.select %8, %c1, %c0 : index
    %11 = arith.addi %3, %10 : index
    %12 = arith.andi %8, %9 : i1
    %13 = arith.select %12, %device_n, %1 : !hal.device
    %14 = arith.addi %2, %c1 : index
    cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    %ok_2, %value_3 = hal.device.query<%4 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %15 = arith.select %value_3, %c0, %c-1 : index
    %16 = arith.cmpi eq, %15, %c0 : index
    util.global.store %4, @__device_0 : !hal.device
    cf.cond_br %16, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %executable = hal.executable.create device(%4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb10(%executable : !hal.executable)
  ^bb9:  // pred: ^bb7
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb10(%0 : !hal.executable)
  ^bb10(%17: !hal.executable):  // 2 preds: ^bb8, ^bb9
    util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  util.global private @__device_0 : !hal.device
  util.initializer {
    %0 = util.null : !hal.executable
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c18_i32 = arith.constant 18 : i32
    %false = arith.constant false
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %1 = util.null : !hal.device
    %device_count = hal.devices.count : index
    cf.br ^bb1(%c0, %c0, %1 : index, index, !hal.device)
  ^bb1(%2: index, %3: index, %4: !hal.device):  // 2 preds: ^bb0, ^bb4
    %5 = util.cmp.eq %4, %1 : !hal.device
    %6 = arith.cmpi slt, %2, %device_count : index
    %7 = arith.andi %5, %6 : i1
    cf.cond_br %7, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %device_n = hal.devices.get %2 : !hal.device
    %ok, %value = hal.device.query<%device_n : !hal.device> key("hal.device.id" :: "cuda") : i1, i1 = false
    cf.cond_br %value, ^bb3, ^bb4(%false : i1)
  ^bb3:  // pred: ^bb2
    %ok_0, %value_1 = hal.device.query<%device_n : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    cf.br ^bb4(%value_1 : i1)
  ^bb4(%8: i1):  // 2 preds: ^bb2, ^bb3
    %9 = arith.cmpi eq, %3, %c0 : index
    %10 = arith.select %8, %c1, %c0 : index
    %11 = arith.addi %3, %10 : index
    %12 = arith.andi %8, %9 : i1
    %13 = arith.select %12, %device_n, %1 : !hal.device
    %14 = arith.addi %2, %c1 : index
    cf.br ^bb1(%14, %11, %13 : index, index, !hal.device)
  ^bb5:  // pred: ^bb1
    cf.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    util.status.check_ok %c18_i32, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    %ok_2, %value_3 = hal.device.query<%4 : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %15 = arith.select %value_3, %c0, %c-1 : index
    %16 = arith.cmpi eq, %15, %c0 : index
    util.global.store %4, @__device_0 : !hal.device
    cf.cond_br %16, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %executable = hal.executable.create device(%4 : !hal.device) target(@matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0::@cuda_nvptx_fb) : !hal.executable
    cf.br ^bb10(%executable : !hal.executable)
  ^bb9:  // pred: ^bb7
    util.status.check_ok %c14_i32, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    cf.br ^bb10(%0 : !hal.executable)
  ^bb10(%17: !hal.executable):  // 2 preds: ^bb8, ^bb9
    util.global.store %17, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %18 = util.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !hal.command_buffer
    util.global.store %18, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    util.return
  }
  util.global private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
  hal.executable private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense_resource<__elided__> : vector<7136xi8>, format = "cuda-nvptx-fb"}
  }
  util.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !hal.command_buffer attributes {inlining_policy = #util.inline.never} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c16 = arith.constant 16 : index
    %c-1_i64 = arith.constant -1 : i64
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = util.global.load immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable
    %cmd = hal.command_buffer.create device(%__device_0 : !hal.device) mode("None") categories("Transfer|Dispatch") affinity(%c-1_i64) bindings(%c3) : !hal.command_buffer
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !hal.executable)[%c0] workgroups([%c16, %c16, %c1]) bindings([
      (%c0 : index)[%c0, %c131072], 
      (%c1 : index)[%c0, %c131072], 
      (%c2 : index)[%c0, %c524288]
    ]) flags("None")
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    util.return %cmd : !hal.command_buffer
  }
  util.global private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
  util.func public @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = arith.constant 512 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c524288 = arith.constant 524288 : index
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %__device_0 = util.global.load immutable @__device_0 : !hal.device
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = util.global.load immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !hal.command_buffer
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c512, %c128]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %allocator = hal.device.allocator<%__device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c128, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c131072) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("input2") shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %fence = hal.fence.create device(%__device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute.indirect<%__device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands(%__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0) bindings([
      (%buffer : !hal.buffer)[%c0, %c131072], 
      (%buffer_0 : !hal.buffer)[%c0, %c131072], 
      (%buffer_1 : !hal.buffer)[%c0, %c524288]
    ])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c524288] shape([%c512, %c512]) type(%element_type_f16) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ConversionPass (iree-vm-conversion) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1 = vm.const.i64 1
      %null_1 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_1 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %req = vm.cmp.eq.ref %4, %null_1 : !vm.ref<!hal.device>
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %5 = vm.and.i32 %req, %slt : i32
      vm.cond_br %5, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %6 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%6) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %buffer = vm.rodata.inline "_utf8_hal_device_id_A6960C904947097B" {alignment = 1 : i64} : !vm.buffer = "hal.device.id"
      %buffer_2 = vm.rodata.inline "_utf8_cuda_3892F998A181BAD9" {alignment = 1 : i64} : !vm.buffer = "cuda"
      %7:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_2) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %7#1 : i64
      %zero_3 = vm.const.i32.zero
      %8 = vm.select.i32 %7#0, %nz, %zero_3 : i32
      %c1_4 = vm.const.i32 1
      vm.cond_br %8, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %buffer_5 = vm.rodata.inline "_utf8_hal_executable_format_476C407664EC05AD" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
      %buffer_6 = vm.rodata.inline "_utf8_cuda_nvptx_fb_F022C9DEA6678D1B" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
      %9:2 = vm.call @hal.device.query.i64(%ref, %buffer_5, %buffer_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %9#1 : i64
      %zero_8 = vm.const.i32.zero
      %10 = vm.select.i32 %9#0, %nz_7, %zero_8 : i32
      %c1_9 = vm.const.i32 1
      vm.br ^bb4(%10 : i32)
    ^bb4(%11: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %12 = vm.select.i64 %11, %c1, %zero_0 : i64
      %13 = vm.add.i64 %3, %12 : i64
      %14 = vm.and.i32 %11, %eq : i32
      %ref_10 = vm.select.ref %14, %ref, %null_1 : !vm.ref<!hal.device>
      %15 = vm.add.i64 %2, %c1 : i64
      vm.br ^bb1(%15, %13, %ref_10 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %req, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.cond_fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<"cuda", [#hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
      vm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      %buffer_11 = vm.rodata.inline "_utf8_hal_executable_format_476C407664EC05AD" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
      %buffer_12 = vm.rodata.inline "_utf8_cuda_nvptx_fb_F022C9DEA6678D1B" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
      %16:2 = vm.call @hal.device.query.i64(%4, %buffer_11, %buffer_12) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_13 = vm.cmp.nz.i64 %16#1 : i64
      %zero_14 = vm.const.i32.zero
      %17 = vm.select.i32 %16#0, %nz_13, %zero_14 : i32
      %c1_15 = vm.const.i32 1
      %18 = vm.select.i64 %17, %zero_0, %c-1 : i64
      %eq_16 = vm.cmp.eq.i64 %18, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_16, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %buffer_17 = vm.rodata.inline "matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb" {alignment = 16 : i64} : !vm.buffer = dense_resource<__elided__> : vector<7136xi8>
      %buffer_18 = vm.rodata.inline "_utf8_cuda_nvptx_fb_F022C9DEA6678D1B" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
      %null_19 = vm.const.ref.zero : !vm.buffer
      %ref_20 = vm.call @hal.executable.create(%4, %buffer_18, %buffer_17, %null_19) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.br ^bb10(%ref_20 : !vm.ref<!hal.executable>)
    ^bb9:  // pred: ^bb7
      vm.cond_fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      vm.br ^bb10(%null : !vm.ref<!hal.executable>)
    ^bb10(%19: !vm.ref<!hal.executable>):  // 2 preds: ^bb8, ^bb9
      vm.global.store.ref %19, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_21 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_21, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %c2 = vm.const.i64 2
      %c1 = vm.const.i64 1
      %zero = vm.const.i64.zero
      %c3 = vm.const.i64 3
      %c16 = vm.const.i64 16
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %zero_0 = vm.const.i32.zero
      %c3_1 = vm.const.i32 3
      %c3_2 = vm.const.i32 3
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero_0, %c3_1, %c-1, %c3_2) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      %zero_3 = vm.const.i32.zero
      %zero_4 = vm.const.i64 0
      %zero_5 = vm.const.i32.zero
      %c16_6 = vm.const.i32 16
      %c16_7 = vm.const.i32 16
      %c1_8 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %zero_9 = vm.const.i32.zero
      %null_10 = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1_11 = vm.const.i32 1
      %null_12 = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c2_13 = vm.const.i32 2
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero_5, %c16_6, %c16_7, %c1_8, %zero_4, [], [(%zero_3, %zero_9, %null, %zero, %c131072), (%zero_3, %c1_11, %null_10, %zero, %c131072), (%zero_3, %c2_13, %null_12, %zero, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %c28 = vm.const.i32 28
      %c13 = vm.const.i32 13
      %zero_14 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero_14) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.update_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer_slot : i32, %target_buffer_slot : i32, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer_slot : i32, %recv_buffer_slot : i32, %send_buffer : !vm.ref<!hal.buffer>, %recv_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer_slot : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.fill(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i64, %pattern_length : i32, %flags : i64)
    vm.import private @hal.device.queue.update(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
    vm.import private @hal.device.queue.copy(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_0 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %c553648144 = vm.const.i32 553648144
      %c1 = vm.const.i32 1
      %buffer = vm.rodata.inline "_utf8_input0_1C2D713EC0D8C61" {alignment = 1 : i64} : !vm.buffer = "input0"
      vm.call.variadic @hal.buffer_view.assert(%arg0, %buffer, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_1 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %buffer_2 = vm.rodata.inline "_utf8_tensor_A0C577B530E5F64B" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16 = vm.const.i32 16
      %c3075 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref, %buffer_2, %ref_1, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %buffer_3 = vm.rodata.inline "_utf8_input1_3C6604AFD2642C80" {alignment = 1 : i64} : !vm.buffer = "input1"
      vm.call.variadic @hal.buffer_view.assert(%arg1, %buffer_3, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_4 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %buffer_5 = vm.rodata.inline "_utf8_tensor_A0C577B530E5F64B" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16_6 = vm.const.i32 16
      %c3075_7 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref_4, %buffer_5, %ref_1, %c131072, %c16_6, %c3075_7) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %buffer_8 = vm.rodata.inline "_utf8_input2_CE2ED9AC1537A2A5" {alignment = 1 : i64} : !vm.buffer = "input2"
      vm.call.variadic @hal.buffer_view.assert(%arg2, %buffer_8, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %buffer_10 = vm.rodata.inline "_utf8_tensor_A0C577B530E5F64B" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16_11 = vm.const.i32 16
      %c3075_12 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref_9, %buffer_10, %ref_1, %c524288, %c16_11, %c3075_12) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %zero_13 = vm.const.i32.zero
      %ref_14 = vm.call @hal.fence.create(%__device_0, %zero_13) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_14, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero, %c131072), (%ref_4, %zero, %c131072), (%ref_9, %zero, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_0, [%ref_14]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %0, "failed to wait on timepoint"
      %ref_15 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_15 : !vm.ref<!hal.buffer_view>
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ReifyRodataTablesPass (iree-vm-reify-rodata-tables) //----- //
vm.module public @module {
  vm.global.ref private @__device_0 : !vm.ref<!hal.device>
  vm.initializer {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1 = vm.const.i64 1
    %null_1 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_1 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %req = vm.cmp.eq.ref %4, %null_1 : !vm.ref<!hal.device>
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %5 = vm.and.i32 %req, %slt : i32
    vm.cond_br %5, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %6 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%6) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %buffer = vm.rodata.inline "_utf8_hal_device_id_A6960C904947097B" {alignment = 1 : i64} : !vm.buffer = "hal.device.id"
    %buffer_2 = vm.rodata.inline "_utf8_cuda_3892F998A181BAD9" {alignment = 1 : i64} : !vm.buffer = "cuda"
    %7:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_2) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %7#1 : i64
    %zero_3 = vm.const.i32.zero
    %8 = vm.select.i32 %7#0, %nz, %zero_3 : i32
    %c1_4 = vm.const.i32 1
    vm.cond_br %8, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %buffer_5 = vm.rodata.inline "_utf8_hal_executable_format_476C407664EC05AD" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
    %buffer_6 = vm.rodata.inline "_utf8_cuda_nvptx_fb_F022C9DEA6678D1B" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
    %9:2 = vm.call @hal.device.query.i64(%ref, %buffer_5, %buffer_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %9#1 : i64
    %zero_8 = vm.const.i32.zero
    %10 = vm.select.i32 %9#0, %nz_7, %zero_8 : i32
    %c1_9 = vm.const.i32 1
    vm.br ^bb4(%10 : i32)
  ^bb4(%11: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %12 = vm.select.i64 %11, %c1, %zero_0 : i64
    %13 = vm.add.i64 %3, %12 : i64
    %14 = vm.and.i32 %11, %eq : i32
    %ref_10 = vm.select.ref %14, %ref, %null_1 : !vm.ref<!hal.device>
    %15 = vm.add.i64 %2, %c1 : i64
    vm.br ^bb1(%15, %13, %ref_10 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %req, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.cond_fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<"cuda", [#hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    vm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    %buffer_11 = vm.rodata.inline "_utf8_hal_executable_format_476C407664EC05AD" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
    %buffer_12 = vm.rodata.inline "_utf8_cuda_nvptx_fb_F022C9DEA6678D1B" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
    %16:2 = vm.call @hal.device.query.i64(%4, %buffer_11, %buffer_12) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_13 = vm.cmp.nz.i64 %16#1 : i64
    %zero_14 = vm.const.i32.zero
    %17 = vm.select.i32 %16#0, %nz_13, %zero_14 : i32
    %c1_15 = vm.const.i32 1
    %18 = vm.select.i64 %17, %zero_0, %c-1 : i64
    %eq_16 = vm.cmp.eq.i64 %18, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_16, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %buffer_17 = vm.rodata.inline "matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb" {alignment = 16 : i64} : !vm.buffer = dense_resource<__elided__> : vector<7136xi8>
    %buffer_18 = vm.rodata.inline "_utf8_cuda_nvptx_fb_F022C9DEA6678D1B" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
    %null_19 = vm.const.ref.zero : !vm.buffer
    %ref_20 = vm.call @hal.executable.create(%4, %buffer_18, %buffer_17, %null_19) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.br ^bb10(%ref_20 : !vm.ref<!hal.executable>)
  ^bb9:  // pred: ^bb7
    vm.cond_fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    vm.br ^bb10(%null : !vm.ref<!hal.executable>)
  ^bb10(%19: !vm.ref<!hal.executable>):  // 2 preds: ^bb8, ^bb9
    vm.global.store.ref %19, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_21 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_21, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  }
  vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %c2 = vm.const.i64 2
    %c1 = vm.const.i64 1
    %zero = vm.const.i64.zero
    %c3 = vm.const.i64 3
    %c16 = vm.const.i64 16
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %zero_0 = vm.const.i32.zero
    %c3_1 = vm.const.i32 3
    %c3_2 = vm.const.i32 3
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero_0, %c3_1, %c-1, %c3_2) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    %zero_3 = vm.const.i32.zero
    %zero_4 = vm.const.i64 0
    %zero_5 = vm.const.i32.zero
    %c16_6 = vm.const.i32 16
    %c16_7 = vm.const.i32 16
    %c1_8 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %zero_9 = vm.const.i32.zero
    %null_10 = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1_11 = vm.const.i32 1
    %null_12 = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c2_13 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero_5, %c16_6, %c16_7, %c1_8, %zero_4, [], [(%zero_3, %zero_9, %null, %zero, %c131072), (%zero_3, %c1_11, %null_10, %zero, %c131072), (%zero_3, %c2_13, %null_12, %zero, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_14 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero_14) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.update_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer_slot : i32, %target_buffer_slot : i32, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer_slot : i32, %recv_buffer_slot : i32, %send_buffer : !vm.ref<!hal.buffer>, %recv_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer_slot : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.fill(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i64, %pattern_length : i32, %flags : i64)
  vm.import private @hal.device.queue.update(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.copy(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_0 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %c553648144 = vm.const.i32 553648144
    %c1 = vm.const.i32 1
    %buffer = vm.rodata.inline "_utf8_input0_1C2D713EC0D8C61" {alignment = 1 : i64} : !vm.buffer = "input0"
    vm.call.variadic @hal.buffer_view.assert(%arg0, %buffer, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_1 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %buffer_2 = vm.rodata.inline "_utf8_tensor_A0C577B530E5F64B" {alignment = 1 : i64} : !vm.buffer = "tensor"
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %buffer_2, %ref_1, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %buffer_3 = vm.rodata.inline "_utf8_input1_3C6604AFD2642C80" {alignment = 1 : i64} : !vm.buffer = "input1"
    vm.call.variadic @hal.buffer_view.assert(%arg1, %buffer_3, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %buffer_5 = vm.rodata.inline "_utf8_tensor_A0C577B530E5F64B" {alignment = 1 : i64} : !vm.buffer = "tensor"
    %c16_6 = vm.const.i32 16
    %c3075_7 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_4, %buffer_5, %ref_1, %c131072, %c16_6, %c3075_7) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %buffer_8 = vm.rodata.inline "_utf8_input2_CE2ED9AC1537A2A5" {alignment = 1 : i64} : !vm.buffer = "input2"
    vm.call.variadic @hal.buffer_view.assert(%arg2, %buffer_8, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %buffer_10 = vm.rodata.inline "_utf8_tensor_A0C577B530E5F64B" {alignment = 1 : i64} : !vm.buffer = "tensor"
    %c16_11 = vm.const.i32 16
    %c3075_12 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_9, %buffer_10, %ref_1, %c524288, %c16_11, %c3075_12) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %zero_13 = vm.const.i32.zero
    %ref_14 = vm.call @hal.fence.create(%__device_0, %zero_13) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_14, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero, %c131072), (%ref_4, %zero, %c131072), (%ref_9, %zero, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_0, [%ref_14]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_15 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_15 : !vm.ref<!hal.buffer_view>
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::HoistInlinedRodataPass (iree-vm-hoist-inlined-rodata) //----- //
vm.module public @module {
  vm.global.ref private @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD_0 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_1 {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_2 {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.initializer {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1 = vm.const.i64 1
    %null_1 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_1 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %req = vm.cmp.eq.ref %4, %null_1 : !vm.ref<!hal.device>
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %5 = vm.and.i32 %req, %slt : i32
    vm.cond_br %5, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %6 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%6) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %7:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %7#1 : i64
    %zero_2 = vm.const.i32.zero
    %8 = vm.select.i32 %7#0, %nz, %zero_2 : i32
    %c1_3 = vm.const.i32 1
    vm.cond_br %8, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %9:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_4 = vm.cmp.nz.i64 %9#1 : i64
    %zero_5 = vm.const.i32.zero
    %10 = vm.select.i32 %9#0, %nz_4, %zero_5 : i32
    %c1_6 = vm.const.i32 1
    vm.br ^bb4(%10 : i32)
  ^bb4(%11: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %12 = vm.select.i64 %11, %c1, %zero_0 : i64
    %13 = vm.add.i64 %3, %12 : i64
    %14 = vm.and.i32 %11, %eq : i32
    %ref_7 = vm.select.ref %14, %ref, %null_1 : !vm.ref<!hal.device>
    %15 = vm.add.i64 %2, %c1 : i64
    vm.br ^bb1(%15, %13, %ref_7 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %req, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.cond_fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<"cuda", [#hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    vm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    %_utf8_hal_executable_format_476C407664EC05AD_0 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD_0 : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_1 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_1 : !vm.buffer
    %16:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_0, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_1) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_8 = vm.cmp.nz.i64 %16#1 : i64
    %zero_9 = vm.const.i32.zero
    %17 = vm.select.i32 %16#0, %nz_8, %zero_9 : i32
    %c1_10 = vm.const.i32 1
    %18 = vm.select.i64 %17, %zero_0, %c-1 : i64
    %eq_11 = vm.cmp.eq.i64 %18, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_11, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_2 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_2 : !vm.buffer
    %null_12 = vm.const.ref.zero : !vm.buffer
    %ref_13 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_2, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null_12) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.br ^bb10(%ref_13 : !vm.ref<!hal.executable>)
  ^bb9:  // pred: ^bb7
    vm.cond_fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    vm.br ^bb10(%null : !vm.ref<!hal.executable>)
  ^bb10(%19: !vm.ref<!hal.executable>):  // 2 preds: ^bb8, ^bb9
    vm.global.store.ref %19, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_14 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_14, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  }
  vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %c2 = vm.const.i64 2
    %c1 = vm.const.i64 1
    %zero = vm.const.i64.zero
    %c3 = vm.const.i64 3
    %c16 = vm.const.i64 16
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %zero_0 = vm.const.i32.zero
    %c3_1 = vm.const.i32 3
    %c3_2 = vm.const.i32 3
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero_0, %c3_1, %c-1, %c3_2) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    %zero_3 = vm.const.i32.zero
    %zero_4 = vm.const.i64 0
    %zero_5 = vm.const.i32.zero
    %c16_6 = vm.const.i32 16
    %c16_7 = vm.const.i32 16
    %c1_8 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %zero_9 = vm.const.i32.zero
    %null_10 = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1_11 = vm.const.i32 1
    %null_12 = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c2_13 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero_5, %c16_6, %c16_7, %c1_8, %zero_4, [], [(%zero_3, %zero_9, %null, %zero, %c131072), (%zero_3, %c1_11, %null_10, %zero, %c131072), (%zero_3, %c2_13, %null_12, %zero, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_14 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero_14) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.update_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer_slot : i32, %target_buffer_slot : i32, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer_slot : i32, %recv_buffer_slot : i32, %send_buffer : !vm.ref<!hal.buffer>, %recv_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer_slot : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.fill(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i64, %pattern_length : i32, %flags : i64)
  vm.import private @hal.device.queue.update(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.copy(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B_3 {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B_4 {alignment = 1 : i64} "tensor"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_0 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %c553648144 = vm.const.i32 553648144
    %c1 = vm.const.i32 1
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_1 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_1, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_2 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_3 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B_3 : !vm.buffer
    %c16_3 = vm.const.i32 16
    %c3075_4 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_2, %_utf8_tensor_A0C577B530E5F64B_3, %ref_1, %c131072, %c16_3, %c3075_4) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B_4 : !vm.buffer
    %c16_6 = vm.const.i32 16
    %c3075_7 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_4, %ref_1, %c524288, %c16_6, %c3075_7) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %zero_8 = vm.const.i32.zero
    %ref_9 = vm.call @hal.fence.create(%__device_0, %zero_8) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_9, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero, %c131072), (%ref_2, %zero, %c131072), (%ref_5, %zero, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_0, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DeduplicateRodataPass (iree-vm-deduplicate-rodata) //----- //
vm.module public @module {
  vm.global.ref private @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.initializer {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1 = vm.const.i64 1
    %null_1 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_1 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %req = vm.cmp.eq.ref %4, %null_1 : !vm.ref<!hal.device>
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %5 = vm.and.i32 %req, %slt : i32
    vm.cond_br %5, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %6 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%6) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %7:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %7#1 : i64
    %zero_2 = vm.const.i32.zero
    %8 = vm.select.i32 %7#0, %nz, %zero_2 : i32
    %c1_3 = vm.const.i32 1
    vm.cond_br %8, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %9:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_4 = vm.cmp.nz.i64 %9#1 : i64
    %zero_5 = vm.const.i32.zero
    %10 = vm.select.i32 %9#0, %nz_4, %zero_5 : i32
    %c1_6 = vm.const.i32 1
    vm.br ^bb4(%10 : i32)
  ^bb4(%11: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %12 = vm.select.i64 %11, %c1, %zero_0 : i64
    %13 = vm.add.i64 %3, %12 : i64
    %14 = vm.and.i32 %11, %eq : i32
    %ref_7 = vm.select.ref %14, %ref, %null_1 : !vm.ref<!hal.device>
    %15 = vm.add.i64 %2, %c1 : i64
    vm.br ^bb1(%15, %13, %ref_7 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %req, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.cond_fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<"cuda", [#hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    vm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    %_utf8_hal_executable_format_476C407664EC05AD_8 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %16:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_8, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_10 = vm.cmp.nz.i64 %16#1 : i64
    %zero_11 = vm.const.i32.zero
    %17 = vm.select.i32 %16#0, %nz_10, %zero_11 : i32
    %c1_12 = vm.const.i32 1
    %18 = vm.select.i64 %17, %zero_0, %c-1 : i64
    %eq_13 = vm.cmp.eq.i64 %18, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_13, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_14 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %null_15 = vm.const.ref.zero : !vm.buffer
    %ref_16 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_14, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null_15) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.br ^bb10(%ref_16 : !vm.ref<!hal.executable>)
  ^bb9:  // pred: ^bb7
    vm.cond_fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    vm.br ^bb10(%null : !vm.ref<!hal.executable>)
  ^bb10(%19: !vm.ref<!hal.executable>):  // 2 preds: ^bb8, ^bb9
    vm.global.store.ref %19, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_17 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_17, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  }
  vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %c2 = vm.const.i64 2
    %c1 = vm.const.i64 1
    %zero = vm.const.i64.zero
    %c3 = vm.const.i64 3
    %c16 = vm.const.i64 16
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %zero_0 = vm.const.i32.zero
    %c3_1 = vm.const.i32 3
    %c3_2 = vm.const.i32 3
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero_0, %c3_1, %c-1, %c3_2) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    %zero_3 = vm.const.i32.zero
    %zero_4 = vm.const.i64 0
    %zero_5 = vm.const.i32.zero
    %c16_6 = vm.const.i32 16
    %c16_7 = vm.const.i32 16
    %c1_8 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %zero_9 = vm.const.i32.zero
    %null_10 = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1_11 = vm.const.i32 1
    %null_12 = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c2_13 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero_5, %c16_6, %c16_7, %c1_8, %zero_4, [], [(%zero_3, %zero_9, %null, %zero, %c131072), (%zero_3, %c1_11, %null_10, %zero, %c131072), (%zero_3, %c2_13, %null_12, %zero, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_14 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero_14) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.update_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer_slot : i32, %target_buffer_slot : i32, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer_slot : i32, %recv_buffer_slot : i32, %send_buffer : !vm.ref<!hal.buffer>, %recv_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer_slot : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.fill(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i64, %pattern_length : i32, %flags : i64)
  vm.import private @hal.device.queue.update(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.copy(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_0 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %c553648144 = vm.const.i32 553648144
    %c1 = vm.const.i32 1
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_1 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_1, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_2 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_3 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    %c16_4 = vm.const.i32 16
    %c3075_5 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_2, %_utf8_tensor_A0C577B530E5F64B_3, %ref_1, %c131072, %c16_4, %c3075_5) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_7 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    %c16_8 = vm.const.i32 16
    %c3075_9 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_A0C577B530E5F64B_7, %ref_1, %c524288, %c16_8, %c3075_9) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %zero_10 = vm.const.i32.zero
    %ref_11 = vm.call @hal.fence.create(%__device_0, %zero_10) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_11, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero, %c131072), (%ref_2, %zero, %c131072), (%ref_6, %zero, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_0, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropUnusedCallsPass (iree-vm-drop-unused-calls) //----- //
vm.module public @module {
  vm.global.ref private @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.initializer {
    %null = vm.const.ref.zero : !vm.buffer
    %null_0 = vm.const.ref.zero : !vm.ref<!hal.executable>
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_1 = vm.const.i64.zero
    %c1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_1, %zero_1, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %req = vm.cmp.eq.ref %4, %null_2 : !vm.ref<!hal.device>
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %5 = vm.and.i32 %req, %slt : i32
    vm.cond_br %5, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %6 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%6) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %7:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %7#1 : i64
    %8 = vm.select.i32 %7#0, %nz, %zero : i32
    vm.cond_br %8, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %9:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %9#1 : i64
    %10 = vm.select.i32 %9#0, %nz_3, %zero : i32
    vm.br ^bb4(%10 : i32)
  ^bb4(%11: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_1 : i64
    %12 = vm.select.i64 %11, %c1, %zero_1 : i64
    %13 = vm.add.i64 %3, %12 : i64
    %14 = vm.and.i32 %11, %eq : i32
    %ref_4 = vm.select.ref %14, %ref, %null_2 : !vm.ref<!hal.device>
    %15 = vm.add.i64 %2, %c1 : i64
    vm.br ^bb1(%15, %13, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %req, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.cond_fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<"cuda", [#hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    vm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %16:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %16#1 : i64
    %17 = vm.select.i32 %16#0, %nz_7, %zero : i32
    %18 = vm.select.i64 %17, %zero_1, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %18, %zero_1 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.br ^bb10(%ref_10 : !vm.ref<!hal.executable>)
  ^bb9:  // pred: ^bb7
    vm.cond_fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    vm.br ^bb10(%null_0 : !vm.ref<!hal.executable>)
  ^bb10(%19: !vm.ref<!hal.executable>):  // 2 preds: ^bb8, ^bb9
    vm.global.store.ref %19, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  }
  vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.update_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %target_buffer_slot : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer_slot : i32, %target_buffer_slot : i32, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer_slot : i32, %recv_buffer_slot : i32, %send_buffer : !vm.ref<!hal.buffer>, %recv_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer_slot : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.fill(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i64, %pattern_length : i32, %flags : i64)
  vm.import private @hal.device.queue.update(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.buffer, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.copy(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i64)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_8 : !vm.ref<!hal.buffer_view>
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %null_0 = vm.const.ref.zero : !vm.ref<!hal.executable>
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_1 = vm.const.i64.zero
      %c1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_1, %zero_1, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %req = vm.cmp.eq.ref %4, %null_2 : !vm.ref<!hal.device>
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %5 = vm.and.i32 %req, %slt : i32
      vm.cond_br %5, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %6 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%6) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %7:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %7#1 : i64
      %8 = vm.select.i32 %7#0, %nz, %zero : i32
      vm.cond_br %8, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %9:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %9#1 : i64
      %10 = vm.select.i32 %9#0, %nz_3, %zero : i32
      vm.br ^bb4(%10 : i32)
    ^bb4(%11: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_1 : i64
      %12 = vm.select.i64 %11, %c1, %zero_1 : i64
      %13 = vm.add.i64 %3, %12 : i64
      %14 = vm.and.i32 %11, %eq : i32
      %ref_4 = vm.select.ref %14, %ref, %null_2 : !vm.ref<!hal.device>
      %15 = vm.add.i64 %2, %c1 : i64
      vm.br ^bb1(%15, %13, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %req, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.cond_fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<"cuda", [#hal.executable.target<"cuda", "cuda-nvptx-fb", {iree.gpu.target = #iree_gpu.target<arch = "sm_86", features = "+ptx76", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
      vm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %16:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %16#1 : i64
      %17 = vm.select.i32 %16#0, %nz_7, %zero : i32
      %18 = vm.select.i64 %17, %zero_1, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %18, %zero_1 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.br ^bb10(%ref_10 : !vm.ref<!hal.executable>)
    ^bb9:  // pred: ^bb7
      vm.cond_fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
      vm.br ^bb10(%null_0 : !vm.ref<!hal.executable>)
    ^bb10(%19: !vm.ref<!hal.executable>):  // 2 preds: ^bb8, ^bb9
      vm.global.store.ref %19, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %0, "failed to wait on timepoint"
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ResolveRodataLoadsPass (iree-vm-resolve-rodata-loads) //----- //
vm.module public @module {
  vm.global.ref private @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.initializer {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
  vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_8 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c2 = vm.const.i32 2
  %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
  %c1 = vm.const.i32 1
  %c16 = vm.const.i32 16
  %c3 = vm.const.i32 3
  %zero = vm.const.i32.zero
  %c524288 = vm.const.i64 524288
  %c131072 = vm.const.i64 131072
  %zero_0 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
  vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
  vm.return %ref : !vm.ref<!hal.command_buffer>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.initializer {
  %c1 = vm.const.i32 1
  %null = vm.const.ref.zero : !vm.buffer
  %c14 = vm.const.i32 14
  %c-1 = vm.const.i64 -1
  %c18 = vm.const.i32 18
  %zero = vm.const.i32.zero
  %zero_0 = vm.const.i64.zero
  %c1_1 = vm.const.i64 1
  %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
  %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
  %1 = vm.ext.i32.i64.s %0 : i32 -> i64
  vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
  %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
  %5 = vm.xor.i32 %rnz, %c1 : i32
  %slt = vm.cmp.lt.i64.s %2, %1 : i64
  %6 = vm.and.i32 %5, %slt : i32
  vm.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %7 = vm.trunc.i64.i32 %2 : i64 -> i32
  %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
  %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
  %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
  %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz = vm.cmp.nz.i64 %8#1 : i64
  %9 = vm.select.i32 %8#0, %nz, %zero : i32
  vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
^bb3:  // pred: ^bb2
  %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
  %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
  %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz_3 = vm.cmp.nz.i64 %10#1 : i64
  %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
  vm.br ^bb4(%11 : i32)
^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
  %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
  %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
  %14 = vm.add.i64 %3, %13 : i64
  %15 = vm.and.i32 %12, %eq : i32
  %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
  %16 = vm.add.i64 %2, %c1_1 : i64
  vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
^bb5:  // pred: ^bb1
  vm.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
^bb7:  // pred: ^bb5
  %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
  %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
  %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz_7 = vm.cmp.nz.i64 %17#1 : i64
  %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
  %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
  %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
  vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
  vm.cond_br %eq_8, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
  %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
  %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
  vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
  vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.return
^bb9:  // pred: ^bb7
  vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %zero = vm.const.i32.zero
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c1 = vm.const.i32 1
  %c553648144 = vm.const.i32 553648144
  %c512 = vm.const.i64 512
  %c128 = vm.const.i64 128
  %zero_0 = vm.const.i64.zero
  %c131072 = vm.const.i64 131072
  %c524288 = vm.const.i64 524288
  %c-1 = vm.const.i64 -1
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c-1_1 = vm.const.i32 -1
  %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
  vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
  vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb2, ^bb1
^bb1:  // pred: ^bb0
  %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_8 : !vm.ref<!hal.buffer_view>
^bb2:  // pred: ^bb0
  vm.fail %0, "failed to wait on timepoint"
}

// -----// IR Dump After Inliner (inline) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropUnusedCallsPass (iree-vm-drop-unused-calls) //----- //
vm.module public @module {
  vm.global.ref private @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.initializer {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
  vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_8 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.initializer {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.global.ref private @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref immutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref immutable @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref immutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_8 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.br ^bb10
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  ^bb10:  // pred: ^bb8
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_4 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B_4, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_5 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_A0C577B530E5F64B_6 = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_A0C577B530E5F64B_6, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_7, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_5, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_8 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
    vm.export @__init
    vm.func private @__init() {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %ref_10 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_9, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_10, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_11 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_11, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After CSE (cse) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_6 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
    vm.export @__init
    vm.func private @__init() {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_6 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
    vm.export @__init
    vm.func private @__init() {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
module attributes {vm.toplevel} {
  vm.module public @module {
    vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
    vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
    vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
    vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
    vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
      %c1 = vm.const.i32 1
      %c16 = vm.const.i32 16
      %c3 = vm.const.i32 3
      %zero = vm.const.i32.zero
      %c524288 = vm.const.i64 524288
      %c131072 = vm.const.i64 131072
      %zero_0 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
      vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
      vm.return %ref : !vm.ref<!hal.command_buffer>
    }
    vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
    vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
    vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
      %zero = vm.const.i32.zero
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i32 1
      %c553648144 = vm.const.i32 553648144
      %c512 = vm.const.i64 512
      %c128 = vm.const.i64 128
      %zero_0 = vm.const.i64.zero
      %c131072 = vm.const.i64 131072
      %c524288 = vm.const.i64 524288
      %c-1 = vm.const.i64 -1
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c-1_1 = vm.const.i32 -1
      %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
      %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb2, ^bb1
    ^bb1:  // pred: ^bb0
      %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_6 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      vm.fail %0, "failed to wait on timepoint"
    }
    vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
    vm.export @__init
    vm.func private @__init() {
      %c1 = vm.const.i32 1
      %null = vm.const.ref.zero : !vm.buffer
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %c18 = vm.const.i32 18
      %zero = vm.const.i32.zero
      %zero_0 = vm.const.i64.zero
      %c1_1 = vm.const.i64 1
      %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
      %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
      %1 = vm.ext.i32.i64.s %0 : i32 -> i64
      vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
    ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
      %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
      %5 = vm.xor.i32 %rnz, %c1 : i32
      %slt = vm.cmp.lt.i64.s %2, %1 : i64
      %6 = vm.and.i32 %5, %slt : i32
      vm.cond_br %6, ^bb2, ^bb5
    ^bb2:  // pred: ^bb1
      %7 = vm.trunc.i64.i32 %2 : i64 -> i32
      %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
      %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
      %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %8#1 : i64
      %9 = vm.select.i32 %8#0, %nz, %zero : i32
      vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
    ^bb3:  // pred: ^bb2
      %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_3 = vm.cmp.nz.i64 %10#1 : i64
      %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
      vm.br ^bb4(%11 : i32)
    ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
      %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
      %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
      %14 = vm.add.i64 %3, %13 : i64
      %15 = vm.and.i32 %12, %eq : i32
      %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
      %16 = vm.add.i64 %2, %c1_1 : i64
      vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
    ^bb5:  // pred: ^bb1
      vm.cond_br %5, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
    ^bb7:  // pred: ^bb5
      %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
      %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
      %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz_7 = vm.cmp.nz.i64 %17#1 : i64
      %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
      %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
      %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
      vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
      vm.cond_br %eq_8, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
      %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
      vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
      vm.return
    ^bb9:  // pred: ^bb7
      vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.br ^bb10
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  ^bb10:  // pred: ^bb8
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.br ^bb10
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  ^bb10:  // pred: ^bb8
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c2 = vm.const.i32 2
  %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
  %c1 = vm.const.i32 1
  %c16 = vm.const.i32 16
  %c3 = vm.const.i32 3
  %zero = vm.const.i32.zero
  %c524288 = vm.const.i64 524288
  %c131072 = vm.const.i64 131072
  %zero_0 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
  %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
  vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
  vm.return %ref : !vm.ref<!hal.command_buffer>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
  %zero = vm.const.i32.zero
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c1 = vm.const.i32 1
  %c553648144 = vm.const.i32 553648144
  %c512 = vm.const.i64 512
  %c128 = vm.const.i64 128
  %zero_0 = vm.const.i64.zero
  %c131072 = vm.const.i64 131072
  %c524288 = vm.const.i64 524288
  %c-1 = vm.const.i64 -1
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c-1_1 = vm.const.i32 -1
  %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
  %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb2, ^bb1
^bb1:  // pred: ^bb0
  %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_6 : !vm.ref<!hal.buffer_view>
^bb2:  // pred: ^bb0
  vm.fail %0, "failed to wait on timepoint"
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__init() {
  %c1 = vm.const.i32 1
  %null = vm.const.ref.zero : !vm.buffer
  %c14 = vm.const.i32 14
  %c-1 = vm.const.i64 -1
  %c18 = vm.const.i32 18
  %zero = vm.const.i32.zero
  %zero_0 = vm.const.i64.zero
  %c1_1 = vm.const.i64 1
  %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
  %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
  %1 = vm.ext.i32.i64.s %0 : i32 -> i64
  vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
  %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
  %5 = vm.xor.i32 %rnz, %c1 : i32
  %slt = vm.cmp.lt.i64.s %2, %1 : i64
  %6 = vm.and.i32 %5, %slt : i32
  vm.cond_br %6, ^bb2, ^bb5
^bb2:  // pred: ^bb1
  %7 = vm.trunc.i64.i32 %2 : i64 -> i32
  %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
  %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
  %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
  %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz = vm.cmp.nz.i64 %8#1 : i64
  %9 = vm.select.i32 %8#0, %nz, %zero : i32
  vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
^bb3:  // pred: ^bb2
  %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
  %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
  %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz_3 = vm.cmp.nz.i64 %10#1 : i64
  %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
  vm.br ^bb4(%11 : i32)
^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
  %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
  %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
  %14 = vm.add.i64 %3, %13 : i64
  %15 = vm.and.i32 %12, %eq : i32
  %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
  %16 = vm.add.i64 %2, %c1_1 : i64
  vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
^bb5:  // pred: ^bb1
  vm.cond_br %5, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
^bb7:  // pred: ^bb5
  %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
  %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
  %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz_7 = vm.cmp.nz.i64 %17#1 : i64
  %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
  %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
  %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
  vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
  vm.cond_br %eq_8, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
  %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
  vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
  vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.return
^bb9:  // pred: ^bb7
  vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
}

// -----// IR Dump After Inliner (inline) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
}

// -----// IR Dump After CSE (cse) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
vm.module public @module {
  vm.global.ref private mutable @__device_0 : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}}
  vm.export @__init
  vm.func private @__init() {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::OrdinalAllocationPass (iree-vm-ordinal-allocation) //----- //
vm.module public @module attributes {ordinal_counts = #vm.ordinal_counts<import_funcs = 16, export_funcs = 2, internal_funcs = 3, global_bytes = 0, global_refs = 3, rodatas = 9, rwdatas = 0>} {
  vm.global.ref private mutable @__device_0 {ordinal = 0 : i32} : !vm.ref<!hal.device>
  vm.rodata private @_utf8_hal_device_id_A6960C904947097B {alignment = 1 : i64, ordinal = 0 : i32} "hal.device.id"
  vm.rodata private @_utf8_cuda_3892F998A181BAD9 {alignment = 1 : i64, ordinal = 1 : i32} "cuda"
  vm.rodata private @_utf8_hal_executable_format_476C407664EC05AD {alignment = 1 : i64, ordinal = 2 : i32} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B {alignment = 1 : i64, ordinal = 3 : i32} "cuda-nvptx-fb"
  vm.rodata private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, ordinal = 4 : i32} dense_resource<__elided__> : vector<7136xi8>
  vm.global.ref private mutable @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 {ordinal = 1 : i32} : !vm.ref<!hal.executable>
  vm.func private @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() -> !vm.ref<!hal.command_buffer> attributes {inlining_policy = #util.inline.never, ordinal = 0 : i32} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.buffer>
    %c1 = vm.const.i32 1
    %c16 = vm.const.i32 16
    %c3 = vm.const.i32 3
    %zero = vm.const.i32.zero
    %c524288 = vm.const.i64 524288
    %c131072 = vm.const.i64 131072
    %zero_0 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 = vm.global.load.ref @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.command_buffer.create(%__device_0, %zero, %c3, %c-1, %c3) : (!vm.ref<!hal.device>, i32, i32, i64, i32) -> !vm.ref<!hal.command_buffer>
    vm.call.variadic @hal.command_buffer.dispatch(%ref, %__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0, %zero, %c16, %c16, %c1, %zero_0, [], [(%zero, %zero, %null, %zero_0, %c131072), (%zero, %c1, %null, %zero_0, %c131072), (%zero, %c2, %null, %zero_0, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32, i64, i32 ..., tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.execution_barrier(%ref, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref) : (!vm.ref<!hal.command_buffer>) -> ()
    vm.return %ref : !vm.ref<!hal.command_buffer>
  }
  vm.global.ref private mutable @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 {ordinal = 2 : i32} : !vm.ref<!hal.command_buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {ordinal = 0 : i32}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, ordinal = 1 : i32}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {ordinal = 2 : i32}
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, ordinal = 3 : i32}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %queue_affinity : i64, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {minimum_version = 5 : i32, ordinal = 4 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {ordinal = 5 : i32}
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {ordinal = 6 : i32}
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32, %flags : i64, %constants : i32 ..., %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {ordinal = 7 : i32}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, ordinal = 8 : i32}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, ordinal = 9 : i32}
  vm.import private @hal.device.queue.execute.indirect(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffer : !vm.ref<!hal.command_buffer>, %binding_table : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {ordinal = 10 : i32}
  vm.import private @hal.devices.count() -> i32 attributes {nosideeffects, ordinal = 11 : i32}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {nosideeffects, ordinal = 12 : i32}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer) -> !vm.ref<!hal.executable> attributes {nosideeffects, ordinal = 13 : i32}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {ordinal = 14 : i32}
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {ordinal = 15 : i32, vm.yield}
  vm.rodata private @_utf8_input0_1C2D713EC0D8C61 {alignment = 1 : i64, ordinal = 5 : i32} "input0"
  vm.rodata private @_utf8_tensor_A0C577B530E5F64B {alignment = 1 : i64, ordinal = 6 : i32} "tensor"
  vm.rodata private @_utf8_input1_3C6604AFD2642C80 {alignment = 1 : i64, ordinal = 7 : i32} "input1"
  vm.rodata private @_utf8_input2_CE2ED9AC1537A2A5 {alignment = 1 : i64, ordinal = 8 : i32} "input2"
  vm.func private @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}, ordinal = 1 : i32} {
    %zero = vm.const.i32.zero
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i32 1
    %c553648144 = vm.const.i32 553648144
    %c512 = vm.const.i64 512
    %c128 = vm.const.i64 128
    %zero_0 = vm.const.i64.zero
    %c131072 = vm.const.i64 131072
    %c524288 = vm.const.i64 524288
    %c-1 = vm.const.i64 -1
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c-1_1 = vm.const.i32 -1
    %__device_0 = vm.global.load.ref @__device_0 : !vm.ref<!hal.device>
    %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 = vm.global.load.ref @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    %_utf8_input0_1C2D713EC0D8C61 = vm.const.ref.rodata @_utf8_input0_1C2D713EC0D8C61 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_1C2D713EC0D8C61, %c553648144, %c1, [%c512, %c128]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.device.allocator(%__device_0) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_A0C577B530E5F64B = vm.const.ref.rodata @_utf8_tensor_A0C577B530E5F64B : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_3C6604AFD2642C80 = vm.const.ref.rodata @_utf8_input1_3C6604AFD2642C80 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_3C6604AFD2642C80, %c553648144, %c1, [%c128, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_3 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_3, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c131072, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input2_CE2ED9AC1537A2A5 = vm.const.ref.rodata @_utf8_input2_CE2ED9AC1537A2A5 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_input2_CE2ED9AC1537A2A5, %c553648144, %c1, [%c512, %c512]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_A0C577B530E5F64B, %ref_2, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_5 = vm.call @hal.fence.create(%__device_0, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute.indirect(%__device_0, %c-1, %null, %ref_5, %__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0, [(%ref, %zero_0, %c131072), (%ref_3, %zero_0, %c131072), (%ref_4, %zero_0, %c524288)]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer>, tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb2, ^bb1
  ^bb1:  // pred: ^bb0
    %ref_6 = vm.call.variadic @hal.buffer_view.create(%ref_4, %zero_0, %c524288, %c553648144, %c1, [%c512, %c512]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_6 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    vm.fail %0, "failed to wait on timepoint"
  }
  vm.export @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1 attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1(%input0: tensor<512x128xf16>, %input1: tensor<128x512xf16>, %input2: tensor<512x512xf16>) -> (%output0: tensor<512x512xf16>)"}, ordinal = 0 : i32}
  vm.export @__init attributes {ordinal = 1 : i32}
  vm.func private @__init() attributes {ordinal = 2 : i32} {
    %c1 = vm.const.i32 1
    %null = vm.const.ref.zero : !vm.buffer
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %c18 = vm.const.i32 18
    %zero = vm.const.i32.zero
    %zero_0 = vm.const.i64.zero
    %c1_1 = vm.const.i64 1
    %null_2 = vm.const.ref.zero : !vm.ref<!hal.device>
    %0 = vm.call @hal.devices.count() {nosideeffects} : () -> i32
    %1 = vm.ext.i32.i64.s %0 : i32 -> i64
    vm.br ^bb1(%zero_0, %zero_0, %null_2 : i64, i64, !vm.ref<!hal.device>)
  ^bb1(%2: i64, %3: i64, %4: !vm.ref<!hal.device>):  // 2 preds: ^bb0, ^bb4
    %rnz = vm.cmp.nz.ref %4 : !vm.ref<!hal.device>
    %5 = vm.xor.i32 %rnz, %c1 : i32
    %slt = vm.cmp.lt.i64.s %2, %1 : i64
    %6 = vm.and.i32 %5, %slt : i32
    vm.cond_br %6, ^bb2, ^bb5
  ^bb2:  // pred: ^bb1
    %7 = vm.trunc.i64.i32 %2 : i64 -> i32
    %ref = vm.call @hal.devices.get(%7) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_device_id_A6960C904947097B = vm.const.ref.rodata @_utf8_hal_device_id_A6960C904947097B : !vm.buffer
    %_utf8_cuda_3892F998A181BAD9 = vm.const.ref.rodata @_utf8_cuda_3892F998A181BAD9 : !vm.buffer
    %8:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_device_id_A6960C904947097B, %_utf8_cuda_3892F998A181BAD9) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %8#1 : i64
    %9 = vm.select.i32 %8#0, %nz, %zero : i32
    vm.cond_br %9, ^bb3, ^bb4(%zero : i32)
  ^bb3:  // pred: ^bb2
    %_utf8_hal_executable_format_476C407664EC05AD = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %10:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_476C407664EC05AD, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_3 = vm.cmp.nz.i64 %10#1 : i64
    %11 = vm.select.i32 %10#0, %nz_3, %zero : i32
    vm.br ^bb4(%11 : i32)
  ^bb4(%12: i32):  // 2 preds: ^bb2, ^bb3
    %eq = vm.cmp.eq.i64 %3, %zero_0 : i64
    %13 = vm.select.i64 %12, %c1_1, %zero_0 : i64
    %14 = vm.add.i64 %3, %13 : i64
    %15 = vm.and.i32 %12, %eq : i32
    %ref_4 = vm.select.ref %15, %ref, %null_2 : !vm.ref<!hal.device>
    %16 = vm.add.i64 %2, %c1_1 : i64
    vm.br ^bb1(%16, %14, %ref_4 : i64, i64, !vm.ref<!hal.device>)
  ^bb5:  // pred: ^bb1
    vm.cond_br %5, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    vm.fail %c18, "HAL device `__device_0` not found or unavailable: #hal.device.target<\22cuda\22, [#hal.executable.target<\22cuda\22, \22cuda-nvptx-fb\22, {iree.gpu.target = #iree_gpu.target<arch = \22sm_86\22, features = \22+ptx76\22, wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<WMMA_F32_16x16x16_F16>, <WMMA_F16_16x16x16_F16>], subgroup_size_choices = [32], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 166912, max_workgroup_counts = [2147483647, 65535, 65535]>>}>]>"
  ^bb7:  // pred: ^bb5
    %_utf8_hal_executable_format_476C407664EC05AD_5 = vm.const.ref.rodata @_utf8_hal_executable_format_476C407664EC05AD : !vm.buffer
    %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_F022C9DEA6678D1B : !vm.buffer
    %17:2 = vm.call @hal.device.query.i64(%4, %_utf8_hal_executable_format_476C407664EC05AD_5, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz_7 = vm.cmp.nz.i64 %17#1 : i64
    %18 = vm.select.i32 %17#0, %nz_7, %zero : i32
    %19 = vm.select.i64 %18, %zero_0, %c-1 : i64
    %eq_8 = vm.cmp.eq.i64 %19, %zero_0 : i64
    vm.global.store.ref %4, @__device_0 : !vm.ref<!hal.device>
    vm.cond_br %eq_8, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_9 = vm.call @hal.executable.create(%4, %_utf8_cuda_nvptx_fb_F022C9DEA6678D1B_6, %matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_cuda_nvptx_fb, %null) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_9, @__device_0_executable_0_matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0 : !vm.ref<!hal.executable>
    %ref_10 = vm.call @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_apply() : () -> !vm.ref<!hal.command_buffer>
    vm.global.store.ref %ref_10, @__matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_memoize_result_0_device_0 : !vm.ref<!hal.command_buffer>
    vm.return
  ^bb9:  // pred: ^bb7
    vm.fail %c14, "HAL device `__device_0` does not support any variant of executable `matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0`; available formats: [cuda-nvptx-fb]"
  }
}

